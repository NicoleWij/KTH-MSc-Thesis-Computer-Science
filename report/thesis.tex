\RequirePackage{ifxetex}
\RequirePackage{ifluatex}
\newif\ifxeorlua
\ifxetex\xeorluatrue\fi
\ifluatex\xeorluatrue\fi

\ifxeorlua

\RequirePackage{expl3}
\ExplSyntaxOn

\ExplSyntaxOff
\else
\RequirePackage{expl3}
\ExplSyntaxOn
%\pdf_version_gset:n{2.0}
\pdf_version_gset:n{1.5}
\ExplSyntaxOff
\fi

\makeatletter
\newcommand{\disablepackage}[2]{%
 \disable@package@load{#1}{#2}%
}
\newcommand{\reenablepackage}[1]{%
 \reenable@package@load{#1}%
}
\makeatother
\ifxeorlua
\disablepackage{transparent}{}
\fi

\documentclass[nomenclature, english, biblatex]{kththesis}

\newcommand*{\generalExpl}[1]{\todo[inline]{#1}}    

\newcommand*{\engExpl}[1]{\todo[inline, backgroundcolor=kth-lightgreen40]{#1}}
\newcommand*{\sweExpl}[1]{\todo[inline, backgroundcolor=kth-lightblue40]{#1}}

\newcommand*{\warningExpl}[1]{\todo[inline, backgroundcolor=kth-lightred40]{#1}}


\usepackage[
  backend=biber,
  style=ieee,
  citestyle=numeric-comp,
  sorting=none
]{biblatex}
\addbibresource{references.bib}

\input{lib/includes}
\input{lib/kthcolors}

\input{lib/defines}
\ExplSyntaxOn
\newcommand\typestoredx[2]{\expandafter\__scontents_typestored_internal:nn\expandafter{#1} {#2}}
\ExplSyntaxOff
\makeatletter
\let\verbatimsc\@undefined
\let\endverbatimsc\@undefined
\lst@AddToHook{Init}{\hyphenpenalty=50\relax}
\makeatother


\lstnewenvironment{verbatimsc}
  {
  \lstset{%
  basicstyle=\ttfamily\tiny,
  backgroundcolor=\color{white},
  columns=[l]fixed,
  language=[LaTeX]TeX,
  keywordstyle=\color{red},
  breaklines=true,
  breakatwhitespace=true,
  breakindent=0em,
  frame=none,
  postbreak={}
  }
}{}

\lstdefinestyle{[LaTeX]TeX}{
morekeywords={begin, todo, textbf, textit, texttt}
}

\newcommand{\colorbitbox}[3]{%
	\rlap{\bitbox{#2}{\color{#1}\rule{\width}{\height}}}%
	\bitbox{#2}{#3}}



\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}

\ifbiblatex
  \usepackage[plainpages=false]{hyperref}
\else
  \usepackage[
  backref=page,
  pagebackref=false,
  plainpages=false,
  unicode=true,
  bookmarks=true,
  bookmarksopen=false,
  pdfpagemode=UseNone,
  destlabel,
  pdfencoding=auto, 
  ]{hyperref}
  \makeatletter
  \ltx@ifpackageloaded{attachfile2}{
  }
  {\usepackage{backref}
  \renewcommand*{\backref}[1]{}
  \renewcommand*{\backrefalt}[4]{%
  \ifcase #1%
   \or [Page~#2.]%
   \else [Pages~#2.]%
  \fi%
  }
  }
  \makeatother

\fi
\usepackage[all]{hypcap}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage[acronym, style=super, section=section, nonumberlist, nomain,
nopostdot]{glossaries}
\setlength{\glsdescwidth}{0.75\textwidth}
\usepackage[]{glossaries-extra}
\usepackage{dirtytalk}
\ifinswedish
  %\usepackage{glossaries-swedish}
\fi

\newglossary[tlg]{readme}{tld}{tdn}{README acronyms}


\input{lib/includes-after-hyperref}

\makeglossaries
\ifxeorlua
\input{lib/acronyms}
\else
\input{lib/acronyms-for-pdflatex}
\fi

\input{custom_configuration}

\title{Evaluating Backend Architectures for Real-Time API-Dependent Applications}
\subtitle{A Comparative Study}

% give the alternative title - i.e., if the thesis is in English, then give a Swedish title
\alttitle{Utvärdering av backend-arkitekturer för realtidsapplikationer beroende av externa API:er}
\altsubtitle{En jämförande studie}

% Enter the English and Swedish keywords here for use in the PDF metadata _and_ for later use
% following the respective abstract.
% Try to put the words in the same order in both languages to facilitate matching. For example:
\EnglishKeywords{Canvas Learning Management System, Docker containers, Performance tuning}
\SwedishKeywords{Canvas Lärplattform, Dockerbehållare, Prestandajustering}

%%%%% For the oral presentation
%% Add this information once your examiner has scheduled your oral presentation
\presentationDateAndTimeISO{2022-03-15 13:00}
\presentationLanguage{eng}
\presentationRoom{via Zoom https://kth-se.zoom.us/j/ddddddddddd}
\presentationAddress{Isafjordsgatan 22 (Kistagången 16)}
\presentationCity{Stockholm}

% When there are multiple opponents, separate their names with '\&'
% Opponent's information
\opponentsNames{A. B. Normal \& A. X. E. Normalè}

% Once a thesis is approved by the examiner, add the TRITA number
% The TRITA number for a thesis consists of two parts: a series (unique to each school)
% and the number in the series, which is formatted as the year followed by a colon and
% then a unique series number for the thesis - starting with 1 each year.
\trita{TRITA -- EECS-EX}{2024:0000}

% Put the title, author, and keyword information into the PDF meta information
\input{lib/pdf_related_includes}


% the custom colors and the commands are defined in defines.tex  
\hypersetup{
	colorlinks = true,
	breaklinks = true,
	linkcolor  = \linkscolor,
	urlcolor  = \urlscolor,
	citecolor  = \refscolor,
	anchorcolor = black
}

\ifnomenclature
\renewcommand*{\pagedeclaration}[1]{\unskip, \dotfill\hyperlink{page.#1}{page\nobreakspace#1}}

\renewcommand{\nomname}{List of Symbols Used}

\renewcommand{\nompreamble}{The following symbols will be later used within the body of the thesis.}
\makenomenclature
\fi

\colorlet{punct}{red!60!black}
\definecolor{delim}{RGB}{20,105,176}
\definecolor{numb}{RGB}{106, 109, 32}
\definecolor{string}{RGB}{0, 0, 0}

\lstdefinelanguage{json}{
  numbers=none,
  numberstyle=\small,
  frame=none,
  rulecolor=\color{black},
  showspaces=false,
  showtabs=false,
  breaklines=true,
  postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\color{gray}\hookrightarrow\space}},
  breakatwhitespace=true,
  basicstyle=\ttfamily\small,
  extendedchars=false,
  upquote=true,
  morestring=[b]",
  stringstyle=\color{string},
  literate=
 *{0}{{{\color{numb}0}}}{1}
 {1}{{{\color{numb}1}}}{1}
 {2}{{{\color{numb}2}}}{1}
 {3}{{{\color{numb}3}}}{1}
 {4}{{{\color{numb}4}}}{1}
 {5}{{{\color{numb}5}}}{1}
 {6}{{{\color{numb}6}}}{1}
 {7}{{{\color{numb}7}}}{1}
 {8}{{{\color{numb}8}}}{1}
 {9}{{{\color{numb}9}}}{1}
 {:}{{{\color{punct}{:}}}}{1}
 {,}{{{\color{punct}{,}}}}{1}
 {\{}{{{\color{delim}{\{}}}}{1}
 {\}}{{{\color{delim}{\}}}}}{1}
 {[}{{{\color{delim}{[}}}}{1}
 {]}{{{\color{delim}{]}}}}{1}
 {'}{{\char13}}1,
}

\lstdefinelanguage{XML}
{
 basicstyle=\ttfamily\color{blue}\bfseries\small,
 morestring=[b]",
 morestring=[s]{>}{<},
 morecomment=[s]{<?}{?>},
 stringstyle=\color{black},
 identifierstyle=\color{blue},
 keywordstyle=\color{cyan},
 breaklines=true,
 postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\color{gray}\hookrightarrow\space}},
 breakatwhitespace=true,
 morekeywords={xmlns,version,type}
}


\makeatletter
\AtBeginDocument{\let\c@listing\c@lstlisting}
\AtBeginDocument{\let\l@listing\l@lstlisting}
\makeatother


\renewcommand{\lstlistlistingname}{Listings}

\numberwithin{listing}{chapter}

\usepackage{subfiles}

% To have Creative Commons (CC) license and logos use the doclicense package
% Note that the lowercase version of the license has to be used in the modifier
% i.e., one of by, by-nc, by-nd, by-nc-nd, by-sa, by-nc-sa, zero.
% For background see:
% https://www.kb.se/samverkan-och-utveckling/oppen-tillgang-och-bibsamkonsortiet/open-access-and-bibsam-consortium/open-access/creative-commons-faq-for-researchers.html
% https://kib.ki.se/en/publish-analyse/publish-your-article-open-access/open-licence-your-publication-cc
\begin{comment}
\usepackage[
  type={CC},
  %modifier={by-nc-nd},
  %version={4.0},
  modifier={by-nc},
  imagemodifier={-eu-88x31}, % to get Euro symbol rather than Dollar sign
  hyphenation={RaggedRight},
  version={4.0},
  %modifier={zero},
  %version={1.0},
]{doclicense}
\end{comment}

\begin{document}
\selectlanguage{english}

\pagenumbering{alph}
\kthcover
\clearpage\thispagestyle{empty}\mbox{}
\titlepage

\bookinfopage

\frontmatter
\setcounter{page}{1}
\begin{abstract}
\markboth{\abstractname}{}
\begin{scontents}[store-env=lang]
eng
\end{scontents}



\begin{scontents}[store-env=abstracts,print-env=true]
\generalExpl{Enter your abstract here!}
An abstract is (typically) about 250 and 350 words (1/2 A4-page) with the following components:
% key parts of the abstract
\begin{itemize}
 \item What is the topic area? (optional) Introduces the subject area for the project.
 \item Short problem statement
 \item Why was this problem worth a Bachelor's/Master's thesis project? (\ie, why is the problem both significant and of a suitable degree of difficulty for a Bachelor's/Master's thesis project? Why has no one else solved it yet?)
 \item How did you solve the problem? What was your method/insight?
 \item Results/Conclusions/Consequences/Impact: What are your key results/\linebreak[4]conclusions? What will others do based on your results? What can be done now that you have finished - that could not be done before your thesis project was completed?
\end{itemize}

\end{scontents}
\engExpl{The following are some notes about what can be included (in terms of LaTeX) in your abstract.}
Choice of typeface with \textbackslash textit, \textbackslash textbf, and \textbackslash texttt: \textit{x}, \textbf{x}, and \texttt{x}.

Text superscripts and subscripts with \textbackslash textsubscript and \textbackslash textsuperscript: A\textsubscript{x} and A\textsuperscript{x}.

Some symbols that you might find useful are available, such as: \textbackslash textregistered, \textbackslash texttrademark, and \textbackslash textcopyright. For example, 
the copyright symbol: \textbackslash textcopyright Maguire 2022 results in \textcopyright Maguire 2022. Additionally, here are some examples of text superscripts (which can be combined with some symbols): \textbackslash textsuperscript\{99m\}Tc, A\textbackslash textsuperscript\{*\}, A\textbackslash textsuperscript\{\textbackslash textregistered\}, and A\textbackslash texttrademark resulting in \textsuperscript{99m}Tc, A\textsuperscript{*}, A\textsuperscript{\textregistered}, and A\texttrademark. Two examples of subscripts are: H\textbackslash textsubscript\{2\}O and CO\textbackslash textsubscript\{2\} which produce H\textsubscript{2}O and CO\textsubscript{2}.

You can use simple environments with begin and end: itemize and enumerate and within these use instances of \textbackslash item.

The following commands can be used: \textbackslash eg, \textbackslash Eg, \textbackslash ie, \textbackslash Ie, \textbackslash etc, and \textbackslash etal: \eg, \Eg, \ie, \Ie, \etc, and \etal.

The following commands for numbering with lowercase Roman numerals: \textbackslash first, \textbackslash Second, \textbackslash third, \textbackslash fourth, \textbackslash fifth, \textbackslash sixth, \textbackslash seventh, and \textbackslash eighth: \first, \Second, \third, \fourth, \fifth, \sixth, \seventh, and \eighth. Note that the second case is set with a capital 'S' to avoid conflicts with the use of second of as a unit in the \texttt{siunitx} package.

Equations using \textbackslash( xxxx \textbackslash) or \textbackslash[ xxxx \textbackslash] can be used in the abstract. For example: \( (C_5O_2H_8)_n \)
or \[ \int_{a}^{b} x^2 \,dx \]
Note that you \textbf{cannot} use an equation between dollar signs.


Even LaTeX comments can be handled by using a backslash to quote the percent symbol, for example: \% comment.
Note that one can include percentages, such as: 51\% or \SI{51}{\percent}.

\subsection*{Keywords}
\begin{scontents}[store-env=keywords,print-env=true]
% If you set the EnglishKeywords earlier, you can retrieve them with:
\InsertKeywords{english}
% If you did not set the EnglishKeywords earlier then simply enter the keywords here:
% comma separate keywords, such as: Canvas Learning Management System, Docker containers, Performance tuning
\end{scontents}
\engExpl{\textbf{Choosing good keywords can help others to locate your paper, thesis, dissertation, \ldots and related work.}}
Choose the most specific keyword from those used in your domain, see for example: the ACM Computing Classification System ({\small \url{https://www.acm.org/publications/computing-classification-system/how-to-use})},
the IEEE Taxonomy ({\small \url{https://www.ieee.org/publications/services/thesaurus-thank-you.html}}), PhySH (Physics Subject Headings)\linebreak[4] ({\small \url{https://physh.aps.org/}}), \ldots or keyword selection tools such as the National Library of Medicine's Medical Subject Headings (MeSH) ({\small \url{https://www.nlm.nih.gov/mesh/authors.html}}) or Google's Keyword Tool ({\small \url{https://keywordtool.io/}})\\

\textbf{Formatting the keywords}:
\begin{itemize}
 \item The first letter of a keyword should be set with a capital letter and proper names should be capitalized as usual.
 \item Spell out acronyms and abbreviations.
 \item Avoid "stop words" - as they generally carry little or no information.
 \item List your keywords separated by commas (``,'').
\end{itemize}  
Since you should have both English and Swedish keywords - you might think of ordering the keywords in corresponding order (\ie, so that the n\textsuperscript{th} word in each list correspond) - this makes it easier to mechanically find matching keywords.
\end{abstract}
\cleardoublepage
\babelpolyLangStart{swedish}
\begin{abstract}
  \markboth{\abstractname}{}
\begin{scontents}[store-env=lang]
swe
\end{scontents}

\subsection*{Nyckelord}
\begin{scontents}[store-env=keywords,print-env=true]
\InsertKeywords{swedish}
\end{scontents}
\end{abstract}
\babelpolyLangStop{swedish}

\cleardoublepage

\section*{Acknowledgments}
\markboth{Acknowledgments}{}
\sweExpl{Författarnas tack}

I would like to thank xxxx for having yyyy.

\acknowlegmentssignature

\fancypagestyle{plain}{}
\renewcommand{\chaptermark}[1]{ \markboth{#1}{}} 
\tableofcontents
 \markboth{\contentsname}{}

\cleardoublepage
\listoffigures

\cleardoublepage

\listoftables
\cleardoublepage
\lstlistoflistings\engExpl{If you have listings in your thesis. If not, then remove this preface page.}
\cleardoublepage
% Align the text expansion of the glossary entries
\newglossarystyle{mylong}{%
 \setglossarystyle{long}%
 \renewenvironment{theglossary}%
 {\begin{longtable}[l]{@{}p{\dimexpr 2cm-\tabcolsep}p{0.8\hsize}}}% <-- change the value here
 {\end{longtable}}%
 }
%\glsaddall
%\printglossaries[type=\acronymtype, title={List of acronyms}]
\printglossary[style=mylong, type=\acronymtype, title={List of acronyms and abbreviations}]
%\printglossary[type=\acronymtype, title={List of acronyms and abbreviations}]

%\printnoidxglossary[style=mylong, title={List of acronyms and abbreviations}]
\engExpl{The list of acronyms and abbreviations should be in alphabetical order based on the spelling of the acronym or abbreviation.
}

% if the nomenclature option was specified, then include the nomenclature page(s)
\ifnomenclature
  \cleardoublepage
  % Output the nomenclature list
  \printnomenclature
\fi

%% The following label is essential to know the page number of the last page of the preface
%% It is used to compute the data for the "For DIVA" pages
\label{pg:lastPageofPreface}
% Mainmatter is where the actual contents of the thesis goes
\mainmatter
\glsresetall
\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\selectlanguage{english}





\chapter{Introduction}
\label{ch:introduction}
Live digital services rely on a continuous flow of fresh data from public \glspl{API}. This thesis is conducted in collaboration with \textit{Klimra}, a Swedish start-up that automates train-delay compensation and now wants to add a live delay dashboard. Whether that dashboard is useful depends entirely on how quickly external updates can be ingested and displayed to travellers. Choosing an architecture that minimises latency under an \gls{API} rate cap is therefore of great importance.

Behind the scenes, two architectural choices dominate both the speed of those updates and the resources they consume: 
\begin{itemize}[noitemsep,leftmargin=*]
 \item[] \textbf{(i) ingestion method:}
 \begin{itemize}[noitemsep,leftmargin=*]
 \item whether the back-end pulls updates at fixed intervals (\emph{polling}) 
 \item or reacts to push events (\emph{event driven})
 \end{itemize}
\end{itemize}
and
\begin{itemize}[noitemsep,leftmargin=*]
 \item[] \textbf{(ii) the storage engine:}
 \begin{itemize}[noitemsep,leftmargin=*]
 \item \emph{Relational:} the classic \say{tables and SQL} databases (e.g.\ PostgreSQL)
 \item \emph{\gls{TSDB}:} built specifically for timestamped sensor data (e.g.\ TimescaleDB, Amazon Timestream)
 \item \emph{NoSQL:} cloud services that store flexible key-value records instead of fixed tables (e.g.\ DynamoDB)
 \end{itemize}
\end{itemize}

\noindent
Prior studies show that switching from polling to event-driven pipelines lowers average and tail latency but can raise CPU bursts \cite{Trindade2021EDAImpact}. 
Database comparisons tell a similar story of trade-offs: relational engines are versatile, time-series stores excel at range queries and fast inserts, and cloud NoSQL services scale reads smoothly, each at the cost of extra memory, disk, or replicas \cite{Heldt2021SciTS,Grzesik2020EdgeIoTBenchmark,Vergara2021PerformanceTSDB}.

\begin{figure}[htbp]
  \centering
  \resizebox{\linewidth}{!}{
  \begin{tikzpicture}[>=latex,
      box/.style = {draw, thick, rounded corners,
                    minimum width = 3cm, minimum height = 1cm,
                    text width    = 2.8cm, align = center}]
    \node[box, fill=blue!10]                (api)    {Trafikverket\\REST API};
    \node[box, fill=orange!10, right=2cm of api] (ingest) {Ingestion layer\\(Polling \emph{or} Events)};
    \node[box, fill=green!10,  right=2cm of ingest] (db)   {Database\\(SQL / TSDB / NoSQL)};
    \node[box, fill=yellow!15, right=2cm of db]   (ui)   {React dashboard\\(traveller view)};
    \draw (api) -- (ingest) -- (db) -- (ui);
  \end{tikzpicture}}%
  \caption{Benchmark scope: API, ingestion layer, database, React dashboard}
  \label{fig:pipeline}
\end{figure}

\noindent
This thesis benchmarks six AWS-based ingestion–storage combinations under Trafikverket's rate limit, aiming to give engineers a data-driven recipe for choosing the right stack. Figure~\ref{fig:pipeline} above illustrates the end-to-end path that each variant implements and that we benchmark.
The remainder of the chapter states the background (Section~\ref{sec:background}), defines the problem (Section~\ref{sec:problem}), describes the purpose and goals (Sections~\ref{sec:purpose}–\ref{sec:goals}), sketches the method (Section~\ref{sec:method}), lists delimitations (Section~\ref{sec:delimitations}), and outlines the structure of the thesis (Section~\ref{sec:structure}).




\section{Background}
\label{sec:background}
Live online services increasingly draw on a steady stream of data from other platforms. Any application that depends on real-time updates must ingest public or partner \glspl{API} quickly enough for the information to remain relevant by the time it reaches end-users. In a cloud environment, two key architectural choices largely decide how fresh the data stays: How updates are fetched (ingestion strategy) and where they are stored (storage engine).

%\begin{itemize}[noitemsep,leftmargin=*]
%  \item[] \textbf{(i) Ingestion strategy:} fixed-interval polling versus event-driven push pipelines
%  \item[] \textbf{(ii) Storage engine:} general-purpose relational stores, specialised time-series engines, or cloud-native NoSQL.
%\end{itemize}

%COMMENT: Too broad - went more into detail in finished sections
%-----------------------
%Prior studies confirm that each choice shifts the balance between latency, throughput, and scalability. Trindade \etal\ show that switching from scheduled polling to an event-driven pipeline cuts the slowest response times under the same load profile \cite{Trindade2021EDAImpact}. In another study, Heldt \etal\ report that purpose-built time-series databases answer range queries up to three times faster than a row-oriented traditional SQL engine running on identical hardware \cite{Heldt2021SciTS}. However, those studies are evaluated in isolation, examining ingestion and storage separately, and they rarely account for the strict request limits that public APIs impose.

\subsection{Ingestion Strategy}
Industrial control systems show what can go wrong with naïve polling. 
In one \gls{SCADA} migration the master asked each sensor for new values every few seconds, even when nothing had changed; the extra traffic clogged the network. 
After switching to push-style updates where the sensors send data only when a value changes, both bandwidth use and wait time dropped sharply \cite{Johansson2021SCADAIaaS}. 
Large data-centre studies show a similar pattern: constant polling wasted about 30\% of CPU, while event notifications halve CPU load and cut the slowest response times by almost 90\% \cite{Lewis2020PowerOfEDA}. 
Battery-powered IoT devices benefit even more: weekly energy use fell roughly \( \sim3000\times \) when devices slept until an event arrived instead of waking on a timer \cite{Makarovi2022EnergyEfficientIoT}.

Event-driven designs are not free, however. 
They need message brokers and extra work to decode each event, and small tests have shown that a single, tightly integrated program (a monolith) can still answer faster than a network of event-driven microservices \cite{Trindade2021EDAImpact}. 
These mixed results set up the first question of this thesis: should a real-time back-end \emph{poll} for updates or react to \emph{events}?

\subsection{Storage Engine}
Three families of databases matter for this study, each with its own strengths and limitations.
\textbf{Relational systems}, such as vanilla PostgreSQL, organise data in tables and are reliable all-rounders, but can slow down when millions of time-stamped rows arrive every minute.
\textbf{Time-series databases} (\glspl{TSDB}) like InfluxDB, TimescaleDB, and Amazon Timestream are built for that exact workload: they append new samples in time-order blocks, so inserts remain fast and range queries over recent hours or days run quickly. For example, TimescaleDB and InfluxDB loaded millions of sensor readings on a Raspberry Pi and answered range queries about three times faster than SQLite, a lightweight relational engine \cite{Grzesik2020EdgeIoTBenchmark}. In larger lab tests, InfluxDB led on heavy aggregations, while TimescaleDB was best for single-row look-ups thanks to PostgreSQL's indexing \cite{Daqouri2023TimeseriesVsSQL,Heldt2021SciTS}. Performance, however, is not guaranteed. In the cloud, Amazon Timestream fell behind DynamoDB and even a simple S3 data-lake when full-history scans were required \cite{Johansson2022AWSCloudData}, and MongoDB beat TimescaleDB on highly selective tag queries \cite{Mohamed2024DBMSComparison}.
Finally, \textbf{cloud NoSQL services} such as DynamoDB trade strict schema and complex joins for limitless horizontal scaling; studies show they can match \gls{TSDB} write speed once enough replicas are added, but the extra nodes consume more memory and network bandwidth \cite{Zhang2023EdgeTSDB,Vergara2021PerformanceTSDB}.

These mixed results underline that no single engine is \say{best} in every situation. Relational databases offer familiarity and strong consistency, \glspl{TSDB} excel at broad time-range analytics, and NoSQL stores scale effortlessly for simple key-value access—each at a cost that must be weighed for Klimra's real-time dashboard.





\section{Problem}
\label{sec:problem}

Klimra's live-delay dashboard—like any service that relies on a public, rate-limited API—must deliver fresh data in seconds. Developers must therefore decide both \textit{how} to fetch updates (polling or push) and \textit{where} to store them (relational, time-series, or NoSQL). The literature treats these decisions separately: ingestion studies vary the fetch style but leave the database unchanged \cite{Trindade2021EDAImpact,Lewis2020PowerOfEDA}, while database benchmarks compare storage engines using a single, high-throughput synthetic workload that ignores \gls{API} rate limits and keeps the ingestion method fixed \cite{Heldt2021SciTS,Grzesik2020EdgeIoTBenchmark}. Because no study evaluates both choices together under a realistic traffic cap, engineers lack evidence for selecting a complete backend stack.

This thesis aims to close that gap by measuring six AWS configurations: polling versus event-driven ingestion paired with RDS PostgreSQL, Amazon Timestream, or DynamoDB. It records update latency, query speed, and resource use while staying within Trafikverket's live request limit, giving practitioners a clear decision matrix and researchers a reproducible baseline.



\section{Purpose}
\label{sec:purpose}
The study has a dual purpose. For Klimra, it aims to guide the choice of ingestion style and database so that the dashboard developed in this project can meet latency targets without breaching Trafikverket's request limit. For the academic and engineering community, it fills the documented research gap by supplying a public, statistically sound benchmark that links ingestion strategy with storage choice under a real-world traffic cap. Achieving these aims also demonstrates the author's ability to design, execute, and analyse a cloud-scale performance study, thereby satisfying the goals of the master's degree project.



\section{Goals}
\label{sec:goals}
The project turns that purpose into four concrete goals:

\begin{enumerate}[label=\textbf{G\arabic*},leftmargin=*]
  \item \textbf{Prototype} six back-end variants that differ only in \emph{(i)} ingestion style (polling, event-driven) and \emph{(ii)} storage engine (RDS PostgreSQL, Amazon Timestream, DynamoDB).

  \item \textbf{Benchmark} each variant against Trafikverket's live train-position API, measuring update latency, query latency, throughput, CPU, memory, and network I/O.

  \item \textbf{Analyse} the results using appropriate statistical tests to identify significant differences and resource trade-offs.

  \item \textbf{Deliverables}:  
        \begin{itemize}[noitemsep,leftmargin=*]
          \item a reusable k6/Grafana benchmarking toolkit,  
          \item a decision matrix that maps workload patterns to the most efficient configuration for Klimra, and  
          \item a reproducible dataset and report that other researchers can build on.
        \end{itemize}
\end{enumerate}




\section{Research Methodology}
\label{sec:method}

This study adopts a \textit{design-science case‐study} approach: one React/Node train-tracking application is implemented once and then redeployed with six back-end variants so that only the ingestion style (polling versus event-driven) and the database (PostgreSQL, Amazon Timestream, DynamoDB) differ.

\subsection{Experimental frame}
The frame states what is varied, what is fixed, and what is measured. A 24-hour recording of Trafikverket's train position feed is replayed at real-time speed with \textsc{k6}, an open-source HTTP load generator scripted in JavaScript. Each back-end is launched by Terraform, which provides an identical virtual machine for every trial. It provides two virtual CPU cores, 4 GiB of RAM, and runs on Amazon's Graviton 3 processor. AWS CloudWatch, the platform's built-in monitoring service, samples once per second five metric groups: update latency, ingestion delay, query latency, CPU + memory load, and network/disk I/O. This multi-metric set mirrors the suites used in recent cloud benchmarks \cite{Heldt2021SciTS,Grzesik2020EdgeIoTBenchmark}.


\subsection{Sampling and analysis}
Each of the six back-ends will be exercised 15 times, for 30 minutes per run, on freshly launched virtual machines. This repetition rate follows the noisy-neighbour guidance of Hidalgo \textit{et al.} for cloud experiments, where background tenants can distort a single run's latency or throughput \cite{Hidalgo2023StreamProcMicroservices}. The resulting time series are evaluated in three steps:

\begin{enumerate}[label=\arabic*., leftmargin=*]
  \item \textbf{Shapiro–Wilk test} checks whether each metric is approximately normally distributed.  
  \item \textbf{One-way ANOVA} (if normal) or \textbf{Kruskal–Wallis} (if non-normal) compares all six back-ends in one go to detect any statistically significant difference.  
  \item \textbf{Holm–Šidák correction} on post-hoc pairwise tests keeps the overall false-positive rate below $5\,\%$.  
\end{enumerate}

\noindent
Every reported difference is accompanied by an \textbf{effect size}: $\eta^{2}$ for ANOVA or $\varepsilon^{2}$ for Kruskal–Wallis. This is so that readers know how large the gap is, not just that it exists.


\subsection{Method choices}
Local emulators and micro-benchmarks were rejected because they mask cloud side variance and ignore the interaction between ingestion logic and storage engine. All Terraform scripts, Dockerfiles, and raw metric logs will be released under an open licence to meet reproducibility guidelines for cloud experiments \cite{PerformanceEvaluationMetrics}.






\section{Delimitations}
\label{sec:delimitations}
This thesis targets the server-side path from Trafikverket's \gls{API} to Klimra's planned dashboard. The work therefore has the following delimitations:

\begin{itemize}[noitemsep,leftmargin=*]
  \item \textbf{Cloud scope:} Only AWS databases are benchmarked (RDS PostgreSQL, Amazon Timestream, DynamoDB). Other cloud providers or locally hosted databases are not included.

  \item \textbf{Ingestion patterns:} The study compares fixed-interval polling with event-driven SQS/Lambda pipelines. Other mechanisms, such as long polling, gRPC streams, or change-data-capture, are left for future work.

  \item \textbf{Databases:} One representative engine is chosen per family. Variants such as InfluxDB, TimescaleDB on self-managed Postgres, or document stores beyond DynamoDB are out of scope.

  \item \textbf{Client side:} UI/UX design, mobile apps, and front-end optimisation are not evaluated. The React dashboard is used only as a constant workload generator.

  \item \textbf{Non-functional aspects:} Security hardening, cost modelling, carbon footprint, and detailed failure-recovery analysis are acknowledged but not measured.

  \item \textbf{Traffic profile:} Experiments replay one 24-hour snapshot at the live request limit. Burst tests above that limit and multi-day retention scenarios are excluded.

  \item \textbf{Geography:} All resources are hosted in a single AWS region; cross-region latency and replication behaviour are not studied.
\end{itemize}

\noindent
These boundaries keep the project focused on the interplay between ingestion style and storage engine under a realistic, rate-limited workload.




\section{Structure of the thesis}
\label{sec:structure}

\begin{description}[leftmargin=0cm]

  \item[Chapter~\ref{ch:introduction}] introduces the study, states the problem, purpose, goals, delimitations, and gives a roadmap for the rest of the thesis.

  \item[Chapter~\ref{ch:background}] reviews related work on ingestion strategies, database families, and cloud-benchmark practices needed to understand the study.

  \item[Chapter~\ref{ch:methods}] explains the research methodology, experimental frame, sampling strategy, and statistical tests.

  \item[Chapter~\ref{ch:whatYouDid}] outlines the implementation of the six AWS back-end variants and the k6/Grafana benchmarking toolkit.

  \item[Chapter~\ref{ch:resultsAndAnalysis}] presents the collected metrics and analyses latency, resource use, and scalability trade-offs.

  \item[Chapter~\ref{ch:discussion}] interprets the results, compares them to prior work, and discusses practical implications for cloud architects.

  \item[Chapter~\ref{ch:conclusionsAndFutureWork}] summarises the main findings, notes limitations, and suggests directions for future research.

  \item[Appendices] list acronyms and supply supporting artefacts (Terraform scripts, code snippets, raw data) to ensure reproducibility.

\end{description}







\cleardoublepage
\chapter{Background}
\label{ch:background}

This chapter provides the technical foundation and overview of prior work needed to follow and understand the rest of the thesis. First, 
%Section \ref{sec:prereq} first defines cloud-computing concepts, such as API rate limits, event brokers, and managed databases, on which the study builds.
Section \ref{sec:ingestion} surveys fixed-interval polling and event-driven pipelines, summarising the latency, throughput, and cost trade-offs reported in recent literature. In a similar way, Section \ref{sec:storage} then reviews the three database families evaluated here (relational, time-series, and NoSQL) and the benchmark results in recent literature. Because earlier studies look at ingestion and storage separately, none test the two together under real-world rate limits. Section \ref{sec:gap} discusses how his thesis fills that gap. Finally, Section \ref{sec:summary} summarises the key points of the chapter.


%\section{Concepts and Prerequisites}
%\label{sec:prereq}
% ---------------------------------------------------------------
%%% >> Keep this section only if the reader needs domain knowledge that a typical CS/SE student would not already have.
%%% >> Bullet ideas:
%%%    – What a “rate-limited public API” is
%%%    – Definitions of latency, throughput, tail latency, etc.
%%%    – AWS basics (Lambda, SQS, DynamoDB, …) if needed
%%%    – Any special statistical terms you will use later
%
% \subsection{Key Terminology}
% \subsection{Cloud Building Blocks (AWS quick primer)}
% \subsection{Performance Metrics Used in This Thesis}
%---------------------------------------------------------------

\section{Ingestion Strategies}
\label{sec:ingestion}
There are two main ways to pull information out of a channel: the application can poll at fixed intervals, or it can let the messaging system notify it as soon as a message arrives \cite[Ch.~10]{Hohpe2003EIP}.
Most cloud back-ends adopt one of these two patterns when they fetch data from external services.


Cloud back-ends usually obtain data in one of two ways: a Polling Consumer repeatedly requests it at a fixed interval, while an Event-Driven Consumer waits for the source to deliver a message \cite[Chap.~10]{Hohpe2003EIP}. This section introduces both patterns and reviews the performance findings reported in earlier studies.


\subsection{Fixed-Interval Polling}
Polling is conceptually simple: a worker thread (or PLC task) issues a request every $\Delta t$, parses the reply, and stores the result.
Because a Polling Consumer calls receive only when it is ready, the application can precisely throttle how many messages it accepts and will thus keep itself from being overloaded.
The trade-off is efficiency: when the channel is empty, the polling thread either blocks or wakes up only to find no work, so CPU time is wasted while nothing is processed \cite[pp.~494–497]{Hohpe2003EIP}.

Johansson and Larsson migrated a \gls{SCADA} system to AWS and found that fixed‐interval polling flooded the network with repeated sensor messages; they therefore recommended replacing the loops with event notifications \cite{Johansson2021SCADAIaaS}.  
Likewise, Makarović \textit{et~al.}\ showed that allowing a household IoT toaster to stay idle until an event arrives cuts Wi-Fi energy use by about \(3000\times\) compared with continuous polling \cite{Makarovi2022EnergyEfficientIoT}.


\subsection{Event-Driven Pipelines}
In the alternative Event-Driven Consumer pattern, the application hands a callback (listener) object to the messaging middleware; the broker then invokes that listener as soon as a new message arrives \cite[pp.~498–501]{Hohpe2003EIP}.  
Since the thread sleeps until the broker calls back, the CPU only do work when there is an actual message, and the system reacts right away instead of waiting for the next poll.
The downside is extra code: the callback has to be thread-safe, the program must handle several listener threads, and it must manage message acknowledgements or retries itself \cite[pp.~498–501]{Hohpe2003EIP}.

Switching to events does not, however, automatically make systems faster. In a load-testing study, Berg showed that a quick microservice rewrite answered requests more slowly than the original monolith, even though it used less CPU, so the extra network and serialization overhead can cancel out the gains of an event-driven design if it is not tuned carefully \cite{Berg2022MonolithVsMicroservices}.
Later work highlights three common problems: writing idempotent handlers, keeping events in the right order, and absorbing sudden bursts of messages without swamping the database \cite{Trindade2021EDAImpact}.


%\subsection{Prior Work}
%Table \ref{tab:ingestion-lit} sums up some key measurements from prior work.

%\begin{table}[H]
%  \centering
%  \caption{Typical impact of moving from polling to event-driven ingestion}
%  \label{tab:ingestion-lit}
%  \begin{tabular}{p{3.4cm}p{4.0cm}p{3.7cm}}
%    \toprule
%    \textbf{Domain} & \textbf{Observed Change} & \textbf{Key Point} \\ \midrule
%    Industrial SCADA           & Bandwidth ↓ (duplicates removed)      & Cuts \say{network flood} traffic \\
%    Data-centre orchestration   & CPU ↓ 35–50 \%; latency ↓ 70 \%        & Less idle spin and better scale \\
%    Home IoT (Wi-Fi sensor)     & Energy use ↓ $\approx\!3000\times$     & Polling wastes standby power \\
%    Micro-service refactor      & CPU ↓ but latency ↑                   & Extra hops can hurt response \\ 
%    \bottomrule
%  \end{tabular}
%\end{table}

%\noindent
%Most studies agree that event-driven designs save CPU time and bandwidth, but the gains depend on workload size, traffic bursts, and how well the system is built. Crucially, none of them compares polling and push methods while testing relational, time-series, and NoSQL databases under a real API rate limit. This is the research gap this thesis tackles.



%%%       – cite Trindade et al., Lewis et al., etc.
%%%       – summarise pros/cons table-style if helpful

%%%    \subsection{Why a combined benchmark is still missing}
%
% (Short quotes, numbers, and citations go here)

% ---------------------------------------------------------------
\section{Storage Technologies}
\label{sec:storage}

Among the several storage paradigms available in the cloud, this study concentrates on three that best match real-time ingestion needs:

\begin{itemize}[leftmargin=*]
  \item \textbf{Relational databases} (e.g.\ PostgreSQL on Amazon RDS) organise data in normalised tables and offer full SQL, strong \gls{ACID} guarantees, and mature indexing.  
  \item \textbf{Time-series databases} (e.g.\ Amazon Timestream) append readings to time-ordered blocks, optimise range scans over recent windows, and compress repeating timestamps.  
  \item \textbf{NoSQL key–value stores} (e.g.\ DynamoDB) relax joins and schema in favour of horizontal scaling and single-digit millisecond reads at any load.  
\end{itemize}

\noindent
Prior work shows that each family excels under a different access pattern. Relational engines are versatile but can slow when millions of inserts arrive every minute \cite{Heldt2021SciTS}. Purpose-built time-series stores answer range queries up to three times faster than Postgres on identical hardware \cite{Grzesik2020EdgeIoTBenchmark}, while NoSQL services match write speed once enough replicas are added, although at higher memory cost \cite{Vergara2021PerformanceTSDB}. No single solution is universally \say{best,} so the rest of this section reviews each option in turn and ends with a literature-based trade-off matrix.

\subsection{Relational Databases (PostgreSQL/RDS)}
Relational databases store information in tables made of rows and columns, where each table has a fixed schema that defines the names and data types of its columns. Queries are written in SQL, and the database engine guarantees \gls{ACID} properties so that transactions stay consistent even if many users change data at the same time \cite[Chs.~1–2]{Silberschatz2020DB}.

PostgreSQL is a popular open-source implementation of the relational model; it adds useful extras such as JSON columns, full-text search, and advanced indexing options \cite[Ch.~27]{Silberschatz2020DB}. On AWS, you can run PostgreSQL as a managed service called Amazon RDS for PostgreSQL. RDS handles routine chores, such as creating automated backups, applying security patches, setting up replication across Availability Zones, and letting you scale storage with a few clicks. It does this so developers can focus on schema design and query tuning instead of server maintenance \cite{AWSRDSPostgreSQL}.


\subsection{Time-Series Databases (Amazon Timestream)}
Time-series databases are built for data whose primary key is a timestamp. Instead of normal rows, they keep new samples in time-ordered batches and squeeze space by compressing columns that change slowly. They also let you run window queries, such as averages or rates, directly. Because data is only added and not rewritten, and most queries scan a continuous time range, a good TSDB can accept a high write load and still answer \say{last hour} or \say{last day} questions quickly; these queries ask for all points from the most recent 60 minutes or 24 hours and summarise them \cite[Chs.~3 \& 11]{Kleppmann2017DDIA}.

Amazon Timestream is a managed, serverless \gls{TSDB}. Storage and compute grow automatically as your metric data increases. New data stays in memory for quick queries and is later moved to cheaper disk storage based on a retention rule you choose. Timestream's SQL lets you resample, fill gaps, or delete old records without separate \gls{ETL} jobs, and AWS takes care of backups, replication, and encryption \cite{AWSTimestreamDoc}.


\subsection{NoSQL Key–Value Stores (DynamoDB)}
Key-value databases keep every record as two parts: a unique key and its associated value.
To read or update a record, the engine hashes the key and goes straight to the right location, so performance stays high even when the dataset grows large.
Because the value can be any binary or JSON document, the database keeps it exactly as it is and does not enforce a fixed schema.
Applications are free to add or remove fields without a formal migration, and clusters can shard data by key to spread load and add replicas easily \cite[Chs.~2 \& 8]{Sadalage2013NoSQLDistilled}.

Amazon DynamoDB is AWS's managed key-value service.
It is serverless, meaning that users never have to create or maintain the underlying machines.
AWS provisions the hardware, applies software updates, and scales capacity up or down as traffic changes.
All you supply are two numbers: your desired read and write capacity units. 
The platform then handles partitioning, replication, and fail-over in the background.
Each item is synchronously copied to three Availability Zones, backed up automatically, and served with single-digit millisecond latency.
Features such as global secondary indexes, point-in-time recovery, and at-rest encryption are included, so developers do not have to run or patch their own database clusters \cite{AWSDynamoDBDoc}.


\subsection{Trade-Off Matrix from Literature}

Taken together, recent benchmarks show a consistency in results.  
Relational engines (PostgreSQL/RDS) excel at ad-hoc joins and point look-ups but slow down once sustained insert rates exceed \(\sim\!10^{4}\) rows s\(^{-1}\) on commodity cloud nodes \cite{Heldt2021SciTS}.  
Purpose-built time-series stores keep up with bursty sensor feeds and answer
range-window queries 2–3× faster than Postgres on the same hardware
\cite{Grzesik2020EdgeIoTBenchmark,Daqouri2023TimeseriesVsSQL}.  
Key–value services such as DynamoDB scale linearly with partitions and can
match TSDB write speed, but they consume more memory and require careful key
design to avoid hot shards \cite{Vergara2021PerformanceTSDB,Zhang2023EdgeTSDB}.
Table~\ref{tab:storage-matrix} contrasts the most relevant findings.

\begin{table}[H]
\centering
\caption{Headline results from storage benchmarks (\(\uparrow\)=higher is
better, \(\downarrow\)=lower is better).}
\label{tab:storage-matrix}
\begin{tabular}{p{2.8cm}p{2.1cm}p{2.1cm}p{5.5cm}}
\toprule
\textbf{Study} & \textbf{Best Write Through\-put} & \textbf{Fastest Range Query} & \textbf{Key Observation} \\
\midrule
Grzesik \textit{et al.}\ (2020) &
InfluxDB \(\uparrow\) 1.6 M rows s\(^{-1}\) &
TimescaleDB \(\downarrow\) 0.35 s (24 h window) &
TSDBs beat SQLite and Riak-TS on Pi-class hardware \cite{Grzesik2020EdgeIoTBenchmark}. \\[4pt]

Heldt \textit{et al.}\ (2021) &
TimescaleDB \(\uparrow\) 3× Postgres &
InfluxDB \(\downarrow\) 60 ms (1 h window) &
Relational joins handy but cost 2–3× more CPU \cite{Heldt2021SciTS}. \\[4pt]

Vergara \textit{et al.}\ (2021) &
DynamoDB equals InfluxDB after 3 replicas &
InfluxDB \(\downarrow\) 55 ms &
NoSQL pays 1.8× RAM for same latency \cite{Vergara2021PerformanceTSDB}. \\[4pt]

Johansson (2022) &
S3 data-lake \(\uparrow\) 2× Timestream (bulk scan) &
DynamoDB fastest for point reads &
Cloud-managed TSDB not always top; depends on query mix \cite{Johansson2022AWSCloudData}. \\[4pt]

Zhang \textit{et al.}\ (2023) &
DynamoDB \(\uparrow\) 2 M rows s\(^{-1}\) (edge cluster) &
TimescaleDB \(\downarrow\) 0.4 s &
Key-design hot-spotting degrades NoSQL latency if ignored \cite{Zhang2023EdgeTSDB}. \\
\bottomrule
\end{tabular}
\end{table}

\noindent
The matrix confirms that no single engine dominates every metric.  
Relational stores offer flexible querying, \glspl{TSDB} shine on time-window analytics, and NoSQL services win on elastic scaling.  
Yet every cited paper fixes the ingestion style (usually batch writes or simple polling). No work measures these engines under both polling and event-driven pipelines while obeying a live API rate limit.  


\section{Related Work and Research Gap}
\label{sec:gap}
Earlier studies tend to split the backend story in two.
Some papers ask only how fresh data is ingested (for example, comparing polling with event-driven updates), while others test where that data is stored (relational, time-series, or NoSQL engines).
These single-focus results are helpful, but they do not answer the practical question facing a live, API-limited service such as Klimra's delay dashboard: Which mix of ingestion style \textit{and} database keeps updates fast without breaking the request cap? 

The next three subsections review what the literature has taught us about (i) ingestion in isolation, (ii) storage in isolation, and (iii) why their combination under a real-world traffic limit is still missing. That gap sets the stage for the benchmark designed in Chapter~\ref{ch:methods}.

\subsection{Ingestion in isolation}
Several empirical studies examine only the data-ingestion side of a backend, keeping the database layer fixed.  
In an industrial‐control study, Johansson and Larsson migrated an EclipseSCADA plant to two Australian IaaS regions and replayed traffic from 800 simulated Modbus sensors \cite{Johansson2021SCADAIaaS}.  
With polling set to one millisecond, round-trip time for a single register climbed from \SI{11}{\milli\second} (1 sensor) to almost \SI{14}{\second} (800 sensors) because the server spent most of its time issuing and queuing requests instead of processing replies; the authors therefore recommended switching to event notifications to eliminate the \say{network flood.}

For battery-powered devices, Makarović \textit{et al.} let a Wi-Fi \say{smart toaster} sleep until an event arrived.  
Energy use fell from \SI{10.9}{\kilo\joule} (continuous polling for one week) to just \SI{3.6}{\joule}, roughly a three-thousand-fold drop \cite{Makarovi2022EnergyEfficientIoT}.

Inside data-centre software, Trindade and Batista replaced REST polling scripts with a Kafka-based Event-Driven microservices Architecture.  
Under the same workload, the event design cut average CPU load by \SI{47}{\percent} and trimmed mean response time by \SI{89}{\percent} \cite{Trindade2021EDAImpact}.

The results are, however, not entirely one-sided.  
Berg ported a small web shop from a monolith to AWS microservices and measured three load scenarios \cite{Berg2022MonolithVsMicroservices}.  
Although the new design used less CPU, its mean response time rose from a stable \SI{13}–\SI{15}{\milli\second} to between \SI{16} and \SI{43}{\milli\second}, largely because each request now passed through multiple event queues.

Taken together, these experiments show that event-driven ingestion can save CPU, bandwidth, and battery power, but poorly tuned pipelines may add extra hops and hurt tail latency.  
All of the above work keeps the storage engine fixed; the next subsection reviews studies that take the opposite approach by varying the database while leaving ingestion unchanged.


\subsection{Storage in isolation}
%add detail about the studies
Separate benchmarks compare relational, time-series, and NoSQL engines with
synthetic workloads that write directly to the database.  
Results agree that PostgreSQL is versatile but slows under very high
insert rates \cite{Heldt2021SciTS}.  
Time-series stores such as InfluxDB and TimescaleDB handle range queries
two to three times faster than relational rows
\cite{Grzesik2020EdgeIoTBenchmark,Daqouri2023TimeseriesVsSQL}.  
Key–value services like DynamoDB match those write speeds once extra
partitions are added, although they use more memory per record
\cite{Vergara2021PerformanceTSDB,Zhang2023EdgeTSDB}.  
These tests, however, feed the databases with pre-generated traces and do
not model the delays of an external API.

\subsection{Missing combination under a live rate limit}
%add detail about the studies
To the best of our knowledge, no prior work evaluates \emph{both} ingestion
style and storage choice together while respecting the hard request limit
imposed by a real public API.  
This gap matters for companies such as Klimra, where the back-end must meet
live traffic caps and still keep data fresh.  
Inspired by the load-repetition method of Hidalgo \textit{et al.}
\cite{Hidalgo2023StreamProcMicroservices}, the next chapter designs a test
loop that replays one day of Trafikverket traffic and measures six AWS
back-end variants.  The goal is to provide the first side-by-side data on
how polling versus events interact with relational, time-series, and
NoSQL storage under realistic limits.



%\section{Summary}
%\label{sec:summary}

%%% >> 4–6 lines that:
%%%    – Recap the most important takeaways
%%%    – Sign-post how they connect to Chapter 3 (Methods)
%%%      e.g. “Based on the gaps identified above, the next chapter
%%%      details how six backend variants were prototyped and
%%%      benchmarked …”










\cleardoublepage
\chapter{Method or Methods}
\label{ch:methods}
\sweExpl{Metod eller Metodval}
\generalExpl{This chapter is about Engineering-related
 content, Methodologies and Methods. Use a self-explaining title.\\The
 contents and structure of this chapter will change with your choice of
 methodology and methods.}



\generalExpl{Describe the engineering-related contents (preferably with models) and the research methodology and methods that are used in the degree project.\\
Give a theoretical description of the scientific or engineering methodology you are going to use and why have you chosen this method. What other methods did you consider and why did you reject them?\\
In this chapter, you describe what engineering-related and scientific skills you are going to apply, such as modeling, analyzing, developing, and evaluating engineering-related and scientific content. The choice of these methods should be appropriate for the problem. Additionally, you should be conscious of aspects relating to society and ethics (if applicable). The choices should also reflect your goals and what you (or someone else) should be able to do as a result of your solution - which could not be done well before you started.}

The purpose of this chapter is to provide an overview of the research method
used in this thesis. Section~\ref{sec:researchProcess} describes the research
process. Section~\ref{sec:researchParadigm} details the research
paradigm. Section~\ref{sec:dataCollection} focuses on the data collection
techniques used for this research. Section~\ref{sec:experimentalDesign}
describes the experimental design. Section~\ref{sec:assessingReliability}
explains the techniques used to evaluate the reliability and validity of the
data collected. Section~\ref{sec:plannedDataAnalysis} describes the method
used for the data analysis. Finally, Section~\ref{sec:evaluationFramework}
describes the framework selected to evaluate xxx.

\sweExpl{Vilka vetenskaplig eller ingenjörs-metodik ska du använda och varför har du valt den här metoden. Vilka andra metoder gjorde du övervägde du och varför du avvisar dem.
Vad är dina mål? (Vad ska du kunna göra som ett resultat av din lösning - vilken inte kan göras i god tid innan du började)
Vad du ska göra? Hur? Varför? Till exempel, om du har implementerat en artefakt vad gjorde du och varför? Hur kommer du utvärdera den.
Syftet med detta kapitel är att ge en översikt över forsknings metod som
används i denna avhandling. Avsnitt~\ref{sec:researchProcess} beskriver forskningsprocessen. Avsnitt~\ref{sec:researchParadigm} beskriver forskningsparadigmen detaljerat. Avsnitt~\ref{sec:dataCollection} fokuserar på datainsamlingstekniker som används för denna forskning. Avsnitt~\ref{sec:experimentalDesign} beskriver experimentell
design. Avsnitt~\ref{sec:assessingReliability} förklarar de tekniker som används för att utvärdera
tillförlitligheten och giltigheten av de insamlade uppgifterna. Avsnitt~\ref{sec:plannedDataAnalysis}
beskriver den metod som används för dataanalysen. Slutligen, Avsnitt~\ref{sec:evaluationFramework}
beskriver ramverket som valts för att utvärdera xxx.\\
Ofta kan man koppla ett antal följdfrågor till undersökningsfrågan och problemlösningen t ex\\
(1) Vilken process skall användas för konstruktion av lösningen och vilken process skall kopplas till denna för att svara på undersökningsfrågan?\\
(2) Hur och vilket resultat (storheter) skall presenteras både för att redovisa svar på undersökningsfrågan (resultatkapitlet i denna rapport) och redovisa resultat av problemlösningen (prototypen, ofta dokument som bilagor men vilka dokument och varför?).\\
(3) Vilken teori/teknik skall väljas och användas både för undersökningen (taxonomi, matematik, grafer, storheter mm) och problemlösning (UML, UseCases, Java mm) och varför?\\
(4) Vad behöver du som student leverera för att uppnå hög kvaliet (minimikrav) eller mycket hög kvalitet på examensarbetet?\\
(5) Frågorna kopplar till de följande underkapitlen.\\
(6) Resonemanget bygger på att studenter på hing-programmet ofta skall konstruera något åt problemägaren och att man till detta måste koppla en intressant ingenjörsfråga. Det finns hela tiden en dualism mellan dessa aspekter i exjobbet.
}

\section{Research Process}
\label{sec:researchProcess}

\sweExpl{Undersökningsrocess och utvecklingsprocess}

Figure~\ref{fig:researchprocess} shows the steps conducted to carry out this research. 

\sweExpl{Figur~\ref{fig:researchprocess} visar de steg som utförs för att genomföra\\
Beskriv, gärna med ett aktivitetsdiagram (UML?), din undersökningsprocess och utvecklingsprocess. Du måste koppla ihop det akademiska intresset (undersökningsprocess) med ursprungsproblemet (utvecklingsprocess)
denna forskning.\\
Aktivitetsdiagram från t ex UML-standard}


 
\begin{figure}[!ht]
 \begin{center}
  \includegraphics[width=0.5\textwidth]{figures/researchprocess.png}
 \end{center}
 \caption{Research Process}
 \label{fig:researchprocess}
\end{figure}

\generalExpl{Example of using customized item labels.}
Some steps in the process:
\begin{enumerate}[leftmargin=*, label=\textbf{Step \arabic*}, ref=Step \arabic*] %labelindent=1em for indent
  \itemsep0em
  \item \label{x:s1} plan experiment,
  \item \label{x:s2} conduct experiment,
  \item \label{x:s3} analyze data from the experiment, and
  \item \label{x:s4} discuss the results of the analysis.
\end{enumerate}

\sweExpl{Forskningsprocessen}


\section{Research Paradigm}
\label{sec:researchParadigm}
\sweExpl{Undersökningsparadigm\\
Exempelvis\\
Positivistisk (vad/hur fungerar det?) kvalitativ fallstudie med en deduktivt (förbestämd) vald ansats och ett induktivt(efterhand uppstår dataområden och data) insamlade av data och erfarenheter.}


\section{Data Collection}
\label{sec:dataCollection}
\sweExpl{Datainsamling\\
(Detta bör också visa att du är medveten om de sociala och etiska frågor som
kan vara relevanta för dina data insamlingsmetod.)}
\generalExpl{This should also show that you are aware of the social and ethical concerns that might be relevant to your data collection method.}



\subsection{Sampling}
\sweExpl{Stickprovsundersökning}

\subsection{Sample Size}
\sweExpl{Provstorleken}

\subsection{Target Population}
\sweExpl{Målgruppen}

\section[Experimental design/Planned Measurements]{Experimental design and\\Planned Measurements}
\label{sec:experimentalDesign}
\sweExpl{Experimentdesign/Mätuppställning}

\subsection{Test environment/test bed/model}
\engExpl{Describe everything that someone else would need to reproduce your test environment/test bed/model/… .}
\sweExpl{Testmiljö/testbädd/modell\\
Beskriv allt att någon annan skulle behöva återskapa din testmiljö / testbädd / modell / …}

\subsection{Hardware/Software to be used}
\sweExpl{Hårdvara / programvara som ska användas}


\section{Assessing reliability and validity of the data collected}
\label{sec:assessingReliability}
\sweExpl{Bedömning av validitet och reliabilitet hos använda metoder och insamlade data }


\subsection{Validity of method}
\label{sec:validtyOfMethod}
\sweExpl{Giltigheten av metoder\\
 Har dina metoder gett dig de rätta svaren och lösningarna? Var metoderna korrekta?}

\engExpl{How will you know if your results are valid?}
\engExpl{Remember that validity is about the \textit{accuracy} of a measurement while reliability is about the \textit{consistency} of the measurement values under the same conditions (\ie repeatability).}

\subsection{Reliability of method}
\label{sec:reliabilityOfMethod}
\sweExpl{Tillförlitlighet av för metoder\\
Hur bra är dina metoder, finns det bättre metoder? Hur kan du förbättra dem?}
\engExpl{How will you know if your results are reliable?}

\subsection{Data validity}
\label{sec:dataValidity}
\sweExpl{Giltigheten av uppgifter\\
Hur vet du om dina resultat är giltiga? Är ditt resultat rättvisande?}

\subsection{Reliability of data}
\label{sec:reliabilityOfData}
\sweExpl{Tillförlitlighet av data\\
Hur vet du om dina resultat är tillförlitliga? Hur bra är dina resultat?}


\section{Planned Data Analysis}
\label{sec:plannedDataAnalysis}
\sweExpl{Metod för analys av data}


\subsection{Data Analysis Technique}
\label{sec:dataAnalysisTechnique}
\sweExpl{Dataanalysteknik}

\subsection{Software Tools}
\label{sec:softwareTools}
\sweExpl{Mjukvaruverktyg}


\section{Evaluation framework}
\label{sec:evaluationFramework}
\sweExpl{Utvärdering och ramverk\\
Metod för utvärdering, jämförelse mm. Kopplar till kapitel~\ref{ch:resultsAndAnalysis}.}

\section{System documentation}
\label{sec:systemDocumentation}
\sweExpl{Systemdokumentation\\
Med vilka dokument och hur skall en konstruerad prototyp dokumenteras? Detta blir ofta bilagor till rapporten och det som problemägaren till det ursprungliga problemet (industrin) ofta vill ha.\\
Bland dessa bilagor återfinns ofta, och enligt någon angiven standard, kravdokument, arkitekturdokument, designdokumnet, implementationsdokument, driftsdokument, testprotokoll mm.}
\generalExpl{If this is going to be a complete document consider putting it in as an appendix, then just put the highlights here.}


\cleardoublepage
\chapter{What you did}\engExpl{Choose your own chapter title to describe this}
\label{ch:whatYouDid}
\sweExpl{[Vad gjorde du? Hur gick det till? – Välj lämplig rubrik (“Genomförande”, “Konstruktion”, ”Utveckling” eller annat]}


\engExpl{What have you done? How did you do it? What design decisions did you make? How did what you did help you to meet your goals?}
\sweExpl{Vad du har gjort? Hur gjorde du det? Vilka designval gjorde du?\\
Hur kom det du hjälpte dig att uppnå dina mål?}

% the following sets the TOC entry to break after the & - note you have to include the first letter of the following word as it get swolled by the \texorpdfstring{}{} processing
\section[Hardware/Software design …/Model/Simulation model \&\texorpdfstring{\\}{ p} parameters/…]{Hardware/Software design …/Model/Simulation model \& parameters/…}
\sweExpl{Hårdvara / Mjukvarudesign ... / modell / Simuleringsmodell och parametrar / …}

Figure~\ref{fig:homepageicon} shows a simple icon for a home page. The time
to access this page when served will be quantified in a series of
experiments. The configurations that have been tested in the test bed are
listed in Table~\ref{tab:configstested}. In \SI{7.0}{\percent} of cases, there was an error indicating xxxxx.

\sweExpl{Figur~\ref{fig:homepageicon} visar en enkel ikon för en hemsida. Tiden för att få tillgång till den här sidan när den laddas kommer att kvantifieras i en serie experiment. De konfigurationer som har testats i provbänk listas ini tabell~\ref{tab:configstested}.\\
Vad du har gjort? Hur gjorde du det? Vilka designval gjorde du?}
 
\begin{figure}[!ht]
 \begin{center}
  \includegraphics[width=0.25\textwidth]{figures/Homepage-icon.png}
 \end{center}
 \caption{Homepage icon}
 \label{fig:homepageicon}
\end{figure}

\begin{table}[!ht]
 \begin{center}
  \caption{Configurations tested}
  \label{tab:configstested}
  \resizebox{\columnwidth}{!}{%
  \begin{tabular}{l|c} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
 \textbf{Configuration} & \textbf{Description} \\
 \hline
 1 & Simple test with one server\\
 2 & Simple test with one server\\
  \end{tabular}
  }
 \end{center}
\end{table}
\sweExpl{Testade konfigurationer}

\section{Implementation …/Modeling/Simulation/…}
\label{sec:implementationDetails}
\sweExpl{Implementering … / modellering / simulering / …}

Two commonly used simulators are:
\begin{description}[labelwidth =\widthof{\textbf{ns-2 or ns-3 simulator}}, leftmargin = !]
  \item[\textbf{Mininet}] This simulator uses traffic control (\texttt{tc}) to simulate network devices connected by links with specific bandwidth, packet loss rates, qdisc methods, etc.
  
  
  \item[\textbf{ns-2 or ns-3 simulator}] These simulators are very useful for simulating wireless communication links between moving devices. You can specify the mobility patterns of the nodes.
\end{description}

\subsection{Some examples of coding}
\engExpl{This section is simply to show some example of how you can include code in your thesis - this is not a section you would have in your thesis.}
\sweExpl{Det här avsnittet är helt enkelt för att visa ett exempel på hur du kan inkludera kod i ditt examensarbete - det här är inte ett avsnitt du skulle ha i ditt examensarbete.}

Listing~\ref{lst:helloWorldInC} shows an example of a simple program written
in C code.

\begin{lstlisting}[language={C}, caption={Hello world in C code}, label=lst:helloWorldInC]
int main() {
printf("hello, world");
return 0;
}
\end{lstlisting}

\engExpl{This template uses the package \texttt{lstlistings} for many different listings. Alternatively, one could use the \texttt{minited} package together with the \texttt{listings} environment, see, for example, \href{https://www.overleaf.com/learn/latex/Code_Highlighting_with_minted}{Code Highlighting with minted}. }

In contrast, Listing~\ref{lst:programmes} is an example of code in Python to
get a list of all of the programs at KTH. Note that on {\DTMsetdatestyle{iso}\DTMdisplaydate{2025}{6}{1}{-1}} the KOPPS \gls{API} will no longer work.
\engExpl{Note that the change to the iso date format was done in a group so that afterwards the date style returns to what it was, resulting in \DTMdisplaydate{2025}{6}{1}{-1}.}

\lstset{extendedchars=true} %% This allows character codes in the range 128-255
\begin{lstlisting}[language={Python}, caption={Using a python program to
  access the KTH \gls{API} to get all of the programs at KTH}, label=lst:programmes]
KOPPSbaseUrl = 'https://www.kth.se'

def v1_get_programmes():
  global Verbose_Flag
  #
  # Use the KOPPS \gls{API} to get the data
  # note that this returns XML
  url = "{0}/api/kopps/v1/programme".format(KOPPSbaseUrl)
  if Verbose_Flag:
  print("url: " + url)
  #
  r = requests.get(url)
  if Verbose_Flag:
  print("result of getting v1 programme: {}".format(r.text))
  #
  if r.status_code == requests.codes.ok:
  return r.text  # simply return the XML
  #
  return None
\end{lstlisting}
\FloatBarrier

In \Cref{lst:exampleUsingMinted}, line \ref{listinline:ExcelWriter} shows the use of the \texttt{ExcelWriter} function. Line \ref{listinline:UsingToexcel} writes a panda dataframe (named \texttt{comp}) to the spreadsheet, while line \ref{listinline:closingWriter} closes the open spreadsheet.

\begin{listing}[!ht]
\begin{minted}[linenos,breaklines, escapeinside=||]{python}
import pandas as pd
def make_spreadsheet_of_differences(pds):
  global school
  publishers=set()
  writer = pd.ExcelWriter(f'/tmp/{school}_compare_duplicates.xlsx', engine='xlsxwriter') |\label{listinline:ExcelWriter}|
  print("starting")
  for idx, p in enumerate(pds):
  p0=list(p)[0]
  p1=list(p)[1]
  print(f"\nfor {p0} and {p1}")
  comp, publisher1, publisher2=compare_two_records_silent(p0, p1)
  sheet_name=f'{p0}'.split(':')[1]
  print(f'Wrote {sheet_name=}')|\label{listinline:UsingToexcel}|
  comp.to_excel(writer, sheet_name=sheet_name)
  publishers.add(publisher1)
  publishers.add(publisher2)
  
  # Close the Pandas Excel writer and output the Excel file.
  writer.close()|\label{listinline:closingWriter}|
  print(f"{publishers=}")
  return publishers
\end{minted}
\caption{Example of using \texttt{minted} with python code}
\label{lst:exampleUsingMinted}
\end{listing}

\FloatBarrier


\subsection{Some examples of figures in tikz}
\engExpl{This section is simply to show some example of how you can draw your own figures for in your thesis - this is not a section you would have in your thesis.}
\sweExpl{Det här avsnittet är helt enkelt för att visa ett exempel på hur du kan rita dina egna figurer i ditt examensarbete – det här är inte ett avsnitt du skulle ha i ditt examensarbete.}

\Needspace*{5\baselineskip}
These figures are just some examples to show that you can draw your own figures for your thesis. This has two advantages: \first you do not have to worry about copyrights -- as these are your own figures and \Second the text is now readable and not simply a picture of text -- so screen readers can read the figure's contents to someone who is listening to the contents of your thesis.

\subsubsection{Azure's Form Recognizer}
\Cref{fig:processAnInvoice} shows the processing of key-value extraction from a PDF document using Azure's Form Recognizer. 

\tikzset{
  processBox/.style={rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, font=\sffamily, draw=black, fill=red!20},
  largeBox/.style={rectangle, rounded corners, minimum width=3cm, minimum height=4cm,text centered, draw=black}
}
\begin{figure}[!ht]
\resizebox{1.1\textwidth}{!}{%
\begin{tikzpicture}
[align=left,node distance=2cm]

\node (document) [tape,tape bend top=none,draw,font=\sffamily] {PDF\\Document};
\node (GDM) [processBox, right=0.5cm of document] {OCR};
\node (OCRoutput) [largeBox, right=1cm of GDM] {OCR output};

\node (kvp) [tape,tape bend top=none,draw,font=\sffamily, below=0.25cm of OCRoutput.north] {key-value\\pairs};
\node (entities) [tape,tape bend top=none,draw,font=\sffamily, above=0.35cm of OCRoutput.south] {Entities};
\node (Manual) [processBox, right=1cm of kvp] {Analyze the extracted\\key-value pairs};
\draw [-latex](document) -- (GDM);
\draw [-latex](kvp) -- (Manual);
\path[ draw
 , -latex'] let \p1=(GDM.east), \p2=(kvp.west) in (GDM.east) -- +(0.25*\x2-0.25*\x1, \y1) -- +(0.5*\x2-0.5*\x1, \y2) -- (kvp.west);
\path[ draw
 , -latex'] let \p1=(GDM.east), \p2=(kvp.west), \p3=(entities.west) in (GDM.east) -- +(0.25*\x2-0.25*\x1, \y1) -- +(0.5*\x3-0.5*\x1, \y3) -- (entities.west);
\end{tikzpicture}
}
\caption{The processing of key-value extraction from a PDF document using Azure's Form Recognizer}
 \label{fig:processAnInvoice}
\end{figure}
\FloatBarrier
\subsubsection{Hyper-V with Containers}
 \Cref{fig:hyperVcontainers} shows how Hyper-V deals with containers.
 
 \tikzset{
  container/.style={rectangle, rounded corners, minimum width=2cm, minimum height=1cm,text centered, draw=black, fill=blue!20},
  containerization/.style={rectangle, rounded corners, minimum width=13.25cm, minimum height=1cm,text centered, draw=black, fill=blue!20},
  hypervisor/.style={rectangle, rounded corners, minimum width=13.25cm, minimum height=1cm,text centered, draw=black, fill=red!20},
  os/.style={rectangle, rounded corners, minimum width=13.25cm, minimum height=1cm,text centered, draw=black, fill=orange!20},
  guestos/.style={rectangle, rounded corners, minimum width=2cm, minimum height=1cm,text centered, draw=black, fill=orange!40},
  infrastructure/.style={rectangle, rounded corners, minimum width=13.25cm, minimum height=1cm,text centered, draw=black, fill=green!20},
  hos/.style={rectangle, rounded corners, minimum width=6cm, minimum height=1cm,text centered, draw=black, fill=orange!20},
  kernel/.style={rectangle, rounded corners, minimum width=6cm, minimum height=1cm,text centered, draw=black, fill=purple!20},
  services/.style={rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, draw=black, fill=pink!20]}
}

\begin{figure}[ht!]
  \centering
\resizebox{1\textwidth}{!}{%
\begin{tikzpicture}
[align=center,node distance=2cm]

\node (Infrastructure) [infrastructure, text width=13cm, text centered] {Infrastructure};
\node (OS1) [hos, anchor=north west, align=left, above=1.5cm of Infrastructure.north west, anchor=north west, text width=6cm, text centered] {Host OS};

\node (OS2) [hos, anchor= west, align=left, right=0.5cm of OS1.east, text width=6cm, anchor= west, text centered] {Host OS};

\node (Kernel1) [kernel, anchor=north west, align=left, above=1.5cm of OS1.north east, anchor=north east, text width=3cm, text centered] {Kernel};

\node (Kernel2) [kernel, anchor=north west, align=left, above=1.5cm of OS2.north east, anchor=north east, text width=3cm, text centered] {Kernel};

\node (ServiceA) [container, anchor=east, above=1 cm of Kernel1.east, anchor=east] {Services};
\node (AppA) [container, left=0.25cm of ServiceA] {App 1};

\node (ServiceB) [container, anchor=east, above=1 cm of Kernel2.east, anchor=east] {Services};
\node (AppB) [container, left=0.25cm of ServiceB] {App 2};
%\node (AppC) [container, right=0.25cm of AppB] {App 3};

\draw[black,thick,dashed] ($(OS2.north west)+(-0.3,3.75)$) rectangle ($(OS2.south east)+(0.5,-0.3)$);
\node[text width=5cm, text=red, above=0.1cm of ServiceB] 
  {\textbf{Container}};

\draw[red,thick,dotted] ($(Kernel2.north west)+(-0.3,1.6)$) rectangle ($(Kernel2.south east)+(0.3,-0.3)$);
\node[text width=5cm, text=black, above=0.8cm of ServiceB] 
  {\textbf{VM}};
\end{tikzpicture}
}
  \caption{Hyper-V with containers}
  \label{fig:hyperVcontainers}
\end{figure}
\FloatBarrier
\subsubsection{\glsfmtshort{VM} versus Containers}
\Cref{fg:vmsVersusContainers} shows a comparison of virtual machines (VMs) versus containers.

\begin{figure*}[ht!]
  \centering
  \begin{subfigure}[t]{0.5\textwidth}
  \centering
\resizebox{1\textwidth}{!}{%
\begin{tikzpicture}
[align=left,node distance=2cm]

\node (AppA) [container,align=left] {App 1};
\node (AppB) [container, right=0.25cm of AppA] {App 2};
\node (AppC) [container, right=0.25cm of AppB] {App 3};

\node (GosA) [guestos,align=left, below=0.25cm of AppA.south west,anchor=north west] {Guest OS};
\node (GosB) [guestos, right=0.25cm of GosA] {Guest OS};
\node (GosC) [guestos, right=0.25cm of GosB] {Guest OS};

\draw [decoration={brace,amplitude=0.5em},decorate, ultra thick,gray, transform canvas={xshift = 0.5cm}]
  (AppC.north -| AppC.east) -- (GosC.south -| AppC.east);
\node[text width=5cm, right=1cm of GosC.north east] 
  {\textbf{VMs}};

\node (Hypervisor) [hypervisor, anchor=north west, align=left, below=0.25cm of GosA.south west, anchor=north west, text width=13cm, text centered] {Hypervisor};

\node (OS) [os, anchor=north west, align=left, below=0.25cm of Hypervisor.south west, anchor=north west, text width=13cm, text centered] {Host OS};

\node (Infrastructure) [infrastructure, anchor=north west, align=left, below=0.25cm of OS.south west, anchor=north west, text width=13cm, text centered] {Infrastructure};


\end{tikzpicture}
}
  \caption{VM}
  \end{subfigure}%
  ~ 
  \begin{subfigure}[t]{0.5\textwidth}
  \centering
  \resizebox{1\textwidth}{!}{%
\begin{tikzpicture}
[align=left,node distance=2cm]

\node (AppA) [container,align=left] {App 1};
\node (AppB) [container, right=0.25cm of AppA] {App 2};
\node (AppC) [container, right=0.25cm of AppB] {App 3};
\node[text width=5cm, right=0.25cm of AppC] 
  {\textbf{Apps running in Containers}};


\node (Containerization) [containerization, anchor=north west, align=left, below=0.25cm of AppA.south west, anchor=north west, text width=13cm, text centered] {Docker Engine};

\node (OS) [os, anchor=north west, align=left, below=0.25cm of Containerization.south west, anchor=north west, text width=13cm, text centered] {Host OS};

\node (Infrastructure) [infrastructure, anchor=north west, align=left, below=0.25cm of OS.south west, anchor=north west, text width=13cm, text centered] {Infrastructure};


\end{tikzpicture}
}
  \caption{Containers}
  \end{subfigure}
  \caption{Virtual machines (VMs) versus Containers}
  \label{fg:vmsVersusContainers}
\end{figure*}

\cleardoublepage
\chapter{Results and Analysis}
\label{ch:resultsAndAnalysis}
\sweExpl{svensk: Resultat och Analys}

\engExpl{Sometimes this is split into two chapters.\\Keep in mind: How you are going to evaluate what you have done? What are your metrics?\\Analysis of your data and proposed solution\\Does this meet the goals which you had when you started?}

In this chapter, we present the results and discuss them.

\sweExpl{I detta kapitel presenterar vi resultaten och diskutera dem.\\Ibland delas detta upp i två kapitel.\\Hur du ska utvärdera vad du har gjort? Vad är din statistik?\\Analys av data och föreslagen lösning\\Innebär detta att uppfyllelse av de mål som du hade när du började?}

\section{Major results}
\sweExpl{Huvudsakliga resultat}

Some statistics of the delay measurements are shown in Table~\ref{tab:delayMeasurements}.
The delay has been computed from the time the GET request is received until the response is sent.

\sweExpl{Lite statistik av fördröjningsmätningarna visas i Tabell~\ref{tab:delayMeasurements}. Förseningen har beräknats från den tidpunkt då begäran GET tas emot fram till svaret skickas.}

\begin{table}[!ht]
 \begin{center}
  \caption{Delay measurement statistics}
  \label{tab:delayMeasurements}
  \begin{tabular}{l|S[table-format=4.2]|S[table-format=3.2]} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
 \textbf{Configuration} & \textbf{Average delay (ns)} & \textbf{Median delay (ns)}\\
 \hline
 1 & 467.35 & 450.10\\
 2 & 1687.5 & 901.23\\
  \end{tabular}
 \end{center}
\end{table}

Table \ref{tab:ping_results} shows the measurement of round trip times from four hosts to and from a server.
\begin{table}[ht!]
\caption[RTT for 4 hosts]{Result for the ping measurements of RTT for 4 hosts} 
\label{tab:ping_results}
\vspace{1em}
\centering
\begin{tabular}{l *{4}{S[table-format=2.3]}}
{} & \multicolumn{4}{c}{host to server RTT in ms} \\
\cmidrule{2-5}
Host & \multicolumn{1}{c}{min} & \multicolumn{1}{c}{avg} & \multicolumn{1}{c}{max} & \multicolumn{1}{c}{mdev} \\
\midrule
h1 & 5.625 & 5.625 & 5.625 & 0.0 \\
h2 & 2.909 & 2.909 & 1.909 & 0.0 \\
h3 & 5.007 & 5.007 & 5.007 & 0.0 \\
h4 & 2.308 & 2.308 & 2.308 & 0.0 \\
\midrule
\end{tabular}
\end{table}
\FloatBarrier

\sweExpl{Fördröj mätstatistik}
\sweExpl{Konfiguration | Genomsnittlig fördröjning (ns) | Median fördröjning (ns)}

Figure \ref{fig:processing_vs_payload_length} shows an example of the performance as measured in the experiments.

\begin{figure}[!ht]
% GNUPLOT: LaTeX picture
\setlength{\unitlength}{0.240900pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi
\begin{picture}(1500,900)(0,0)
\sbox{\plotpoint}{\rule[-0.200pt]{0.400pt}{0.400pt}}%
\put(171.0,131.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(151,131){\makebox(0,0)[r]{ 1.5}}
\put(1419.0,131.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(171.0,212.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(151,212){\makebox(0,0)[r]{ 2}}
\put(1419.0,212.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(171.0,292.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(151,292){\makebox(0,0)[r]{ 2.5}}
\put(1419.0,292.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(171.0,373.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(151,373){\makebox(0,0)[r]{ 3}}
\put(1419.0,373.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(171.0,454.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(151,454){\makebox(0,0)[r]{ 3.5}}
\put(1419.0,454.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(171.0,534.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(151,534){\makebox(0,0)[r]{ 4}}
\put(1419.0,534.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(171.0,615.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(151,615){\makebox(0,0)[r]{ 4.5}}
\put(1419.0,615.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(171.0,695.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(151,695){\makebox(0,0)[r]{ 5}}
\put(1419.0,695.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(171.0,776.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(151,776){\makebox(0,0)[r]{ 5.5}}
\put(1419.0,776.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(171.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(171,90){\makebox(0,0){ 0}}
\put(171.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(298.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(298,90){\makebox(0,0){ 10}}
\put(298.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(425.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(425,90){\makebox(0,0){ 20}}
\put(425.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(551.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(551,90){\makebox(0,0){ 30}}
\put(551.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(678.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(678,90){\makebox(0,0){ 40}}
\put(678.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(805.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(805,90){\makebox(0,0){ 50}}
\put(805.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(932.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(932,90){\makebox(0,0){ 60}}
\put(932.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1059.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1059,90){\makebox(0,0){ 70}}
\put(1059.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1185.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1185,90){\makebox(0,0){ 80}}
\put(1185.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1312.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1312,90){\makebox(0,0){ 90}}
\put(1312.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1439.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1439,90){\makebox(0,0){ 100}}
\put(1439.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(171.0,131.0){\rule[-0.200pt]{0.400pt}{155.380pt}}
\put(171.0,131.0){\rule[-0.200pt]{305.461pt}{0.400pt}}
\put(1439.0,131.0){\rule[-0.200pt]{0.400pt}{155.380pt}}
\put(171.0,776.0){\rule[-0.200pt]{305.461pt}{0.400pt}}
\put(30,453){\rotatebox{-270}{\makebox(0,0){Processing time (ms)}}}
\put(805,29){\makebox(0,0){Payload size (bytes)}}
\put(868.0,131.0){\rule[-0.200pt]{0.400pt}{84.074pt}}
\put(995.0,131.0){\rule[-0.200pt]{0.400pt}{98.287pt}}
\put(1173.0,131.0){\rule[-0.200pt]{0.400pt}{118.041pt}}
\put(1325.0,131.0){\rule[-0.200pt]{0.400pt}{134.904pt}}
\put(1350.0,131.0){\rule[-0.200pt]{0.400pt}{137.795pt}}
\put(1439.0,131.0){\rule[-0.200pt]{0.400pt}{155.380pt}}
\end{picture}
\caption[A GNUplot figure]{Processing time vs. payload length}\vspace{0.5cm}
\label{fig:processing_vs_payload_length}
\end{figure}
\FloatBarrier		

Given these measurements, we can calculate our processing bit rate as the inverse of the time it takes to process an additional byte divided by 8 bits per byte:

\[
	\text{bit rate} = \frac{1}{\frac{\text{time}_{\text{byte}}}{8}} = 20.03 \quad kb/s
\] 

\Cref{tab:majorMarkupLMDetailedResult} shows another table in which some values have been set in bold (using \textbackslash B) to emphasize them. Note how the \texttt{S} formatting has been modified so that it considers the weight of the characters and this is able to decimal align even these hold-faced numbers with the numbers in the column above them.

\begin{table}[!ht]
  \centering
  \caption{Median values of sandwich attributes}
  \label{tab:majorMarkupLMDetailedResult}
  \begin{tabular}{l *{2}{S[detect-weight,mode=text,table-format=3.2]}}
  & \multicolumn{2}{c}{\textbf{sites}}\\
  \cmidrule{2-3}
  \textbf{Attribute} & \textbf{A} & \textbf{B} \\
  \midrule
  price (in SEK) & 36.5 & 71.3 \\
  protean (g) & 97.2 & 100.0 \\
  salt (mg) & 9.7 & 9.3 \\
  \hline
  \textbf{Average customer rating in \%} & \B 82.2 & \B 89.9 \\
  \midrule
  \end{tabular}
\end{table}
\FloatBarrier


\Needspace*{4\baselineskip}
\Cref{fig:stackedrust} shows a stacked bar chart using pgfplots. It illustrates how easy it is to take a set of data and make a stacked bar plot. One of the features is the shifted values -- this is very useful when the bar itself is too small to put the value into.

\pgfplotstableread{
Label Numbers Refs Struct/Enum Heap Arrays
cratesio 70.04 19.83 8.31 1.3 0.52
librs 49.26 30.49 10.80 7.92 1.53
rustc 55.01 24.80 11.54 6.16 2.49
}\testdata


\pgfkeys{
  /pgf/number format/.cd,
  fixed,
  fixed zerofill,
  precision=2
}
\begin{figure}[ht!]
  \centering
  \scalebox{0.9}{
  \begin{tikzpicture}
  \begin{axis}[
  ybar stacked,
  %reverse legend,
  reverse legend=false,
  %https://tex.stackexchange.com/questions/88892/pgfplots-bar-plot-spacing-inbetween-bars
  enlarge x limits=0.4,
	  bar width=45pt,
  /pgfplots/nodes near coords*/.append style={
  every node near coord/.style={
  color=black,
  font=\small,
  name=X,
%  shift={  
%    (50pt,25pt)
%    },
  xshift={50pt},
    yshift ={
    ifthenelse((\plotnum == 4), 30pt,20pt)},
  },
  scatter/@post marker code/.append code={
    \node(Y){};
    \draw(X)--(Y.center);
  }
  },
	  nodes near coords,
  bar shift=5pt,
  ymin=0,
  ymax=115,
  xtick=data,
  width=1\textwidth,
  legend style={draw=none},
  legend image post style={scale=2.0},
  legend style={
  at={(0.5,-0.2)},
  anchor=north,
  legend columns=-2,
  font=\large,
  %mark size=20pt,
  },
  ylabel=Percentage points (\%),
  xticklabels from table={\testdata}{Label},
  xticklabel style={rotate=30},
  ]
  \addplot table [y=Numbers, meta=Label, x expr=\coordindex] {\testdata};
  \addlegendentry{Numbers}
  \addplot table [y=Refs, meta=Label, x expr=\coordindex] {\testdata};
  \addlegendentry{Refs}
  \addplot table [y=Struct/Enum, meta=Label, x expr=\coordindex] {\testdata};
  \addlegendentry{Struct/Enum}
  \addplot table [y=Heap, meta=Label, x expr=\coordindex] {\testdata};
  \addlegendentry{Heap}
  \addplot table [y=Arrays, meta=Label, x expr=\coordindex] {\testdata};
  \addlegendentry{Arrays}
  \end{axis}
  \end{tikzpicture}}
\caption{Rust types distribution for the compiler, crates.io, and lib.rs.
(percentage) - appears here with the permission of the author - see the thesis at \url{https://urn.kb.se/resolve?urn=urn\%3Anbn\%3Ase\%3Akth\%3Adiva-332124}}
\label{fig:stackedrust}
\end{figure}
\FloatBarrier



\section{Reliability Analysis}
\sweExpl{Analys av tillförlitlighet\\
Tillförlitlighet i metod och data}

\section{Validity Analysis}
\sweExpl{Analys av validitet\\
Validitet i metod och data}

\cleardoublepage
\chapter{Discussion}
\label{ch:discussion}
\sweExpl{Diskussion\\
Förbättringsförslag?}
\generalExpl{This can be a separate chapter or a section in the previous chapter.}

\cleardoublepage
\chapter{Conclusions and Future work}
\label{ch:conclusionsAndFutureWork}
\sweExpl{Slutsats och framtida arbete}

\generalExpl{Add text to introduce the subsections of this chapter.}

\section{Conclusions}
\label{sec:conclusions}
\sweExpl{Slutsatser}
\engExpl{Describe the conclusions (reflect on the whole introduction given in Chapter 1).}


 
\engExpl{Discuss the positive effects and the drawbacks.\\
Describe the evaluation of the results of the degree project.\\
Did you meet your goals?\\
What insights have you gained?\\
What suggestions can you give to others working in this area?\\
If you had it to do again, what would you have done differently?}

\sweExpl{Uppfyllde du dina mål?\\
Vilka insikter har du fått?\\
Vilka förslag kan du ge till andra som arbetar inom detta område?
Om du skulle göra detta igen, vad skulle du ha gjort annorlunda?}

\section{Limitations}
\label{sec:limitations}
\sweExpl{Begränsande faktorer\\Vad gjorde du som begränsade dina ansträngningar? Vilka är begränsningarna i dina resultat?}
\engExpl{What did you find that limited your efforts? What are the limitations of your results?}


\section{Future work}
\label{sec:futureWork}
\sweExpl{Vad du har kvar ogjort?\\Vad är nästa självklara saker som ska göras?\\Vad tips kan du ge till nästa person som kommer att följa upp på ditt arbete?}
\engExpl{Describe valid future work that you or someone else could or should do.\\
Consider: What you have left undone? What are the next obvious things to be done? What hints can you give to the next person who is going to follow up on your work?}



Due to the breadth of the problem, only some of the initial goals have been
met. In these section we will focus on some of the remaining issues that
should be addressed in future work. ...

\subsection{What has been left undone?}
\label{what-has-been-left-undone}

The prototype does not address the third requirment, \ie a yearly unavailability of less than 3 minutes; this remains an open problem. ...

\subsubsection{Cost analysis}
\generalExpl{Example of a missing component}
The current prototype works, but the performance from a cost perspective makes this an impractical solution. Future work must reduce the cost of this solution; to do so, a cost analysis needs to first be done. ...

\subsubsection{Security}
\generalExpl{Example of a missing component}
A future research effort is needed to address the security holes that results from using a self-signed certificate. Page filling text mass. Page filling text mass. ...


\subsection{Next obvious things to be done}

In particular, the author of this thesis wishes to point out xxxxxx remains as a problem to be solved. Solving this problem is the next thing that should be done. ...

\section{Reflections}
\label{sec:reflections}
\sweExpl{Reflektioner}
\sweExpl{Vilka är de relevanta ekonomiska, sociala, miljömässiga och etiska aspekter av ditt arbete?}
\engExpl{What are the relevant economic, social,
 environmental, and ethical aspects of your work?
}



One of the most important results is the reduction in the amount of
energy required to process each packet while at the same time reducing the
time required to process each packet.

The thesis contributes to the \gls{UN}\enspace\glspl{SDG} numbers 1 and 9 by
xxxx. 




\noindent\rule{\textwidth}{0.4mm}
\engExpl{In the references, let Zotero or other tool fill this in for you. I suggest an extended version of the IEEE style, to include URLs, DOIs, ISBNs, etc., to make it easier for your reader to find them. This will make life easier for your opponents and examiner. \\IEEE Editorial Style Manual: \url{https://www.ieee.org/content/dam/ieee-org/ieee/web/org/conferences/style_references_manual.pdf}}
\sweExpl{Låt Zotero eller annat verktyg fylla i det här för dig. Jag föreslår en utökad version av IEEE stil - att inkludera webbadresser, DOI, ISBN osv. - för att göra det lättare för läsaren att hitta dem. Detta kommer att göra livet lättare för dina opponenter och examinator.}

\cleardoublepage
% Print the bibliography (and make it appear in the table of contents)
\renewcommand{\bibname}{References}


\ifbiblatex
  %\typeout{Biblatex current language is \currentlang}
  \printbibliography[heading=bibintoc]
\else
  \phantomsection % make it include a hyperref - see https://tex.stackexchange.com/a/98995
  \addcontentsline{toc}{chapter}{References}
  \bibliography{references}
\fi



\warningExpl{If you do not have an appendix, do not include the \textbackslash cleardoublepage command below; otherwise, the last page number in the metadata will be one too large.}
\cleardoublepage
\appendix
\renewcommand{\chaptermark}[1]{\markboth{Appendix \thechapter\relax:\thinspace\relax#1}{}}
\chapter{Supporting materials}
\label{sec:supportingMaterial}
\generalExpl{Here is a place to add supporting material that can help others build upon your work. You can include files as attachments to the PDF file or indirectly via URLs. Alternatively, consider adding supporting material uploaded as separate files in DiVA.}

% Attach the BibTeX for your references to make it easy for a reader to find and use them
The BibTeX references used in this thesis are attached. \attachfile[description={references.bib}]{references.bib}

% Attach source code file(s) or add a URL to the github or other repository
Some source code relevant to this project can be found at \url{https://github.com/gqmaguirejr/E-learning} and \url{https://github.com/gqmaguirejr/Canvas-tools}.

Your reader can access the attached (embedded) files using a PDF tool such as Adobe Acrobat Reader using the paperclip icon in the left menu, as shown in \Cref{fig:PDFreaderPaperclipExample} or by right-clicking on the push-pin icon in the PDF file and then using the menu to save the embedded file as shown in \Cref{fig:PDFreaderPushpinExample}.

An argument for including supporting material in the PDF file is that it will be available to anyone who has a copy of the PDF file. As a result, they do not have to look elsewhere for this material. This comes at the cost of a larger PDF file. However, the embedded files are encoded into a compressed stream within the PDF file; thus, reducing the number of additional bytes. For example, the references.bib file that was used in this example is \SI{10617}{\byte} in size but only occupies \SI{4261}{\byte} in the PDF file.

\warningExpl{DiVA is limited to $\approx$\SI{1}{\giga\byte} for each supporting file. If you have very large amounts of supporting material, you will probably want to use one of the data repositories. For additional help with this, contact KTH Library via 
\href{mailto:researchdata@kth.se}{researchdata@kth.se}.\\As of Spring 2024, there are plans to migrate this supporting data from DiVA to a research data repository.
}

\begin{figure}[!ht]
 \begin{center}
  \includegraphics[width=0.50\textwidth]{README_notes/pdf-viewer-attached-files.png}
 \end{center}
 \caption{Adobe Acrobat Reader using the paperclip icon for the attached references.bib file}
 \label{fig:PDFreaderPaperclipExample}
\end{figure}
\FloatBarrier

\begin{figure}[!ht]
 \begin{center}
  \includegraphics[width=0.99\textwidth]{README_notes/Bib-save-embedded-example.png}
 \end{center}
 \caption{Adobe Acrobat Reader after right-clicking on the push-pin icon for the attached references.bib file}
 \label{fig:PDFreaderPushpinExample}
\end{figure}
\FloatBarrier
\cleardoublepage

\chapter{Something Extra}
\sweExpl{svensk: Extra Material som Bilaga}

\section{Just for testing KTH colors}
\ifdigitaloutput
  \textbf{You have selected to optimize for digital output}
\else
  \textbf{You have selected to optimize for print output}
\fi
\begin{itemize}[noitemsep]
  \item Primary color
  \begin{itemize}
  \item \textcolor{kth-blue}{kth-blue \ifdigitaloutput
  actually Deep sea
  \fi} {\color{kth-blue} \rule{0.3\linewidth}{1mm} }\\

  \item \textcolor{kth-blue80}{kth-blue80} {\color{kth-blue80} \rule{0.3\linewidth}{1mm} }\\
\end{itemize}

\item Secondary colors
\begin{itemize}[noitemsep]
  \item \textcolor{kth-lightblue}{kth-lightblue \ifdigitaloutput
  actually Stratosphere
  \fi} {\color{kth-lightblue} \rule{0.3\linewidth}{1mm} }\\

  \item \textcolor{kth-lightred}{kth-lightred \ifdigitaloutput
  actually Fluorescence\fi} {\color{kth-lightred} \rule{0.3\linewidth}{1mm} }\\

  \item \textcolor{kth-lightred80}{kth-lightred80} {\color{kth-lightred80} \rule{0.3\linewidth}{1mm} }\\

  \item \textcolor{kth-lightgreen}{kth-lightgreen \ifdigitaloutput
  actually Front-lawn\fi} {\color{kth-lightgreen} \rule{0.3\linewidth}{1mm} }\\

  \item \textcolor{kth-coolgray}{kth-coolgray \ifdigitaloutput
  actually Office\fi} {\color{kth-coolgray} \rule{0.3\linewidth}{1mm} }\\

  \item \textcolor{kth-coolgray80}{kth-coolgray80} {\color{kth-coolgray80} \rule{0.3\linewidth}{1mm} }
\end{itemize}
\end{itemize}

\textcolor{black}{black} {\color{black} \rule{\linewidth}{1mm} }

% Include an example of using nomenclature
\ifnomenclature
  \cleardoublepage
  \chapter{Main equations}
  \label{ch:NomenclatureExamples}
  This appendix gives some examples of equations that are used throughout this thesis.
  \section{A simple example}
  The following example is adapted from Figure 1 of the documentation for the package nomencl (\url{https://ctan.org/pkg/nomencl}).
  \begin{equation}\label{eq:mainEq}
  a=\frac{N}{A}
  \end{equation}
  \nomenclature{$a$}{The number of angels per unit area\nomrefeq}%  %% include the equation number in the list
  \nomenclature{$N$}{The number of angels per needle point\nomrefpage}% %% include the page number in the list
  \nomenclature{$A$}{The area of the needle point}%
  The equation $\sigma = m a$%
  \nomenclature{$\sigma$}{The total mass of angels per unit area\nomrefeqpage}%
  \nomenclature{$m$}{The mass of one angel}
follows easily from \Cref{eq:mainEq}.

  \section{An even simpler example}
  The formula for the diameter of a circle is shown in \Cref{eq:secondEq} area of a circle in \cref{eq:thirdEq}.
  \begin{equation}\label{eq:secondEq}
  D_{circle}=2\pi r
  \end{equation}
  \nomenclature{$D_{circle}$}{The diameter of a circle\nomrefeqpage}%
  \nomenclature{$r$}{The radius of a circle\nomrefeqpage}%

  \begin{equation}\label{eq:thirdEq}
  A_{circle}=\pi r^2
  \end{equation}
  \nomenclature{$A_{circle}$}{The area of a circle\nomrefeqpage}%

  Some more text that refers to \eqref{eq:thirdEq}.
\fi %% end of nomenclature example

\cleardoublepage
% Information for authors
%\include{README_author}
\subfile{README_author}

\cleardoublepage
% information about the template for everyone
\input{README_notes/README_notes}

\begin{comment}
% information for examiners
\ifxeorlua
\cleardoublepage
\input{README_notes/README_examiner_notes}
\fi
\end{comment}

\begin{comment}
% Information for administrators
\ifxeorlua
\cleardoublepage
\input{README_notes/README_for_administrators.tex}
\fi
\end{comment}

\begin{comment}
% Information for Course Coordinators
\ifxeorlua
\cleardoublepage
\input{README_notes/README_for_course_coordinators}
\fi
\end{comment}

%% The following label is necessary for computing the last page number of the body of the report to include in the "For DIVA" information
\label{pg:lastPageofMainmatter}

\cleardoublepage
\clearpage\thispagestyle{empty}\mbox{} % empty page with backcover on the other side
\kthbackcover
\fancyhead{} % Do not use header on this extra page or pages
\section*{€€€€ For DIVA €€€€}
\lstset{numbers=none} %% remove any list line numbering
\divainfo{pg:lastPageofPreface}{pg:lastPageofMainmatter}

% If there is an acronyms.tex file,
% add it to the end of the For DIVA information
% so that it can be used with the abstracts
% Note that the option "nolol" stops it from being listed in the List of Listings

% The following bit of ugliness is because of the problems PDFLaTeX has handling a non-breaking hyphen
% unless it is converted to UTF-8 encoding.
% If you do not use such characters in your acronyms, this could be simplified.
\ifxeorlua
\IfFileExists{lib/acronyms.tex}{
\section*{acronyms.tex}
\lstinputlisting[language={[LaTeX]TeX}, nolol, basicstyle=\ttfamily\color{black},
commentstyle=\color{black}, backgroundcolor=\color{white}]{lib/acronyms.tex}
}
{}
\else
\IfFileExists{lib/acronyms-for-pdflatex.tex}{
\section*{acronyms.tex}
\lstinputlisting[language={[LaTeX]TeX}, nolol, basicstyle=\ttfamily\color{black},
commentstyle=\color{black}, backgroundcolor=\color{white}]{lib/acronyms-for-pdflatex.tex}
}
{}
\fi


\end{document}
