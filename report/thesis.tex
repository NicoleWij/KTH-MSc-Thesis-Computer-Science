\RequirePackage{ifxetex}
\RequirePackage{ifluatex}
\newif\ifxeorlua
\ifxetex\xeorluatrue\fi
\ifluatex\xeorluatrue\fi

\ifxeorlua

\RequirePackage{expl3}
\ExplSyntaxOn

\ExplSyntaxOff
\else
\RequirePackage{expl3}
\ExplSyntaxOn
%\pdf_version_gset:n{2.0}
\pdf_version_gset:n{1.5}
\ExplSyntaxOff
\fi

\makeatletter
\newcommand{\disablepackage}[2]{%
 \disable@package@load{#1}{#2}%
}
\newcommand{\reenablepackage}[1]{%
 \reenable@package@load{#1}%
}
\makeatother
\ifxeorlua
\disablepackage{transparent}{}
\fi

\documentclass[nomenclature, english, biblatex]{kththesis}

\newcommand*{\generalExpl}[1]{\todo[inline]{#1}} 

\newcommand*{\engExpl}[1]{\todo[inline, backgroundcolor=kth-lightgreen40]{#1}}
\newcommand*{\sweExpl}[1]{\todo[inline, backgroundcolor=kth-lightblue40]{#1}}

\newcommand*{\warningExpl}[1]{\todo[inline, backgroundcolor=kth-lightred40]{#1}}


\usepackage[
 backend=biber,
 style=ieee,
 citestyle=numeric-comp,
 sorting=none
]{biblatex}
\addbibresource{references.bib}

\input{lib/includes}
\input{lib/kthcolors}

\input{lib/defines}
\ExplSyntaxOn
\newcommand\typestoredx[2]{\expandafter\__scontents_typestored_internal:nn\expandafter{#1} {#2}}
\ExplSyntaxOff
\makeatletter
\let\verbatimsc\@undefined
\let\endverbatimsc\@undefined
\lst@AddToHook{Init}{\hyphenpenalty=50\relax}
\makeatother


\lstnewenvironment{verbatimsc}
 {
 \lstset{%
 basicstyle=\ttfamily\tiny,
 backgroundcolor=\color{white},
 columns=[l]fixed,
 language=[LaTeX]TeX,
 keywordstyle=\color{red},
 breaklines=true,
 breakatwhitespace=true,
 breakindent=0em,
 frame=none,
 postbreak={}
 }
}{}

\lstdefinestyle{[LaTeX]TeX}{
morekeywords={begin, todo, textbf, textit, texttt}
}

\newcommand{\colorbitbox}[3]{%
	\rlap{\bitbox{#2}{\color{#1}\rule{\width}{\height}}}%
	\bitbox{#2}{#3}}



\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}

\ifbiblatex
 \usepackage[plainpages=false]{hyperref}
\else
 \usepackage[
 backref=page,
 pagebackref=false,
 plainpages=false,
 unicode=true,
 bookmarks=true,
 bookmarksopen=false,
 pdfpagemode=UseNone,
 destlabel,
 pdfencoding=auto, 
 ]{hyperref}
 \makeatletter
 \ltx@ifpackageloaded{attachfile2}{
 }
 {\usepackage{backref}
 \renewcommand*{\backref}[1]{}
 \renewcommand*{\backrefalt}[4]{%
 \ifcase #1%
 \or [Page~#2.]%
 \else [Pages~#2.]%
 \fi%
 }
 }
 \makeatother

\fi
\usepackage[all]{hypcap}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{threeparttable}
\usepackage[acronym, style=super, section=section, nonumberlist, nomain,
nopostdot]{glossaries}
\setlength{\glsdescwidth}{0.75\textwidth}
\usepackage[]{glossaries-extra}
\usepackage{dirtytalk}
\ifinswedish
 %\usepackage{glossaries-swedish}
\fi

\newglossary[tlg]{readme}{tld}{tdn}{README acronyms}


\input{lib/includes-after-hyperref}

\makeglossaries
\ifxeorlua
\input{lib/acronyms}
\else
\input{lib/acronyms-for-pdflatex}
\fi

\input{custom_configuration}

\title{Evaluating Backend Architectures for Real-Time API-Dependent Applications}
\subtitle{A Comparative Study}

% give the alternative title - i.e., if the thesis is in English, then give a Swedish title
\alttitle{Utvärdering av backend-arkitekturer för realtidsapplikationer beroende av externa API:er}
\altsubtitle{En jämförande studie}

% Enter the English and Swedish keywords here for use in the PDF metadata _and_ for later use
% following the respective abstract.
% Try to put the words in the same order in both languages to facilitate matching. For example:
\EnglishKeywords{Canvas Learning Management System, Docker containers, Performance tuning}
\SwedishKeywords{Canvas Lärplattform, Dockerbehållare, Prestandajustering}

%%%%% For the oral presentation
%% Add this information once your examiner has scheduled your oral presentation
\presentationDateAndTimeISO{2022-03-15 13:00}
\presentationLanguage{eng}
\presentationRoom{via Zoom https://kth-se.zoom.us/j/ddddddddddd}
\presentationAddress{Isafjordsgatan 22 (Kistagången 16)}
\presentationCity{Stockholm}

% When there are multiple opponents, separate their names with '\&'
% Opponent's information
\opponentsNames{A. B. Normal \& A. X. E. Normalè}

% Once a thesis is approved by the examiner, add the TRITA number
% The TRITA number for a thesis consists of two parts: a series (unique to each school)
% and the number in the series, which is formatted as the year followed by a colon and
% then a unique series number for the thesis - starting with 1 each year.
\trita{TRITA -- EECS-EX}{2024:0000}

% Put the title, author, and keyword information into the PDF meta information
\input{lib/pdf_related_includes}


% the custom colors and the commands are defined in defines.tex 
\hypersetup{
	colorlinks = true,
	breaklinks = true,
	linkcolor = \linkscolor,
	urlcolor = \urlscolor,
	citecolor = \refscolor,
	anchorcolor = black
}

\ifnomenclature
\renewcommand*{\pagedeclaration}[1]{\unskip, \dotfill\hyperlink{page.#1}{page\nobreakspace#1}}

\renewcommand{\nomname}{List of Symbols Used}

\renewcommand{\nompreamble}{The following symbols will be later used within the body of the thesis.}
\makenomenclature
\fi

\colorlet{punct}{red!60!black}
\definecolor{delim}{RGB}{20,105,176}
\definecolor{numb}{RGB}{106, 109, 32}
\definecolor{string}{RGB}{0, 0, 0}

\lstdefinelanguage{json}{
 numbers=none,
 numberstyle=\small,
 frame=none,
 rulecolor=\color{black},
 showspaces=false,
 showtabs=false,
 breaklines=true,
 postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\color{gray}\hookrightarrow\space}},
 breakatwhitespace=true,
 basicstyle=\ttfamily\small,
 extendedchars=false,
 upquote=true,
 morestring=[b]",
 stringstyle=\color{string},
 literate=
 *{0}{{{\color{numb}0}}}{1}
 {1}{{{\color{numb}1}}}{1}
 {2}{{{\color{numb}2}}}{1}
 {3}{{{\color{numb}3}}}{1}
 {4}{{{\color{numb}4}}}{1}
 {5}{{{\color{numb}5}}}{1}
 {6}{{{\color{numb}6}}}{1}
 {7}{{{\color{numb}7}}}{1}
 {8}{{{\color{numb}8}}}{1}
 {9}{{{\color{numb}9}}}{1}
 {:}{{{\color{punct}{:}}}}{1}
 {,}{{{\color{punct}{,}}}}{1}
 {\{}{{{\color{delim}{\{}}}}{1}
 {\}}{{{\color{delim}{\}}}}}{1}
 {[}{{{\color{delim}{[}}}}{1}
 {]}{{{\color{delim}{]}}}}{1}
 {'}{{\char13}}1,
}

\lstdefinelanguage{XML}
{
 basicstyle=\ttfamily\color{blue}\bfseries\small,
 morestring=[b]",
 morestring=[s]{>}{<},
 morecomment=[s]{<?}{?>},
 stringstyle=\color{black},
 identifierstyle=\color{blue},
 keywordstyle=\color{cyan},
 breaklines=true,
 postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\color{gray}\hookrightarrow\space}},
 breakatwhitespace=true,
 morekeywords={xmlns,version,type}
}


\makeatletter
\AtBeginDocument{\let\c@listing\c@lstlisting}
\AtBeginDocument{\let\l@listing\l@lstlisting}
\makeatother


\renewcommand{\lstlistlistingname}{Listings}

\numberwithin{listing}{chapter}

\usepackage{subfiles}

% To have Creative Commons (CC) license and logos use the doclicense package
% Note that the lowercase version of the license has to be used in the modifier
% i.e., one of by, by-nc, by-nd, by-nc-nd, by-sa, by-nc-sa, zero.
% For background see:
% https://www.kb.se/samverkan-och-utveckling/oppen-tillgang-och-bibsamkonsortiet/open-access-and-bibsam-consortium/open-access/creative-commons-faq-for-researchers.html
% https://kib.ki.se/en/publish-analyse/publish-your-article-open-access/open-licence-your-publication-cc
\begin{comment}
\usepackage[
 type={CC},
 %modifier={by-nc-nd},
 %version={4.0},
 modifier={by-nc},
 imagemodifier={-eu-88x31}, % to get Euro symbol rather than Dollar sign
 hyphenation={RaggedRight},
 version={4.0},
 %modifier={zero},
 %version={1.0},
]{doclicense}
\end{comment}

\begin{document}
\selectlanguage{english}

\pagenumbering{alph}
\kthcover
\clearpage\thispagestyle{empty}\mbox{}
\titlepage

\bookinfopage

\frontmatter
\setcounter{page}{1}
\begin{abstract}
\markboth{\abstractname}{}
\begin{scontents}[store-env=lang]
eng
\end{scontents}



\begin{scontents}[store-env=abstracts,print-env=true]
\generalExpl{Enter your abstract here!}
An abstract is (typically) about 250 and 350 words (1/2 A4-page) with the following components:
% key parts of the abstract
\begin{itemize}
 \item What is the topic area? (optional) Introduces the subject area for the project.
 \item Short problem statement
 \item Why was this problem worth a Bachelor's/Master's thesis project? (\ie, why is the problem both significant and of a suitable degree of difficulty for a Bachelor's/Master's thesis project? Why has no one else solved it yet?)
 \item How did you solve the problem? What was your method/insight?
 \item Results/Conclusions/Consequences/Impact: What are your key results/\linebreak[4]conclusions? What will others do based on your results? What can be done now that you have finished - that could not be done before your thesis project was completed?
\end{itemize}

\end{scontents}
\engExpl{The following are some notes about what can be included (in terms of LaTeX) in your abstract.}
Choice of typeface with \textbackslash textit, \textbackslash textbf, and \textbackslash texttt: \textit{x}, \textbf{x}, and \texttt{x}.

Text superscripts and subscripts with \textbackslash textsubscript and \textbackslash textsuperscript: A\textsubscript{x} and A\textsuperscript{x}.

Some symbols that you might find useful are available, such as: \textbackslash textregistered, \textbackslash texttrademark, and \textbackslash textcopyright. For example, 
the copyright symbol: \textbackslash textcopyright Maguire 2022 results in \textcopyright Maguire 2022. Additionally, here are some examples of text superscripts (which can be combined with some symbols): \textbackslash textsuperscript\{99m\}Tc, A\textbackslash textsuperscript\{*\}, A\textbackslash textsuperscript\{\textbackslash textregistered\}, and A\textbackslash texttrademark resulting in \textsuperscript{99m}Tc, A\textsuperscript{*}, A\textsuperscript{\textregistered}, and A\texttrademark. Two examples of subscripts are: H\textbackslash textsubscript\{2\}O and CO\textbackslash textsubscript\{2\} which produce H\textsubscript{2}O and CO\textsubscript{2}.

You can use simple environments with begin and end: itemize and enumerate and within these use instances of \textbackslash item.

The following commands can be used: \textbackslash eg, \textbackslash Eg, \textbackslash ie, \textbackslash Ie, \textbackslash etc, and \textbackslash etal: \eg, \Eg, \ie, \Ie, \etc, and \etal.

The following commands for numbering with lowercase Roman numerals: \textbackslash first, \textbackslash Second, \textbackslash third, \textbackslash fourth, \textbackslash fifth, \textbackslash sixth, \textbackslash seventh, and \textbackslash eighth: \first, \Second, \third, \fourth, \fifth, \sixth, \seventh, and \eighth. Note that the second case is set with a capital 'S' to avoid conflicts with the use of second of as a unit in the \texttt{siunitx} package.

Equations using \textbackslash( xxxx \textbackslash) or \textbackslash[ xxxx \textbackslash] can be used in the abstract. For example: \( (C_5O_2H_8)_n \)
or \[ \int_{a}^{b} x^2 \,dx \]
Note that you \textbf{cannot} use an equation between dollar signs.


Even LaTeX comments can be handled by using a backslash to quote the percent symbol, for example: \% comment.
Note that one can include percentages, such as: 51\% or \SI{51}{\percent}.

\subsection*{Keywords}
\begin{scontents}[store-env=keywords,print-env=true]
% If you set the EnglishKeywords earlier, you can retrieve them with:
\InsertKeywords{english}
% If you did not set the EnglishKeywords earlier then simply enter the keywords here:
% comma separate keywords, such as: Canvas Learning Management System, Docker containers, Performance tuning
\end{scontents}
\engExpl{\textbf{Choosing good keywords can help others to locate your paper, thesis, dissertation, \ldots and related work.}}
Choose the most specific keyword from those used in your domain, see for example: the ACM Computing Classification System ({\small \url{https://www.acm.org/publications/computing-classification-system/how-to-use})},
the IEEE Taxonomy ({\small \url{https://www.ieee.org/publications/services/thesaurus-thank-you.html}}), PhySH (Physics Subject Headings)\linebreak[4] ({\small \url{https://physh.aps.org/}}), \ldots or keyword selection tools such as the National Library of Medicine's Medical Subject Headings (MeSH) ({\small \url{https://www.nlm.nih.gov/mesh/authors.html}}) or Google's Keyword Tool ({\small \url{https://keywordtool.io/}})\\

\textbf{Formatting the keywords}:
\begin{itemize}
 \item The first letter of a keyword should be set with a capital letter and proper names should be capitalized as usual.
 \item Spell out acronyms and abbreviations.
 \item Avoid "stop words" - as they generally carry little or no information.
 \item List your keywords separated by commas (``,'').
\end{itemize} 
Since you should have both English and Swedish keywords - you might think of ordering the keywords in corresponding order (\ie, so that the n\textsuperscript{th} word in each list correspond) - this makes it easier to mechanically find matching keywords.
\end{abstract}
\cleardoublepage
\babelpolyLangStart{swedish}
\begin{abstract}
 \markboth{\abstractname}{}
\begin{scontents}[store-env=lang]
swe
\end{scontents}

\subsection*{Nyckelord}
\begin{scontents}[store-env=keywords,print-env=true]
\InsertKeywords{swedish}
\end{scontents}
\end{abstract}
\babelpolyLangStop{swedish}

\cleardoublepage

\section*{Acknowledgments}
\markboth{Acknowledgments}{}
\sweExpl{Författarnas tack}

I would like to thank xxxx for having yyyy.

\acknowlegmentssignature

\fancypagestyle{plain}{}
\renewcommand{\chaptermark}[1]{ \markboth{#1}{}} 
\tableofcontents
 \markboth{\contentsname}{}

\cleardoublepage
\listoffigures

\cleardoublepage

\listoftables
\cleardoublepage
\lstlistoflistings\engExpl{If you have listings in your thesis. If not, then remove this preface page.}
\cleardoublepage
% Align the text expansion of the glossary entries
\newglossarystyle{mylong}{%
 \setglossarystyle{long}%
 \renewenvironment{theglossary}%
 {\begin{longtable}[l]{@{}p{\dimexpr 2cm-\tabcolsep}p{0.8\hsize}}}% <-- change the value here
 {\end{longtable}}%
 }
%\glsaddall
%\printglossaries[type=\acronymtype, title={List of acronyms}]
\printglossary[style=mylong, type=\acronymtype, title={List of acronyms and abbreviations}]
%\printglossary[type=\acronymtype, title={List of acronyms and abbreviations}]

%\printnoidxglossary[style=mylong, title={List of acronyms and abbreviations}]
\engExpl{The list of acronyms and abbreviations should be in alphabetical order based on the spelling of the acronym or abbreviation.
}

% if the nomenclature option was specified, then include the nomenclature page(s)
\ifnomenclature
 \cleardoublepage
 % Output the nomenclature list
 \printnomenclature
\fi

%% The following label is essential to know the page number of the last page of the preface
%% It is used to compute the data for the "For DIVA" pages
\label{pg:lastPageofPreface}
% Mainmatter is where the actual contents of the thesis goes
\mainmatter
\glsresetall
\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\selectlanguage{english}





\chapter{Introduction}
\label{ch:introduction}
Live digital services rely on a continuous flow of fresh data from public \glspl{API}. This thesis is conducted in collaboration with \textit{Klimra}, a Swedish start-up that automates train-delay compensation and now wants to add a live delay dashboard. Whether that dashboard is useful depends entirely on how quickly external updates can be ingested and displayed to travellers. Choosing an architecture that minimises latency while making efficient use of a shared public \gls{API} is therefore of great importance.

Behind the scenes, two architectural choices dominate both the speed of those updates and the resources they consume: 
\begin{itemize}[noitemsep,leftmargin=*]
 \item[] \textbf{(i) ingestion method:}
 \begin{itemize}[noitemsep,leftmargin=*]
 \item whether the back-end pulls updates at fixed intervals (\emph{polling}) 
 \item or reacts to push events (\emph{event driven})
 \end{itemize}
\end{itemize}
and
\begin{itemize}[noitemsep,leftmargin=*]
 \item[] \textbf{(ii) the storage engine:}
 \begin{itemize}[noitemsep,leftmargin=*]
 \item \emph{Relational:} the classic \say{tables and SQL} databases (e.g.\ PostgreSQL)
 \item \emph{\gls{TSDB}:} built specifically for timestamped sensor data (e.g.\ TimescaleDB, Amazon Timestream)
 \item \emph{NoSQL:} cloud services that store flexible key-value records instead of fixed tables (e.g.\ DynamoDB)
 \end{itemize}
\end{itemize}

\noindent
Prior studies show that switching from polling to event-driven pipelines lowers average and tail latency but can raise CPU bursts \cite{Trindade2021EDAImpact}. 
Database comparisons tell a similar story of trade-offs: relational engines are versatile, time-series stores excel at range queries and fast inserts, and cloud NoSQL services scale reads smoothly, each at the cost of extra memory, disk, or replicas \cite{Heldt2021SciTS,Grzesik2020EdgeIoTBenchmark,Vergara2021PerformanceTSDB}.

\begin{figure}[htbp]
 \centering
 \resizebox{\linewidth}{!}{
 \begin{tikzpicture}[>=latex,
  box/.style = {draw, thick, rounded corners,
     minimum width = 3cm, minimum height = 1cm,
     text width = 2.8cm, align = center}]
 \node[box, fill=blue!10]    (api) {Trafikverket\\REST API};
 \node[box, fill=orange!10, right=2cm of api] (ingest) {Ingestion layer\\(Polling \emph{or} Events)};
 \node[box, fill=green!10, right=2cm of ingest] (db) {Database\\(SQL / TSDB / NoSQL)};
 \node[box, fill=yellow!15, right=2cm of db] (ui) { Dashboard / Website\\(traveller view)};
 \draw (api) -- (ingest) -- (db) -- (ui);
 \end{tikzpicture}}%
 \caption{Benchmark scope: API, ingestion layer, database, React dashboard}
 \label{fig:pipeline}
\end{figure}

\noindent
This thesis benchmarks six AWS-based ingestion–storage combinations to assess their performance under continuous, real-time updates from Trafikverket's API, aiming to provide engineers with a data-driven recipe for choosing the right stack. Figure~\ref{fig:pipeline} above illustrates the end-to-end path that each variant implements and that we benchmark.
The remainder of the chapter states the background (Section~\ref{sec:background}), defines the problem (Section~\ref{sec:problem}), describes the purpose and goals (Sections~\ref{sec:purpose}–\ref{sec:goals}), sketches the method (Section~\ref{sec:method}), lists delimitations (Section~\ref{sec:delimitations}), and outlines the structure of the thesis (Section~\ref{sec:structure}).




\section{Background}
\label{sec:background}
Live online services increasingly draw on a steady stream of data from other platforms. Any application that depends on real-time updates must ingest public or partner \glspl{API} quickly enough for the information to remain relevant by the time it reaches end-users. In a cloud environment, two key architectural choices largely decide how fresh the data stays: How updates are fetched (ingestion strategy) and where they are stored (storage engine).

%\begin{itemize}[noitemsep,leftmargin=*]
% \item[] \textbf{(i) Ingestion strategy:} fixed-interval polling versus event-driven push pipelines
% \item[] \textbf{(ii) Storage engine:} general-purpose relational stores, specialised time-series engines, or cloud-native NoSQL.
%\end{itemize}

%COMMENT: Too broad - went more into detail in finished sections
%-----------------------
%Prior studies confirm that each choice shifts the balance between latency, throughput, and scalability. Trindade \etal\ show that switching from scheduled polling to an event-driven pipeline cuts the slowest response times under the same load profile \cite{Trindade2021EDAImpact}. In another study, Heldt \etal\ report that purpose-built time-series databases answer range queries up to three times faster than a row-oriented traditional SQL engine running on identical hardware \cite{Heldt2021SciTS}. However, those studies are evaluated in isolation, examining ingestion and storage separately, and they rarely account for the strict request limits that public APIs impose.

\subsection{Ingestion Strategy}
Industrial control systems show what can go wrong with naïve polling. 
In one \gls{SCADA} migration the master asked each sensor for new values every few seconds, even when nothing had changed; the extra traffic clogged the network. 
After switching to push-style updates where the sensors send data only when a value changes, both bandwidth use and wait time dropped sharply \cite{Johansson2021SCADAIaaS}. 
Large data-centre studies show a similar pattern: constant polling wasted about 30\% of CPU, while event notifications halve CPU load and cut the slowest response times by almost 90\% \cite{Lewis2020PowerOfEDA}. 
Battery-powered \gls{IoT} devices benefit even more: weekly energy use fell roughly \( \sim3000\times \) when devices slept until an event arrived instead of waking on a timer \cite{Makarovi2022EnergyEfficientIoT}.

Event-driven designs are not free, however. 
They need message brokers and extra work to decode each event, and small tests have shown that a single, tightly integrated program (a monolith) can still answer faster than a network of event-driven microservices \cite{Trindade2021EDAImpact}. 
These mixed results set up the first question of this thesis: should a real-time back-end \emph{poll} for updates or react to \emph{events}?

\subsection{Storage Engine}
Three families of databases matter for this study, each with its own strengths and limitations.
\textbf{Relational systems}, such as vanilla PostgreSQL, organise data in tables and are reliable all-rounders, but can slow down when millions of time-stamped rows arrive every minute.
\textbf{Time-series databases} (\glspl{TSDB}) like InfluxDB, TimescaleDB, and Amazon Timestream are built for that exact workload: they append new samples in time-order blocks, so inserts remain fast and range queries over recent hours or days run quickly. For example, TimescaleDB and InfluxDB loaded millions of sensor readings on a Raspberry Pi and answered range queries about three times faster than SQLite, a lightweight relational engine \cite{Grzesik2020EdgeIoTBenchmark}. In larger lab tests, InfluxDB led on heavy aggregations, while TimescaleDB was best for single-row look-ups thanks to PostgreSQL's indexing \cite{Daqouri2023TimeseriesVsSQL,Heldt2021SciTS}. Performance, however, is not guaranteed. In the cloud, Amazon Timestream fell behind DynamoDB and even a simple S3 data-lake when full-history scans were required \cite{Johansson2022AWSCloudData}, and MongoDB beat TimescaleDB on highly selective tag queries \cite{Mohamed2024DBMSComparison}.
Finally, \textbf{cloud NoSQL services} such as DynamoDB trade strict schema and complex joins for limitless horizontal scaling; studies show they can match \gls{TSDB} write speed once enough replicas are added, but the extra nodes consume more memory and network bandwidth \cite{Zhang2023EdgeTSDB,Vergara2021PerformanceTSDB}.

These mixed results underline that no single engine is \say{best} in every situation. Relational databases offer familiarity and strong consistency, \glspl{TSDB} excel at broad time-range analytics, and NoSQL stores scale effortlessly for simple key-value access—each at a cost that must be weighed for Klimra's real-time dashboard.





\section{Problem}
\label{sec:problem}

Klimra's live-delay dashboard, like any service that relies on shared public \glspl{API}, must deliver fresh data in seconds. Developers must therefore decide both \textit{how} to fetch updates (polling or push) and \textit{where} to store them (relational, time-series, or NoSQL). The literature treats these decisions separately: ingestion studies vary the fetch style but leave the database unchanged \cite{Trindade2021EDAImpact,Lewis2020PowerOfEDA}, while database benchmarks compare storage engines using a single, high-throughput synthetic workload that ignores \gls{API} rate limits and keeps the ingestion method fixed \cite{Heldt2021SciTS,Grzesik2020EdgeIoTBenchmark}. Because no study evaluates both choices together under realistic traffic conditions, engineers lack evidence for selecting a complete backend stack.

This thesis aims to close that gap by measuring six AWS configurations: polling versus event-driven ingestion paired with RDS PostgreSQL, Amazon Timestream, or DynamoDB. It records update latency, query speed, and resource use while staying within Trafikverket's live request limit, giving practitioners a clear decision matrix and researchers a reproducible baseline.



\section{Purpose}
\label{sec:purpose}
The study has a dual purpose. For Klimra, it aims to guide the choice of ingestion style and database so that the dashboard developed in this project can meet latency targets without breaching Trafikverket's request limit. For the academic and engineering community, it fills the documented research gap by supplying a public, statistically sound benchmark that links ingestion strategy with storage choice under a real-world traffic cap. Achieving these aims also demonstrates the author's ability to design, execute, and analyse a cloud-scale performance study, thereby satisfying the goals of the master's degree project.



\section{Goals}
\label{sec:goals}
The project turns that purpose into four concrete goals:

\begin{enumerate}[label=\textbf{G\arabic*},leftmargin=*]
 \item \textbf{Prototype} six back-end variants that differ only in \emph{(i)} ingestion style (polling, event-driven) and \emph{(ii)} storage engine (RDS PostgreSQL, Amazon Timestream, DynamoDB).

 \item \textbf{Benchmark} each variant against Trafikverket's live train-position API, measuring update latency, query latency, throughput, CPU, memory, and network I/O.

 \item \textbf{Analyse} the results using appropriate statistical tests to identify significant differences and resource trade-offs.

 \item \textbf{Deliverables}: 
  \begin{itemize}[noitemsep,leftmargin=*]
   \item a reusable k6/Grafana benchmarking toolkit, 
   \item a decision matrix that maps workload patterns to the most efficient configuration for Klimra, and 
   \item a reproducible dataset and report that other researchers can build on.
  \end{itemize}
\end{enumerate}




\section{Research Methodology}
\label{sec:method}

This study adopts a \textit{design-science case‐study} approach: one React/Node train-tracking application is implemented once and then redeployed with six back-end variants so that only the ingestion style (polling versus event-driven) and the database (PostgreSQL, Amazon Timestream, DynamoDB) differ.

\subsection{Experimental frame}
The frame states what is varied, what is fixed, and what is measured. A 24-hour recording of Trafikverket's train position feed is replayed at real-time speed with \textsc{k6}, an open-source HTTP load generator scripted in JavaScript. Each back-end is launched by Terraform, which provides an identical virtual machine for every trial. It provides two virtual CPU cores, 4 GiB of RAM, and runs on Amazon's Graviton 3 processor. AWS CloudWatch, the platform's built-in monitoring service, samples once per second five metric groups: update latency, ingestion delay, query latency, CPU + memory load, and network/disk I/O. This multi-metric set mirrors the suites used in recent cloud benchmarks \cite{Heldt2021SciTS,Grzesik2020EdgeIoTBenchmark}.


\subsection{Sampling and analysis}
Each of the six back-ends will be exercised 15 times, for 30 minutes per run, on freshly launched virtual machines. This repetition rate follows the noisy-neighbour guidance of Hidalgo \textit{et al.} for cloud experiments, where background tenants can distort a single run's latency or throughput \cite{Hidalgo2023StreamProcMicroservices}. The resulting time series are evaluated in three steps:

\begin{enumerate}[label=\arabic*., leftmargin=*]
 \item \textbf{Shapiro–Wilk test} checks whether each metric is approximately normally distributed. 
 \item \textbf{One-way ANOVA} (if normal) or \textbf{Kruskal–Wallis} (if non-normal) compares all six back-ends in one go to detect any statistically significant difference. 
 \item \textbf{Holm–Šidák correction} on post-hoc pairwise tests keeps the overall false-positive rate below $5\,\%$. 
\end{enumerate}

\noindent
Every reported difference is accompanied by an \textbf{effect size}: $\eta^{2}$ for ANOVA or $\varepsilon^{2}$ for Kruskal–Wallis. This is so that readers know how large the gap is, not just that it exists.


\subsection{Method choices}
Local emulators and micro-benchmarks were rejected because they mask cloud side variance and ignore the interaction between ingestion logic and storage engine. All Terraform scripts, Dockerfiles, and raw metric logs will be released under an open licence to meet reproducibility guidelines for cloud experiments \cite{PerformanceEvaluationMetrics}.






\section{Delimitations}
\label{sec:delimitations}
This thesis targets the server-side path from Trafikverket's \gls{API} to Klimra's planned dashboard. The work therefore has the following delimitations:

\begin{itemize}[noitemsep,leftmargin=*]
 \item \textbf{Cloud scope:} Only AWS databases are benchmarked (RDS PostgreSQL, Amazon Timestream, DynamoDB). Other cloud providers or locally hosted databases are not included.

 \item \textbf{Ingestion patterns:} The study compares fixed-interval polling with event-driven SQS/Lambda pipelines. Other mechanisms, such as long polling, gRPC streams, or change-data-capture, are left for future work.

 \item \textbf{Databases:} One representative engine is chosen per family. Variants such as InfluxDB, TimescaleDB on self-managed Postgres, or document stores beyond DynamoDB are out of scope.

 \item \textbf{Client side:} UI/UX design, mobile apps, and front-end optimisation are not evaluated. The React dashboard is used only as a constant workload generator.

 \item \textbf{Non-functional aspects:} Security hardening, cost modelling, carbon footprint, and detailed failure-recovery analysis are acknowledged but not measured.

 \item \textbf{Traffic profile:} Experiments replay one 24-hour snapshot at the live request limit. Burst tests above that limit and multi-day retention scenarios are excluded.

 \item \textbf{Geography:} All resources are hosted in a single AWS region; cross-region latency and replication behaviour are not studied.
\end{itemize}

\noindent
These boundaries keep the project focused on the interplay between ingestion style and storage engine under a realistic, rate-limited workload.




\section{Structure of the thesis}
\label{sec:structure}

\begin{description}[leftmargin=0cm]

 \item[Chapter~\ref{ch:introduction}] introduces the study, states the problem, purpose, goals, delimitations, and gives a roadmap for the rest of the thesis.

 \item[Chapter~\ref{ch:background}] reviews related work on ingestion strategies, database families, and cloud-benchmark practices needed to understand the study.

 \item[Chapter~\ref{ch:methods}] explains the research methodology, experimental frame, sampling strategy, and statistical tests.

 \item[Chapter~\ref{ch:whatYouDid}] outlines the implementation of the six AWS back-end variants and the k6/Grafana benchmarking toolkit.

 \item[Chapter~\ref{ch:resultsAndAnalysis}] presents the collected metrics and analyses latency, resource use, and scalability trade-offs.

 \item[Chapter~\ref{ch:discussion}] interprets the results, compares them to prior work, and discusses practical implications for cloud architects.

 \item[Chapter~\ref{ch:conclusionsAndFutureWork}] summarises the main findings, notes limitations, and suggests directions for future research.

 \item[Appendices] list acronyms and supply supporting artefacts (Terraform scripts, code snippets, raw data) to ensure reproducibility.

\end{description}







\cleardoublepage
\chapter{Background}
\label{ch:background}

This chapter provides the technical foundation and overview of prior work needed to follow and understand the rest of the thesis. First,
Section \ref{sec:ingestion} surveys fixed-interval polling and event-driven pipelines, summarising the latency, throughput, and cost trade-offs reported in recent literature. Section \ref{sec:storage} then reviews the three database families evaluated here (relational, time-series, and NoSQL) and the benchmark results in recent literature. Because earlier studies look at ingestion and storage separately, none test the two together under real-world rate limits. Finally, Section \ref{sec:gap} discusses how his thesis fills that gap.



\section{Ingestion Strategies}
\label{sec:ingestion}
There are two main ways to pull information out of a channel: the application can poll at fixed intervals, or it can let the messaging system notify it as soon as a message arrives \cite[Ch.~10]{Hohpe2003EIP}.
Most cloud back-ends adopt one of these two patterns when fetching data from external services. This section introduces both patterns and reviews the performance findings reported in earlier studies.


\subsection{Fixed-Interval Polling}
Polling is conceptually simple: a worker thread (or PLC task) issues a request every $\Delta t$, parses the reply, and stores the result.
Because a Polling Consumer calls receive only when it is ready, the application can precisely throttle how many messages it accepts and will thus keep itself from being overloaded.
The trade-off is efficiency: when the channel is empty, the polling thread either blocks or wakes up only to find no work, wasting CPU time while nothing is processed \cite[pp.~494–497]{Hohpe2003EIP}.

Johansson and Larsson migrated a \gls{SCADA} system to AWS and found that fixed‐interval polling flooded the network with repeated sensor messages; they therefore recommended replacing the loops with event notifications \cite{Johansson2021SCADAIaaS}. 
Likewise, Makarović \textit{et~al.}\ showed that allowing a household  \gls{IoT} toaster to stay idle until an event arrives cuts Wi-Fi energy use by about \(3000\times\) compared with continuous polling \cite{Makarovi2022EnergyEfficientIoT}.


\subsection{Event-Driven Pipelines}
In the alternative Event-Driven Consumer pattern, the application hands a callback (listener) object to the messaging middleware. The broker then invokes that listener as soon as a new message arrives. 
Since the thread sleeps until the broker calls back, the CPU only does work when there is an actual message, and the system reacts right away instead of waiting for the next poll.
The downside is the need for extra code: the callback must be thread-safe, the program must handle multiple listener threads, and it must manage message acknowledgments or retries itself \cite[pp.~498–501]{Hohpe2003EIP}.

Switching to events, however, does not automatically make systems faster. In a load-testing study, Berg demonstrated that a quick microservice rewrite responded to requests more slowly than the original monolith, even though it used less CPU, indicating that the extra network and serialization overhead can cancel out the benefits of an event-driven design if it is not carefully tuned \cite{Berg2022MonolithVsMicroservices}.
Later work highlights three common problems: writing idempotent handlers, keeping events in the right order, and absorbing sudden bursts of messages without swamping the database \cite{Trindade2021EDAImpact}.


%\subsection{Prior Work}
%Table \ref{tab:ingestion-lit} sums up some key measurements from prior work.

%\begin{table}[H]
% \centering
% \caption{Typical impact of moving from polling to event-driven ingestion}
% \label{tab:ingestion-lit}
% \begin{tabular}{p{3.4cm}p{4.0cm}p{3.7cm}}
% \toprule
% \textbf{Domain} & \textbf{Observed Change} & \textbf{Key Point} \\ \midrule
% Industrial SCADA   & Bandwidth ↓ (duplicates removed)  & Cuts \say{network flood} traffic \\
% Data-centre orchestration & CPU ↓ 35–50 \%; latency ↓ 70 \%  & Less idle spin and better scale \\
% Home  \gls{IoT} (Wi-Fi sensor)  & Energy use ↓ $\approx\!3000\times$  & Polling wastes standby power \\
% Micro-service refactor  & CPU ↓ but latency ↑     & Extra hops can hurt response \\ 
% \bottomrule
% \end{tabular}
%\end{table}

%\noindent
%Most studies agree that event-driven designs save CPU time and bandwidth, but the gains depend on workload size, traffic bursts, and how well the system is built. Crucially, none of them compares polling and push methods while testing relational, time-series, and NoSQL databases under a real API rate limit. This is the research gap this thesis tackles.



%%%  – cite Trindade et al., Lewis et al., etc.
%%%  – summarise pros/cons table-style if helpful

%%% \subsection{Why a combined benchmark is still missing}
%
% (Short quotes, numbers, and citations go here)

% ---------------------------------------------------------------
\section{Storage Technologies}
\label{sec:storage}

Among the several storage paradigms available in the cloud, this study concentrates on three that best match real-time ingestion needs:

\begin{itemize}[leftmargin=*]
 \item \textbf{Relational databases} (e.g.\ PostgreSQL on Amazon RDS) organise data in normalised tables and offer full SQL, strong \gls{ACID} guarantees, and mature indexing. 
 \item \textbf{Time-series databases} (e.g.\ Amazon Timestream) append readings to time-ordered blocks, optimise range scans over recent windows, and compress repeating timestamps. 
 \item \textbf{NoSQL key–value stores} (e.g.\ DynamoDB) relax joins and schema in favour of horizontal scaling and single-digit millisecond reads at any load. 
\end{itemize}

\noindent
Prior work shows that each family excels under a different access pattern. Relational engines are versatile but can slow down when millions of inserts arrive every minute \cite{Heldt2021SciTS}. Purpose-built time-series stores answer range queries up to three times faster than Postgres on identical hardware \cite{Grzesik2020EdgeIoTBenchmark}, while NoSQL services match write speed once enough replicas are added, although at higher memory cost \cite{Vergara2021PerformanceTSDB}. No single solution is universally \say{best,} so the rest of this section reviews each option in turn and ends with a literature-based trade-off matrix.

\subsection{Relational Databases (PostgreSQL/RDS)}
Relational databases store information in tables made of rows and columns, where each table has a fixed schema that defines the names and data types of its columns. Queries are written in SQL, and the database engine guarantees \gls{ACID} properties so that transactions stay consistent even if many users change data at the same time \cite[Chs.~1–2]{Silberschatz2020DB}.

PostgreSQL is a popular open-source implementation of the relational model; it adds useful extras such as JSON columns, full-text search, and advanced indexing options \cite[Ch.~27]{Silberschatz2020DB}. On AWS, you can run PostgreSQL as a managed service called Amazon RDS for PostgreSQL. RDS handles routine chores, such as creating automated backups, applying security patches, setting up replication across Availability Zones, and letting you scale storage with a few clicks. It does this so developers can focus on schema design and query tuning instead of server maintenance \cite{AWSRDSPostgreSQL}.


\subsection{Time-Series Databases (Amazon Timestream)}
Time-series databases are built for data whose primary key is a timestamp. Instead of normal rows, they keep new samples in time-ordered batches and squeeze space by compressing columns that change slowly. They also let you run window queries, such as averages or rates, directly. Because data is only added and not rewritten, and most queries scan a continuous time range, a good TSDB can accept a high write load and still answer \say{last hour} or \say{last day} questions quickly; these queries ask for all points from the most recent 60 minutes or 24 hours and summarise them \cite[Chs.~3 \& 11]{Kleppmann2017DDIA}.

Amazon Timestream is a managed, serverless \gls{TSDB}. Storage and compute grow automatically as your metric data increases. New data stays in memory for quick queries and is later moved to cheaper disk storage based on a retention rule you choose. Timestream's SQL lets you resample, fill gaps, or delete old records without separate \gls{ETL} jobs, and AWS takes care of backups, replication, and encryption \cite{AWSTimestreamDoc}.


\subsection{NoSQL Key–Value Stores (DynamoDB)}
Key-value databases keep every record as two parts: a unique key and its associated value.
To read or update a record, the engine hashes the key and goes straight to the right location, so performance stays high even when the dataset grows large.
Because the value can be any binary or JSON document, the database keeps it exactly as it is and does not enforce a fixed schema.
Applications are free to add or remove fields without a formal migration, and clusters can shard data by key to spread load and add replicas easily \cite[Chs.~2 \& 8]{Sadalage2013NoSQLDistilled}.

Amazon DynamoDB is AWS's managed key-value service.
It is serverless, meaning that users never have to create or maintain the underlying machines.
AWS provisions the hardware, applies software updates, and scales capacity up or down as traffic changes.
All you supply are two numbers: your desired read and write capacity units. 
The platform then handles partitioning, replication, and fail-over in the background.
Each item is synchronously copied to three Availability Zones, backed up automatically, and served with single-digit millisecond latency.
Features such as global secondary indexes, point-in-time recovery, and at-rest encryption are included, so developers do not have to run or patch their own database clusters \cite{AWSDynamoDBDoc}.


\subsection{Trade-Off Matrix from Literature}

Taken together, recent benchmarks show a consistency in results. 
Relational engines (PostgreSQL/RDS) excel at ad-hoc joins and point look-ups but slow down once sustained insert rates exceed \(\sim\!10^{4}\) rows s\(^{-1}\) on commodity cloud nodes \cite{Heldt2021SciTS}. 
Purpose-built time-series stores keep up with bursty sensor feeds and answer
range-window queries 2–3× faster than Postgres on the same hardware
\cite{Grzesik2020EdgeIoTBenchmark,Daqouri2023TimeseriesVsSQL}. 
Key–value services such as DynamoDB scale linearly with partitions and can
match TSDB write speed, but they consume more memory and require careful key
design to avoid hot shards \cite{Vergara2021PerformanceTSDB,Zhang2023EdgeTSDB}.
Table~\ref{tab:storage-matrix} contrasts the most relevant findings.

\begin{table}[htbp]
 \centering
 \begin{threeparttable}
 \caption{Headline results from recent storage benchmarks
   (\(\uparrow\!=\) higher is better,\; \(\downarrow\!=\) lower is better)}
 \label{tab:storage-matrix}
 %--------------------------------------------------------------
 % X column stretches, the others have fixed width for numbers
 \begin{tabularx}{\linewidth}{@{} l c c c @{}}
 \toprule
 \textbf{Study} & \textbf{Best write} & \textbf{Fastest range} \\[-2pt]
     & (rows s\(^{-1}\)) & query (s)    &    \\
 \midrule
 Grzesik 20\tnote{a} & InfluxDB ↑1.6 M & TimescaleDB ↓0.35 \\[2pt]
 Heldt 21\tnote{b} & Timescale ↑3× & InfluxDB ↓0.06 \\[2pt]
 Vergara 21\tnote{c} & Dynamo = Influx (3 reps) & InfluxDB ↓0.055 \\[2pt]
 Johansson 22\tnote{d}& S3 lake ↑2× & Dynamo fastest \\[2pt]
 Zhang 23\tnote{e} & Dynamo ↑2 M  & TimescaleDB ↓0.40 \\
 \bottomrule
 \end{tabularx}
 %--------------------------------------------------------------
 \begin{tablenotes}[flushleft]\footnotesize
 \item[a] TSDBs outperform SQLite and Riak-TS on Raspberry Pi hardware \cite{Grzesik2020EdgeIoTBenchmark}.
 \item[b] Relational joins handy but cost roughly 2–3× more CPU \cite{Heldt2021SciTS}.
 \item[c] NoSQL matches write speed but uses \(\sim\)1.8× more RAM \cite{Vergara2021PerformanceTSDB}.
 \item[d] Cloud TSDB not always fastest; depends on query mix \cite{Johansson2022AWSCloudData}.
 \item[e] Hot-spotting design degrades NoSQL latency if ignored \cite{Zhang2023EdgeTSDB}.
 \end{tablenotes}
 \end{threeparttable}
\end{table}


\noindent
The matrix confirms that no single engine dominates every metric. 
Relational stores offer flexible querying, \glspl{TSDB} shine on time-window analytics, and NoSQL services win on elastic scaling. 
Yet every cited paper fixes the ingestion style (usually batch writes or simple polling). No work measures these engines under both polling and event-driven pipelines while obeying a live API rate limit. 


\section{Related Work and Research Gap}
\label{sec:gap}
Earlier studies tend to split the focus of the backend into two.
Some papers ask only how fresh data is ingested (for example, comparing polling with event-driven updates), while others test where that data is stored (relational, time-series, or NoSQL engines).
These single-focus results are helpful, but they do not answer the practical question facing a live, API-limited service such as Klimra's delay dashboard: Which mix of ingestion style \textit{and} database keeps updates fast without breaking the request cap? 

The next three subsections review what the literature has taught us about (i) ingestion in isolation, (ii) storage in isolation, and (iii) why their combination under a real-world traffic limit is still missing. That gap sets the stage for the benchmark designed in Chapter~\ref{ch:methods}.




\subsection{Ingestion-Centric Studies}
Ingestion performance has been studied across various domains, including industrial automation, energy-aware \gls{IoT}, and microservice-based cloud platforms. These studies evaluate polling and event-driven patterns in isolation, with a fixed database back-end. Although they differ in setting, each one displays how ingestion style can influence resource use, latency, and efficiency. Together, these studies show that ingestion style affects performance across very different domains.

\subsubsection{Industrial control}
In time-critical control systems, meaning systems where sensors and actuators must respond within strict timing limits, delays in sensor updates can result in errors or failures. Here, ingestion style directly impacts how up-to-date the control loop data is.
Johansson and Larsson explored this in an EclipseSCADA plant migrated to two Australian IaaS regions. They simulated traffic from 800 Modbus sensors and measured performance under one-millisecond polling. As the number of sensors increased, the round-trip time for a single register rose from \SI{11}{\milli\second} to nearly \SI{14}{\second}. The system spent more time issuing requests than processing replies, leading to severe congestion. The authors recommend replacing polling with event notifications to avoid what they described as a \say{network flood} \cite{Johansson2021SCADAIaaS}.

\subsubsection{Battery-powered IoT}
In small embedded systems, power consumption is often the main constraint. Polling can drain batteries quickly by keeping radios and processors awake even when there is no data to fetch. 
To test this, Makarović \textit{et al.} implemented a smart toaster with five wireless protocols and measured power draw under polling versus event-driven designs. When polling continuously, the device used \SI{10.9}{\kilo\joule} in one week. Switching to event-driven mode allowed the Wi-Fi module to stay asleep until a message arrived, reducing consumption to just \SI{3.6}{\joule} — a drop of nearly \(3000\times\) \cite{Makarovi2022EnergyEfficientIoT}.

\subsubsection{Cloud orchestration}
In cloud infrastructure, orchestration tools manage the flow of tasks between services. They need to respond quickly while scaling efficiently under heavy load.
Trindade and Batista studied how an event-driven approach changes performance in this setting. They rewrote a polling-based orchestration loop to use Kafka and RabbitMQ to deliver events between services, meaning they notified different parts of the system when new data or tasks were available. Under the same workload, CPU usage dropped by \SI{47}{\percent} and mean response time improved by \SI{89}{\percent}, showing that events help eliminate idle spins and improve reactivity when tuned correctly \cite{Trindade2021EDAImpact}.

\subsubsection{Micro-service refactors}
Not all event-driven designs yield gains by default. When breaking a system into microservices, each new component adds potential overhead. 
Berg examined this by refactoring a small web shop from a monolith to AWS microservices. Although the new design used less CPU overall, the mean response time increased from \SIrange{13}{15}{\milli\second} to as high as \SI{43}{\milli\second} in the worst scenario. The added queueing layers introduced latency, and only after batching messages did the event-driven setup match the monolith's responsiveness \cite{Berg2022MonolithVsMicroservices}.



\subsection{Storage-Centric Studies}
While the previous section focused on how data enters the system, this one looks at where it ends up. Several benchmark studies evaluate relational, time-series, and NoSQL databases in isolation, using synthetic traces to stress-test their performance. These papers do not model real-time API ingestion but help highlight the strengths and weaknesses of the different storage engines under high-throughput loads and common query patterns. Taken together, these results show that each storage engine excels under different access patterns.

\subsubsection{Relational baselines}
Relational databases, such as PostgreSQL, are widely used because they support full SQL queries, joins, and transactions, making them suitable for complex applications that require structured data.
However, under very high insert rates, they can become a bottleneck. Heldt \textit{et al.} compared PostgreSQL, which is a general-purpose relational database, with TimescaleDB and InfluxDB, which are time-series databases designed for timestamped data.
Although PostgreSQL was the most flexible, it consumed two to three times more CPU than the time-series alternatives during sustained writes and range queries \cite{Heldt2021SciTS}.

\subsubsection{Time-series optimisations}
Time-series databases are designed specifically for workloads where each data point is associated with a timestamp. They append data in time order and compress values that change slowly.
Grzesik \textit{et al.} benchmarked three popular time-series databases: TimescaleDB, InfluxDB, and Riak-TS, using Raspberry Pi hardware. InfluxDB achieved the highest write throughput at 1.6 million rows per second, while TimescaleDB returned 24-hour range queries three times faster than Riak-TS \cite{Grzesik2020EdgeIoTBenchmark}.
Similarly, Daqouri compared TimescaleDB with PostgreSQL in an \gls{IoT} analytics scenario and found that TimescaleDB handled large, ordered inserts more efficiently and offered better query performance on time-based filters \cite{Daqouri2023TimeseriesVsSQL}.

\subsubsection{NoSQL scaling trade-offs}
NoSQL key–value stores scale horizontally and support fast lookups at the cost of limited querying.  
Vergara \textit{et al.} found that DynamoDB could match InfluxDB in write throughput once three replicas were added, but this came at a cost of 1.8× higher memory usage \cite{Vergara2021PerformanceTSDB}.  
Zhang \textit{et al.} ran a similar comparison in an edge-computing setup.  
They measured DynamoDB ingesting up to 2 million rows per second, but found that poorly chosen keys caused “hot spots” that slowed down performance.  
Meanwhile, TimescaleDB gave consistent range query latency below 0.5 seconds \cite{Zhang2023EdgeTSDB}.



\subsection{Research Gap in Real-Time, API-Limited Backends}
While prior work has studied ingestion and storage separately, few studies combine the two in a realistic setting where the back-end must update continuously while obeying a strict external rate limit.
This combination is common in real-time applications, such as the traffic dashboard being built for Klimra, as well as other monitoring tools that rely on frequent updates from public \glspl{API}.

To the best of our knowledge, no existing benchmark measures both ingestion style and storage engine choice in a real-time system that fetches continuous updates from a shared public API.
This leaves open a practical question for systems like Klimra's delay dashboard: which mix of polling or events, and which kind of database, best keeps updates fast while staying within allowed request quotas?

%To answer that, the next chapter presents a prototype test loop inspired by the load-repetition method of Hidalgo \textit{et al.} \cite{Hidalgo2023StreamProcMicroservices}.
%The test replays one day of Trafikverket traffic and measures six AWS back-end configurations across three database types.
%The goal is to provide a side-by-side evaluation of ingestion and storage combinations in a rate-limited, real-time update scenario.




\cleardoublepage
\chapter{Method}
\label{ch:methods}
This chapter outlines the method used to evaluate how ingestion style and storage technology affect performance in real-time backends constrained by external data sources. The goal is to design repeatable experiments that isolate the impact of each backend component while reflecting a realistic use case.

Section~\ref{sec:researchProcess} introduces the overall research approach, while Section~\ref{sec:researchParadigm} explains the methodological motivation and prior work that inspired this design. Section~\ref{sec:dataCollection} discusses the metrics and data sources used, and Section~\ref{sec:experimentalDesign} describes the prototype platform and test loop. These sections together explain how the benchmarking setup was constructed to answer the research questions posed in this thesis.

\section{Research Approach}
\label{sec:researchProcess}

This study uses an engineering-focused experimental method to isolate how different backend design choices affect the performance of real-time applications. The system under test is a traffic dashboard that fetches and displays live train delay data from Trafikverket’s public API. While the API does not enforce a strict numeric quota, it is shared among many users and not intended for excessive automated polling. This makes it a useful stand-in for the kind of externally constrained APIs often encountered in production systems.

To evaluate backend performance in this setting, I build a single application and deploy it with six different configurations. Each setup combines one of two ingestion strategies (fixed-interval polling or event-driven updates) with one of three storage options: a relational database (PostgreSQL), a time-series database (Amazon Timestream), or a NoSQL key–value store (DynamoDB).

The application logic stays identical across all versions, allowing each test to isolate the performance impact of ingestion and storage choices. This follows a controlled-experimentation model common in systems research, where the same input is fed into multiple variants under repeatable conditions. The result is a side-by-side comparison of how different backend strategies perform under realistic traffic and update constraints.

\section{Motivation and Prior Work}
\label{sec:researchParadigm}

Earlier studies typically isolate a single backend concern: either how data is ingested or how it is stored. As reviewed in Chapter~\ref{ch:background}, this leaves open the question of how ingestion and storage interact under live conditions. This thesis addresses that gap by testing combinations of ingestion style and storage engine within a working system that must keep up with external data.

The design draws inspiration from Hidalgo \textit{et al.}~\cite{Hidalgo2023StreamProcMicroservices}, who replay one day of real traffic to evaluate stream processing pipelines. I adopt a similar method here: a test loop replays one day of train data in compressed form, issues requests to Trafikverket’s API, and logs performance metrics under different backend setups.

\section{Method Rationale}
\label{sec:dataCollection}

I considered using synthetic benchmarks, emulators, or isolated microbenchmarks, but these lack the timing characteristics and usage constraints of real data sources. Instead, I opted for a full prototype fed by real data and deployed on AWS. This ensures that the findings reflect realistic challenges for companies like Klimra, which depend on real-time public APIs.

The evaluation focuses on backend metrics that matter in practice:
\begin{itemize}
  \item \textbf{Ingestion delay:} Time from data availability to ingestion.
  \item \textbf{End-to-end latency:} Time to deliver updated results to the UI.
  \item \textbf{System load:} CPU and memory use under each configuration.
  \item \textbf{Query performance:} Response time for common queries.
\end{itemize}

\section{Prototype and Test Loop}
\label{sec:experimentalDesign}

The test application is deployed to AWS using infrastructure-as-code. Each variant uses the same ingestion logic and UI, differing only in backend architecture. Logs record all ingest, store, and query actions. Where available, AWS CloudWatch metrics supplement these logs to track system behavior.

Each test runs for a fixed period and uses the same replayed input. To reduce random variation, each experiment is repeated multiple times and averaged. These tests are not designed for extreme scalability, but to highlight how ingestion and storage decisions influence responsiveness and resource use in a real-time, API-aware environment.

% Later sections: planned data analysis, reliability, validity, etc.




\section{Research Process}
\label{sec:researchProcess}

\sweExpl{Undersökningsrocess och utvecklingsprocess}

Figure~\ref{fig:researchprocess} shows the steps conducted to carry out this research. 

\sweExpl{Figur~\ref{fig:researchprocess} visar de steg som utförs för att genomföra\\
Beskriv, gärna med ett aktivitetsdiagram (UML?), din undersökningsprocess och utvecklingsprocess. Du måste koppla ihop det akademiska intresset (undersökningsprocess) med ursprungsproblemet (utvecklingsprocess)
denna forskning.\\
Aktivitetsdiagram från t ex UML-standard}


 
\begin{figure}[!ht]
 \begin{center}
 \includegraphics[width=0.5\textwidth]{figures/researchprocess.png}
 \end{center}
 \caption{Research Process}
 \label{fig:researchprocess}
\end{figure}

\generalExpl{Example of using customized item labels.}
Some steps in the process:
\begin{enumerate}[leftmargin=*, label=\textbf{Step \arabic*}, ref=Step \arabic*] %labelindent=1em for indent
 \itemsep0em
 \item \label{x:s1} plan experiment,
 \item \label{x:s2} conduct experiment,
 \item \label{x:s3} analyze data from the experiment, and
 \item \label{x:s4} discuss the results of the analysis.
\end{enumerate}

\sweExpl{Forskningsprocessen}


\section{Research Paradigm}
\label{sec:researchParadigm}
\sweExpl{Undersökningsparadigm\\
Exempelvis\\
Positivistisk (vad/hur fungerar det?) kvalitativ fallstudie med en deduktivt (förbestämd) vald ansats och ett induktivt(efterhand uppstår dataområden och data) insamlade av data och erfarenheter.}


\section{Data Collection}
\label{sec:dataCollection}
\sweExpl{Datainsamling\\
(Detta bör också visa att du är medveten om de sociala och etiska frågor som
kan vara relevanta för dina data insamlingsmetod.)}
\generalExpl{This should also show that you are aware of the social and ethical concerns that might be relevant to your data collection method.}



\subsection{Sampling}
\sweExpl{Stickprovsundersökning}

\subsection{Sample Size}
\sweExpl{Provstorleken}

\subsection{Target Population}
\sweExpl{Målgruppen}

\section[Experimental design/Planned Measurements]{Experimental design and\\Planned Measurements}
\label{sec:experimentalDesign}
\sweExpl{Experimentdesign/Mätuppställning}

\subsection{Test environment/test bed/model}
\engExpl{Describe everything that someone else would need to reproduce your test environment/test bed/model/… .}
\sweExpl{Testmiljö/testbädd/modell\\
Beskriv allt att någon annan skulle behöva återskapa din testmiljö / testbädd / modell / …}

\subsection{Hardware/Software to be used}
\sweExpl{Hårdvara / programvara som ska användas}


\section{Assessing reliability and validity of the data collected}
\label{sec:assessingReliability}
\sweExpl{Bedömning av validitet och reliabilitet hos använda metoder och insamlade data }


\subsection{Validity of method}
\label{sec:validtyOfMethod}
\sweExpl{Giltigheten av metoder\\
 Har dina metoder gett dig de rätta svaren och lösningarna? Var metoderna korrekta?}

\engExpl{How will you know if your results are valid?}
\engExpl{Remember that validity is about the \textit{accuracy} of a measurement while reliability is about the \textit{consistency} of the measurement values under the same conditions (\ie repeatability).}

\subsection{Reliability of method}
\label{sec:reliabilityOfMethod}
\sweExpl{Tillförlitlighet av för metoder\\
Hur bra är dina metoder, finns det bättre metoder? Hur kan du förbättra dem?}
\engExpl{How will you know if your results are reliable?}

\subsection{Data validity}
\label{sec:dataValidity}
\sweExpl{Giltigheten av uppgifter\\
Hur vet du om dina resultat är giltiga? Är ditt resultat rättvisande?}

\subsection{Reliability of data}
\label{sec:reliabilityOfData}
\sweExpl{Tillförlitlighet av data\\
Hur vet du om dina resultat är tillförlitliga? Hur bra är dina resultat?}


\section{Planned Data Analysis}
\label{sec:plannedDataAnalysis}
\sweExpl{Metod för analys av data}


\subsection{Data Analysis Technique}
\label{sec:dataAnalysisTechnique}
\sweExpl{Dataanalysteknik}

\subsection{Software Tools}
\label{sec:softwareTools}
\sweExpl{Mjukvaruverktyg}


\section{Evaluation framework}
\label{sec:evaluationFramework}
\sweExpl{Utvärdering och ramverk\\
Metod för utvärdering, jämförelse mm. Kopplar till kapitel~\ref{ch:resultsAndAnalysis}.}

\section{System documentation}
\label{sec:systemDocumentation}
\sweExpl{Systemdokumentation\\
Med vilka dokument och hur skall en konstruerad prototyp dokumenteras? Detta blir ofta bilagor till rapporten och det som problemägaren till det ursprungliga problemet (industrin) ofta vill ha.\\
Bland dessa bilagor återfinns ofta, och enligt någon angiven standard, kravdokument, arkitekturdokument, designdokumnet, implementationsdokument, driftsdokument, testprotokoll mm.}
\generalExpl{If this is going to be a complete document consider putting it in as an appendix, then just put the highlights here.}


\cleardoublepage
\chapter{What you did}\engExpl{Choose your own chapter title to describe this}
\label{ch:whatYouDid}
\sweExpl{[Vad gjorde du? Hur gick det till? – Välj lämplig rubrik (“Genomförande”, “Konstruktion”, ”Utveckling” eller annat]}


\engExpl{What have you done? How did you do it? What design decisions did you make? How did what you did help you to meet your goals?}
\sweExpl{Vad du har gjort? Hur gjorde du det? Vilka designval gjorde du?\\
Hur kom det du hjälpte dig att uppnå dina mål?}

% the following sets the TOC entry to break after the & - note you have to include the first letter of the following word as it get swolled by the \texorpdfstring{}{} processing
\section[Hardware/Software design …/Model/Simulation model \&\texorpdfstring{\\}{ p} parameters/…]{Hardware/Software design …/Model/Simulation model \& parameters/…}
\sweExpl{Hårdvara / Mjukvarudesign ... / modell / Simuleringsmodell och parametrar / …}

Figure~\ref{fig:homepageicon} shows a simple icon for a home page. The time
to access this page when served will be quantified in a series of
experiments. The configurations that have been tested in the test bed are
listed in Table~\ref{tab:configstested}. In \SI{7.0}{\percent} of cases, there was an error indicating xxxxx.

\sweExpl{Figur~\ref{fig:homepageicon} visar en enkel ikon för en hemsida. Tiden för att få tillgång till den här sidan när den laddas kommer att kvantifieras i en serie experiment. De konfigurationer som har testats i provbänk listas ini tabell~\ref{tab:configstested}.\\
Vad du har gjort? Hur gjorde du det? Vilka designval gjorde du?}
 
\begin{figure}[!ht]
 \begin{center}
 \includegraphics[width=0.25\textwidth]{figures/Homepage-icon.png}
 \end{center}
 \caption{Homepage icon}
 \label{fig:homepageicon}
\end{figure}

\begin{table}[!ht]
 \begin{center}
 \caption{Configurations tested}
 \label{tab:configstested}
 \resizebox{\columnwidth}{!}{%
 \begin{tabular}{l|c} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
 \textbf{Configuration} & \textbf{Description} \\
 \hline
 1 & Simple test with one server\\
 2 & Simple test with one server\\
 \end{tabular}
 }
 \end{center}
\end{table}
\sweExpl{Testade konfigurationer}

\section{Implementation …/Modeling/Simulation/…}
\label{sec:implementationDetails}
\sweExpl{Implementering … / modellering / simulering / …}

Two commonly used simulators are:
\begin{description}[labelwidth =\widthof{\textbf{ns-2 or ns-3 simulator}}, leftmargin = !]
 \item[\textbf{Mininet}] This simulator uses traffic control (\texttt{tc}) to simulate network devices connected by links with specific bandwidth, packet loss rates, qdisc methods, etc.
 
 
 \item[\textbf{ns-2 or ns-3 simulator}] These simulators are very useful for simulating wireless communication links between moving devices. You can specify the mobility patterns of the nodes.
\end{description}

\subsection{Some examples of coding}
\engExpl{This section is simply to show some example of how you can include code in your thesis - this is not a section you would have in your thesis.}
\sweExpl{Det här avsnittet är helt enkelt för att visa ett exempel på hur du kan inkludera kod i ditt examensarbete - det här är inte ett avsnitt du skulle ha i ditt examensarbete.}

Listing~\ref{lst:helloWorldInC} shows an example of a simple program written
in C code.

\begin{lstlisting}[language={C}, caption={Hello world in C code}, label=lst:helloWorldInC]
int main() {
printf("hello, world");
return 0;
}
\end{lstlisting}

\engExpl{This template uses the package \texttt{lstlistings} for many different listings. Alternatively, one could use the \texttt{minited} package together with the \texttt{listings} environment, see, for example, \href{https://www.overleaf.com/learn/latex/Code_Highlighting_with_minted}{Code Highlighting with minted}. }

In contrast, Listing~\ref{lst:programmes} is an example of code in Python to
get a list of all of the programs at KTH. Note that on {\DTMsetdatestyle{iso}\DTMdisplaydate{2025}{6}{1}{-1}} the KOPPS \gls{API} will no longer work.
\engExpl{Note that the change to the iso date format was done in a group so that afterwards the date style returns to what it was, resulting in \DTMdisplaydate{2025}{6}{1}{-1}.}

\lstset{extendedchars=true} %% This allows character codes in the range 128-255
\begin{lstlisting}[language={Python}, caption={Using a python program to
 access the KTH \gls{API} to get all of the programs at KTH}, label=lst:programmes]
KOPPSbaseUrl = 'https://www.kth.se'

def v1_get_programmes():
 global Verbose_Flag
 #
 # Use the KOPPS \gls{API} to get the data
 # note that this returns XML
 url = "{0}/api/kopps/v1/programme".format(KOPPSbaseUrl)
 if Verbose_Flag:
 print("url: " + url)
 #
 r = requests.get(url)
 if Verbose_Flag:
 print("result of getting v1 programme: {}".format(r.text))
 #
 if r.status_code == requests.codes.ok:
 return r.text # simply return the XML
 #
 return None
\end{lstlisting}
\FloatBarrier

In \Cref{lst:exampleUsingMinted}, line \ref{listinline:ExcelWriter} shows the use of the \texttt{ExcelWriter} function. Line \ref{listinline:UsingToexcel} writes a panda dataframe (named \texttt{comp}) to the spreadsheet, while line \ref{listinline:closingWriter} closes the open spreadsheet.

\begin{listing}[!ht]
\begin{minted}[linenos,breaklines, escapeinside=||]{python}
import pandas as pd
def make_spreadsheet_of_differences(pds):
 global school
 publishers=set()
 writer = pd.ExcelWriter(f'/tmp/{school}_compare_duplicates.xlsx', engine='xlsxwriter') |\label{listinline:ExcelWriter}|
 print("starting")
 for idx, p in enumerate(pds):
 p0=list(p)[0]
 p1=list(p)[1]
 print(f"\nfor {p0} and {p1}")
 comp, publisher1, publisher2=compare_two_records_silent(p0, p1)
 sheet_name=f'{p0}'.split(':')[1]
 print(f'Wrote {sheet_name=}')|\label{listinline:UsingToexcel}|
 comp.to_excel(writer, sheet_name=sheet_name)
 publishers.add(publisher1)
 publishers.add(publisher2)
 
 # Close the Pandas Excel writer and output the Excel file.
 writer.close()|\label{listinline:closingWriter}|
 print(f"{publishers=}")
 return publishers
\end{minted}
\caption{Example of using \texttt{minted} with python code}
\label{lst:exampleUsingMinted}
\end{listing}

\FloatBarrier


\subsection{Some examples of figures in tikz}
\engExpl{This section is simply to show some example of how you can draw your own figures for in your thesis - this is not a section you would have in your thesis.}
\sweExpl{Det här avsnittet är helt enkelt för att visa ett exempel på hur du kan rita dina egna figurer i ditt examensarbete – det här är inte ett avsnitt du skulle ha i ditt examensarbete.}

\Needspace*{5\baselineskip}
These figures are just some examples to show that you can draw your own figures for your thesis. This has two advantages: \first you do not have to worry about copyrights -- as these are your own figures and \Second the text is now readable and not simply a picture of text -- so screen readers can read the figure's contents to someone who is listening to the contents of your thesis.

\subsubsection{Azure's Form Recognizer}
\Cref{fig:processAnInvoice} shows the processing of key-value extraction from a PDF document using Azure's Form Recognizer. 

\tikzset{
 processBox/.style={rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, font=\sffamily, draw=black, fill=red!20},
 largeBox/.style={rectangle, rounded corners, minimum width=3cm, minimum height=4cm,text centered, draw=black}
}
\begin{figure}[!ht]
\resizebox{1.1\textwidth}{!}{%
\begin{tikzpicture}
[align=left,node distance=2cm]

\node (document) [tape,tape bend top=none,draw,font=\sffamily] {PDF\\Document};
\node (GDM) [processBox, right=0.5cm of document] {OCR};
\node (OCRoutput) [largeBox, right=1cm of GDM] {OCR output};

\node (kvp) [tape,tape bend top=none,draw,font=\sffamily, below=0.25cm of OCRoutput.north] {key-value\\pairs};
\node (entities) [tape,tape bend top=none,draw,font=\sffamily, above=0.35cm of OCRoutput.south] {Entities};
\node (Manual) [processBox, right=1cm of kvp] {Analyze the extracted\\key-value pairs};
\draw [-latex](document) -- (GDM);
\draw [-latex](kvp) -- (Manual);
\path[ draw
 , -latex'] let \p1=(GDM.east), \p2=(kvp.west) in (GDM.east) -- +(0.25*\x2-0.25*\x1, \y1) -- +(0.5*\x2-0.5*\x1, \y2) -- (kvp.west);
\path[ draw
 , -latex'] let \p1=(GDM.east), \p2=(kvp.west), \p3=(entities.west) in (GDM.east) -- +(0.25*\x2-0.25*\x1, \y1) -- +(0.5*\x3-0.5*\x1, \y3) -- (entities.west);
\end{tikzpicture}
}
\caption{The processing of key-value extraction from a PDF document using Azure's Form Recognizer}
 \label{fig:processAnInvoice}
\end{figure}
\FloatBarrier
\subsubsection{Hyper-V with Containers}
 \Cref{fig:hyperVcontainers} shows how Hyper-V deals with containers.
 
 \tikzset{
 container/.style={rectangle, rounded corners, minimum width=2cm, minimum height=1cm,text centered, draw=black, fill=blue!20},
 containerization/.style={rectangle, rounded corners, minimum width=13.25cm, minimum height=1cm,text centered, draw=black, fill=blue!20},
 hypervisor/.style={rectangle, rounded corners, minimum width=13.25cm, minimum height=1cm,text centered, draw=black, fill=red!20},
 os/.style={rectangle, rounded corners, minimum width=13.25cm, minimum height=1cm,text centered, draw=black, fill=orange!20},
 guestos/.style={rectangle, rounded corners, minimum width=2cm, minimum height=1cm,text centered, draw=black, fill=orange!40},
 infrastructure/.style={rectangle, rounded corners, minimum width=13.25cm, minimum height=1cm,text centered, draw=black, fill=green!20},
 hos/.style={rectangle, rounded corners, minimum width=6cm, minimum height=1cm,text centered, draw=black, fill=orange!20},
 kernel/.style={rectangle, rounded corners, minimum width=6cm, minimum height=1cm,text centered, draw=black, fill=purple!20},
 services/.style={rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, draw=black, fill=pink!20]}
}

\begin{figure}[ht!]
 \centering
\resizebox{1\textwidth}{!}{%
\begin{tikzpicture}
[align=center,node distance=2cm]

\node (Infrastructure) [infrastructure, text width=13cm, text centered] {Infrastructure};
\node (OS1) [hos, anchor=north west, align=left, above=1.5cm of Infrastructure.north west, anchor=north west, text width=6cm, text centered] {Host OS};

\node (OS2) [hos, anchor= west, align=left, right=0.5cm of OS1.east, text width=6cm, anchor= west, text centered] {Host OS};

\node (Kernel1) [kernel, anchor=north west, align=left, above=1.5cm of OS1.north east, anchor=north east, text width=3cm, text centered] {Kernel};

\node (Kernel2) [kernel, anchor=north west, align=left, above=1.5cm of OS2.north east, anchor=north east, text width=3cm, text centered] {Kernel};

\node (ServiceA) [container, anchor=east, above=1 cm of Kernel1.east, anchor=east] {Services};
\node (AppA) [container, left=0.25cm of ServiceA] {App 1};

\node (ServiceB) [container, anchor=east, above=1 cm of Kernel2.east, anchor=east] {Services};
\node (AppB) [container, left=0.25cm of ServiceB] {App 2};
%\node (AppC) [container, right=0.25cm of AppB] {App 3};

\draw[black,thick,dashed] ($(OS2.north west)+(-0.3,3.75)$) rectangle ($(OS2.south east)+(0.5,-0.3)$);
\node[text width=5cm, text=red, above=0.1cm of ServiceB] 
 {\textbf{Container}};

\draw[red,thick,dotted] ($(Kernel2.north west)+(-0.3,1.6)$) rectangle ($(Kernel2.south east)+(0.3,-0.3)$);
\node[text width=5cm, text=black, above=0.8cm of ServiceB] 
 {\textbf{VM}};
\end{tikzpicture}
}
 \caption{Hyper-V with containers}
 \label{fig:hyperVcontainers}
\end{figure}
\FloatBarrier
\subsubsection{\glsfmtshort{VM} versus Containers}
\Cref{fg:vmsVersusContainers} shows a comparison of virtual machines (VMs) versus containers.

\begin{figure*}[ht!]
 \centering
 \begin{subfigure}[t]{0.5\textwidth}
 \centering
\resizebox{1\textwidth}{!}{%
\begin{tikzpicture}
[align=left,node distance=2cm]

\node (AppA) [container,align=left] {App 1};
\node (AppB) [container, right=0.25cm of AppA] {App 2};
\node (AppC) [container, right=0.25cm of AppB] {App 3};

\node (GosA) [guestos,align=left, below=0.25cm of AppA.south west,anchor=north west] {Guest OS};
\node (GosB) [guestos, right=0.25cm of GosA] {Guest OS};
\node (GosC) [guestos, right=0.25cm of GosB] {Guest OS};

\draw [decoration={brace,amplitude=0.5em},decorate, ultra thick,gray, transform canvas={xshift = 0.5cm}]
 (AppC.north -| AppC.east) -- (GosC.south -| AppC.east);
\node[text width=5cm, right=1cm of GosC.north east] 
 {\textbf{VMs}};

\node (Hypervisor) [hypervisor, anchor=north west, align=left, below=0.25cm of GosA.south west, anchor=north west, text width=13cm, text centered] {Hypervisor};

\node (OS) [os, anchor=north west, align=left, below=0.25cm of Hypervisor.south west, anchor=north west, text width=13cm, text centered] {Host OS};

\node (Infrastructure) [infrastructure, anchor=north west, align=left, below=0.25cm of OS.south west, anchor=north west, text width=13cm, text centered] {Infrastructure};


\end{tikzpicture}
}
 \caption{VM}
 \end{subfigure}%
 ~ 
 \begin{subfigure}[t]{0.5\textwidth}
 \centering
 \resizebox{1\textwidth}{!}{%
\begin{tikzpicture}
[align=left,node distance=2cm]

\node (AppA) [container,align=left] {App 1};
\node (AppB) [container, right=0.25cm of AppA] {App 2};
\node (AppC) [container, right=0.25cm of AppB] {App 3};
\node[text width=5cm, right=0.25cm of AppC] 
 {\textbf{Apps running in Containers}};


\node (Containerization) [containerization, anchor=north west, align=left, below=0.25cm of AppA.south west, anchor=north west, text width=13cm, text centered] {Docker Engine};

\node (OS) [os, anchor=north west, align=left, below=0.25cm of Containerization.south west, anchor=north west, text width=13cm, text centered] {Host OS};

\node (Infrastructure) [infrastructure, anchor=north west, align=left, below=0.25cm of OS.south west, anchor=north west, text width=13cm, text centered] {Infrastructure};


\end{tikzpicture}
}
 \caption{Containers}
 \end{subfigure}
 \caption{Virtual machines (VMs) versus Containers}
 \label{fg:vmsVersusContainers}
\end{figure*}

\cleardoublepage
\chapter{Results and Analysis}
\label{ch:resultsAndAnalysis}
\sweExpl{svensk: Resultat och Analys}

\engExpl{Sometimes this is split into two chapters.\\Keep in mind: How you are going to evaluate what you have done? What are your metrics?\\Analysis of your data and proposed solution\\Does this meet the goals which you had when you started?}

In this chapter, we present the results and discuss them.

\sweExpl{I detta kapitel presenterar vi resultaten och diskutera dem.\\Ibland delas detta upp i två kapitel.\\Hur du ska utvärdera vad du har gjort? Vad är din statistik?\\Analys av data och föreslagen lösning\\Innebär detta att uppfyllelse av de mål som du hade när du började?}

\section{Major results}
\sweExpl{Huvudsakliga resultat}

Some statistics of the delay measurements are shown in Table~\ref{tab:delayMeasurements}.
The delay has been computed from the time the GET request is received until the response is sent.

\sweExpl{Lite statistik av fördröjningsmätningarna visas i Tabell~\ref{tab:delayMeasurements}. Förseningen har beräknats från den tidpunkt då begäran GET tas emot fram till svaret skickas.}

\begin{table}[!ht]
 \begin{center}
 \caption{Delay measurement statistics}
 \label{tab:delayMeasurements}
 \begin{tabular}{l|S[table-format=4.2]|S[table-format=3.2]} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
 \textbf{Configuration} & \textbf{Average delay (ns)} & \textbf{Median delay (ns)}\\
 \hline
 1 & 467.35 & 450.10\\
 2 & 1687.5 & 901.23\\
 \end{tabular}
 \end{center}
\end{table}

Table \ref{tab:ping_results} shows the measurement of round trip times from four hosts to and from a server.
\begin{table}[ht!]
\caption[RTT for 4 hosts]{Result for the ping measurements of RTT for 4 hosts} 
\label{tab:ping_results}
\vspace{1em}
\centering
\begin{tabular}{l *{4}{S[table-format=2.3]}}
{} & \multicolumn{4}{c}{host to server RTT in ms} \\
\cmidrule{2-5}
Host & \multicolumn{1}{c}{min} & \multicolumn{1}{c}{avg} & \multicolumn{1}{c}{max} & \multicolumn{1}{c}{mdev} \\
\midrule
h1 & 5.625 & 5.625 & 5.625 & 0.0 \\
h2 & 2.909 & 2.909 & 1.909 & 0.0 \\
h3 & 5.007 & 5.007 & 5.007 & 0.0 \\
h4 & 2.308 & 2.308 & 2.308 & 0.0 \\
\midrule
\end{tabular}
\end{table}
\FloatBarrier

\sweExpl{Fördröj mätstatistik}
\sweExpl{Konfiguration | Genomsnittlig fördröjning (ns) | Median fördröjning (ns)}

Figure \ref{fig:processing_vs_payload_length} shows an example of the performance as measured in the experiments.

\begin{figure}[!ht]
% GNUPLOT: LaTeX picture
\setlength{\unitlength}{0.240900pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi
\begin{picture}(1500,900)(0,0)
\sbox{\plotpoint}{\rule[-0.200pt]{0.400pt}{0.400pt}}%
\put(171.0,131.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(151,131){\makebox(0,0)[r]{ 1.5}}
\put(1419.0,131.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(171.0,212.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(151,212){\makebox(0,0)[r]{ 2}}
\put(1419.0,212.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(171.0,292.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(151,292){\makebox(0,0)[r]{ 2.5}}
\put(1419.0,292.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(171.0,373.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(151,373){\makebox(0,0)[r]{ 3}}
\put(1419.0,373.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(171.0,454.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(151,454){\makebox(0,0)[r]{ 3.5}}
\put(1419.0,454.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(171.0,534.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(151,534){\makebox(0,0)[r]{ 4}}
\put(1419.0,534.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(171.0,615.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(151,615){\makebox(0,0)[r]{ 4.5}}
\put(1419.0,615.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(171.0,695.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(151,695){\makebox(0,0)[r]{ 5}}
\put(1419.0,695.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(171.0,776.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(151,776){\makebox(0,0)[r]{ 5.5}}
\put(1419.0,776.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(171.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(171,90){\makebox(0,0){ 0}}
\put(171.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(298.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(298,90){\makebox(0,0){ 10}}
\put(298.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(425.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(425,90){\makebox(0,0){ 20}}
\put(425.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(551.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(551,90){\makebox(0,0){ 30}}
\put(551.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(678.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(678,90){\makebox(0,0){ 40}}
\put(678.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(805.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(805,90){\makebox(0,0){ 50}}
\put(805.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(932.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(932,90){\makebox(0,0){ 60}}
\put(932.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1059.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1059,90){\makebox(0,0){ 70}}
\put(1059.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1185.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1185,90){\makebox(0,0){ 80}}
\put(1185.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1312.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1312,90){\makebox(0,0){ 90}}
\put(1312.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1439.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1439,90){\makebox(0,0){ 100}}
\put(1439.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(171.0,131.0){\rule[-0.200pt]{0.400pt}{155.380pt}}
\put(171.0,131.0){\rule[-0.200pt]{305.461pt}{0.400pt}}
\put(1439.0,131.0){\rule[-0.200pt]{0.400pt}{155.380pt}}
\put(171.0,776.0){\rule[-0.200pt]{305.461pt}{0.400pt}}
\put(30,453){\rotatebox{-270}{\makebox(0,0){Processing time (ms)}}}
\put(805,29){\makebox(0,0){Payload size (bytes)}}
\put(868.0,131.0){\rule[-0.200pt]{0.400pt}{84.074pt}}
\put(995.0,131.0){\rule[-0.200pt]{0.400pt}{98.287pt}}
\put(1173.0,131.0){\rule[-0.200pt]{0.400pt}{118.041pt}}
\put(1325.0,131.0){\rule[-0.200pt]{0.400pt}{134.904pt}}
\put(1350.0,131.0){\rule[-0.200pt]{0.400pt}{137.795pt}}
\put(1439.0,131.0){\rule[-0.200pt]{0.400pt}{155.380pt}}
\end{picture}
\caption[A GNUplot figure]{Processing time vs. payload length}\vspace{0.5cm}
\label{fig:processing_vs_payload_length}
\end{figure}
\FloatBarrier		

Given these measurements, we can calculate our processing bit rate as the inverse of the time it takes to process an additional byte divided by 8 bits per byte:

\[
	\text{bit rate} = \frac{1}{\frac{\text{time}_{\text{byte}}}{8}} = 20.03 \quad kb/s
\] 

\Cref{tab:majorMarkupLMDetailedResult} shows another table in which some values have been set in bold (using \textbackslash B) to emphasize them. Note how the \texttt{S} formatting has been modified so that it considers the weight of the characters and this is able to decimal align even these hold-faced numbers with the numbers in the column above them.

\begin{table}[!ht]
 \centering
 \caption{Median values of sandwich attributes}
 \label{tab:majorMarkupLMDetailedResult}
 \begin{tabular}{l *{2}{S[detect-weight,mode=text,table-format=3.2]}}
 & \multicolumn{2}{c}{\textbf{sites}}\\
 \cmidrule{2-3}
 \textbf{Attribute} & \textbf{A} & \textbf{B} \\
 \midrule
 price (in SEK) & 36.5 & 71.3 \\
 protean (g) & 97.2 & 100.0 \\
 salt (mg) & 9.7 & 9.3 \\
 \hline
 \textbf{Average customer rating in \%} & \B 82.2 & \B 89.9 \\
 \midrule
 \end{tabular}
\end{table}
\FloatBarrier


\Needspace*{4\baselineskip}
\Cref{fig:stackedrust} shows a stacked bar chart using pgfplots. It illustrates how easy it is to take a set of data and make a stacked bar plot. One of the features is the shifted values -- this is very useful when the bar itself is too small to put the value into.

\pgfplotstableread{
Label Numbers Refs Struct/Enum Heap Arrays
cratesio 70.04 19.83 8.31 1.3 0.52
librs 49.26 30.49 10.80 7.92 1.53
rustc 55.01 24.80 11.54 6.16 2.49
}\testdata


\pgfkeys{
 /pgf/number format/.cd,
 fixed,
 fixed zerofill,
 precision=2
}
\begin{figure}[ht!]
 \centering
 \scalebox{0.9}{
 \begin{tikzpicture}
 \begin{axis}[
 ybar stacked,
 %reverse legend,
 reverse legend=false,
 %https://tex.stackexchange.com/questions/88892/pgfplots-bar-plot-spacing-inbetween-bars
 enlarge x limits=0.4,
	 bar width=45pt,
 /pgfplots/nodes near coords*/.append style={
 every node near coord/.style={
 color=black,
 font=\small,
 name=X,
% shift={ 
% (50pt,25pt)
% },
 xshift={50pt},
 yshift ={
 ifthenelse((\plotnum == 4), 30pt,20pt)},
 },
 scatter/@post marker code/.append code={
 \node(Y){};
 \draw(X)--(Y.center);
 }
 },
	 nodes near coords,
 bar shift=5pt,
 ymin=0,
 ymax=115,
 xtick=data,
 width=1\textwidth,
 legend style={draw=none},
 legend image post style={scale=2.0},
 legend style={
 at={(0.5,-0.2)},
 anchor=north,
 legend columns=-2,
 font=\large,
 %mark size=20pt,
 },
 ylabel=Percentage points (\%),
 xticklabels from table={\testdata}{Label},
 xticklabel style={rotate=30},
 ]
 \addplot table [y=Numbers, meta=Label, x expr=\coordindex] {\testdata};
 \addlegendentry{Numbers}
 \addplot table [y=Refs, meta=Label, x expr=\coordindex] {\testdata};
 \addlegendentry{Refs}
 \addplot table [y=Struct/Enum, meta=Label, x expr=\coordindex] {\testdata};
 \addlegendentry{Struct/Enum}
 \addplot table [y=Heap, meta=Label, x expr=\coordindex] {\testdata};
 \addlegendentry{Heap}
 \addplot table [y=Arrays, meta=Label, x expr=\coordindex] {\testdata};
 \addlegendentry{Arrays}
 \end{axis}
 \end{tikzpicture}}
\caption{Rust types distribution for the compiler, crates.io, and lib.rs.
(percentage) - appears here with the permission of the author - see the thesis at \url{https://urn.kb.se/resolve?urn=urn\%3Anbn\%3Ase\%3Akth\%3Adiva-332124}}
\label{fig:stackedrust}
\end{figure}
\FloatBarrier



\section{Reliability Analysis}
\sweExpl{Analys av tillförlitlighet\\
Tillförlitlighet i metod och data}

\section{Validity Analysis}
\sweExpl{Analys av validitet\\
Validitet i metod och data}

\cleardoublepage
\chapter{Discussion}
\label{ch:discussion}
\sweExpl{Diskussion\\
Förbättringsförslag?}
\generalExpl{This can be a separate chapter or a section in the previous chapter.}

\cleardoublepage
\chapter{Conclusions and Future work}
\label{ch:conclusionsAndFutureWork}
\sweExpl{Slutsats och framtida arbete}

\generalExpl{Add text to introduce the subsections of this chapter.}

\section{Conclusions}
\label{sec:conclusions}
\sweExpl{Slutsatser}
\engExpl{Describe the conclusions (reflect on the whole introduction given in Chapter 1).}


 
\engExpl{Discuss the positive effects and the drawbacks.\\
Describe the evaluation of the results of the degree project.\\
Did you meet your goals?\\
What insights have you gained?\\
What suggestions can you give to others working in this area?\\
If you had it to do again, what would you have done differently?}

\sweExpl{Uppfyllde du dina mål?\\
Vilka insikter har du fått?\\
Vilka förslag kan du ge till andra som arbetar inom detta område?
Om du skulle göra detta igen, vad skulle du ha gjort annorlunda?}

\section{Limitations}
\label{sec:limitations}
\sweExpl{Begränsande faktorer\\Vad gjorde du som begränsade dina ansträngningar? Vilka är begränsningarna i dina resultat?}
\engExpl{What did you find that limited your efforts? What are the limitations of your results?}


\section{Future work}
\label{sec:futureWork}
\sweExpl{Vad du har kvar ogjort?\\Vad är nästa självklara saker som ska göras?\\Vad tips kan du ge till nästa person som kommer att följa upp på ditt arbete?}
\engExpl{Describe valid future work that you or someone else could or should do.\\
Consider: What you have left undone? What are the next obvious things to be done? What hints can you give to the next person who is going to follow up on your work?}



Due to the breadth of the problem, only some of the initial goals have been
met. In these section we will focus on some of the remaining issues that
should be addressed in future work. ...

\subsection{What has been left undone?}
\label{what-has-been-left-undone}

The prototype does not address the third requirment, \ie a yearly unavailability of less than 3 minutes; this remains an open problem. ...

\subsubsection{Cost analysis}
\generalExpl{Example of a missing component}
The current prototype works, but the performance from a cost perspective makes this an impractical solution. Future work must reduce the cost of this solution; to do so, a cost analysis needs to first be done. ...

\subsubsection{Security}
\generalExpl{Example of a missing component}
A future research effort is needed to address the security holes that results from using a self-signed certificate. Page filling text mass. Page filling text mass. ...


\subsection{Next obvious things to be done}

In particular, the author of this thesis wishes to point out xxxxxx remains as a problem to be solved. Solving this problem is the next thing that should be done. ...

\section{Reflections}
\label{sec:reflections}
\sweExpl{Reflektioner}
\sweExpl{Vilka är de relevanta ekonomiska, sociala, miljömässiga och etiska aspekter av ditt arbete?}
\engExpl{What are the relevant economic, social,
 environmental, and ethical aspects of your work?
}



One of the most important results is the reduction in the amount of
energy required to process each packet while at the same time reducing the
time required to process each packet.

The thesis contributes to the \gls{UN}\enspace\glspl{SDG} numbers 1 and 9 by
xxxx. 




\noindent\rule{\textwidth}{0.4mm}
\engExpl{In the references, let Zotero or other tool fill this in for you. I suggest an extended version of the IEEE style, to include URLs, DOIs, ISBNs, etc., to make it easier for your reader to find them. This will make life easier for your opponents and examiner. \\IEEE Editorial Style Manual: \url{https://www.ieee.org/content/dam/ieee-org/ieee/web/org/conferences/style_references_manual.pdf}}
\sweExpl{Låt Zotero eller annat verktyg fylla i det här för dig. Jag föreslår en utökad version av IEEE stil - att inkludera webbadresser, DOI, ISBN osv. - för att göra det lättare för läsaren att hitta dem. Detta kommer att göra livet lättare för dina opponenter och examinator.}

\cleardoublepage
% Print the bibliography (and make it appear in the table of contents)
\renewcommand{\bibname}{References}


\ifbiblatex
 %\typeout{Biblatex current language is \currentlang}
 \printbibliography[heading=bibintoc]
\else
 \phantomsection % make it include a hyperref - see https://tex.stackexchange.com/a/98995
 \addcontentsline{toc}{chapter}{References}
 \bibliography{references}
\fi



\warningExpl{If you do not have an appendix, do not include the \textbackslash cleardoublepage command below; otherwise, the last page number in the metadata will be one too large.}
\cleardoublepage
\appendix
\renewcommand{\chaptermark}[1]{\markboth{Appendix \thechapter\relax:\thinspace\relax#1}{}}
\chapter{Supporting materials}
\label{sec:supportingMaterial}
\generalExpl{Here is a place to add supporting material that can help others build upon your work. You can include files as attachments to the PDF file or indirectly via URLs. Alternatively, consider adding supporting material uploaded as separate files in DiVA.}

% Attach the BibTeX for your references to make it easy for a reader to find and use them
The BibTeX references used in this thesis are attached. \attachfile[description={references.bib}]{references.bib}

% Attach source code file(s) or add a URL to the github or other repository
Some source code relevant to this project can be found at \url{https://github.com/gqmaguirejr/E-learning} and \url{https://github.com/gqmaguirejr/Canvas-tools}.

Your reader can access the attached (embedded) files using a PDF tool such as Adobe Acrobat Reader using the paperclip icon in the left menu, as shown in \Cref{fig:PDFreaderPaperclipExample} or by right-clicking on the push-pin icon in the PDF file and then using the menu to save the embedded file as shown in \Cref{fig:PDFreaderPushpinExample}.

An argument for including supporting material in the PDF file is that it will be available to anyone who has a copy of the PDF file. As a result, they do not have to look elsewhere for this material. This comes at the cost of a larger PDF file. However, the embedded files are encoded into a compressed stream within the PDF file; thus, reducing the number of additional bytes. For example, the references.bib file that was used in this example is \SI{10617}{\byte} in size but only occupies \SI{4261}{\byte} in the PDF file.

\warningExpl{DiVA is limited to $\approx$\SI{1}{\giga\byte} for each supporting file. If you have very large amounts of supporting material, you will probably want to use one of the data repositories. For additional help with this, contact KTH Library via 
\href{mailto:researchdata@kth.se}{researchdata@kth.se}.\\As of Spring 2024, there are plans to migrate this supporting data from DiVA to a research data repository.
}

\begin{figure}[!ht]
 \begin{center}
 \includegraphics[width=0.50\textwidth]{README_notes/pdf-viewer-attached-files.png}
 \end{center}
 \caption{Adobe Acrobat Reader using the paperclip icon for the attached references.bib file}
 \label{fig:PDFreaderPaperclipExample}
\end{figure}
\FloatBarrier

\begin{figure}[!ht]
 \begin{center}
 \includegraphics[width=0.99\textwidth]{README_notes/Bib-save-embedded-example.png}
 \end{center}
 \caption{Adobe Acrobat Reader after right-clicking on the push-pin icon for the attached references.bib file}
 \label{fig:PDFreaderPushpinExample}
\end{figure}
\FloatBarrier
\cleardoublepage

\chapter{Something Extra}
\sweExpl{svensk: Extra Material som Bilaga}

\section{Just for testing KTH colors}
\ifdigitaloutput
 \textbf{You have selected to optimize for digital output}
\else
 \textbf{You have selected to optimize for print output}
\fi
\begin{itemize}[noitemsep]
 \item Primary color
 \begin{itemize}
 \item \textcolor{kth-blue}{kth-blue \ifdigitaloutput
 actually Deep sea
 \fi} {\color{kth-blue} \rule{0.3\linewidth}{1mm} }\\

 \item \textcolor{kth-blue80}{kth-blue80} {\color{kth-blue80} \rule{0.3\linewidth}{1mm} }\\
\end{itemize}

\item Secondary colors
\begin{itemize}[noitemsep]
 \item \textcolor{kth-lightblue}{kth-lightblue \ifdigitaloutput
 actually Stratosphere
 \fi} {\color{kth-lightblue} \rule{0.3\linewidth}{1mm} }\\

 \item \textcolor{kth-lightred}{kth-lightred \ifdigitaloutput
 actually Fluorescence\fi} {\color{kth-lightred} \rule{0.3\linewidth}{1mm} }\\

 \item \textcolor{kth-lightred80}{kth-lightred80} {\color{kth-lightred80} \rule{0.3\linewidth}{1mm} }\\

 \item \textcolor{kth-lightgreen}{kth-lightgreen \ifdigitaloutput
 actually Front-lawn\fi} {\color{kth-lightgreen} \rule{0.3\linewidth}{1mm} }\\

 \item \textcolor{kth-coolgray}{kth-coolgray \ifdigitaloutput
 actually Office\fi} {\color{kth-coolgray} \rule{0.3\linewidth}{1mm} }\\

 \item \textcolor{kth-coolgray80}{kth-coolgray80} {\color{kth-coolgray80} \rule{0.3\linewidth}{1mm} }
\end{itemize}
\end{itemize}

\textcolor{black}{black} {\color{black} \rule{\linewidth}{1mm} }

% Include an example of using nomenclature
\ifnomenclature
 \cleardoublepage
 \chapter{Main equations}
 \label{ch:NomenclatureExamples}
 This appendix gives some examples of equations that are used throughout this thesis.
 \section{A simple example}
 The following example is adapted from Figure 1 of the documentation for the package nomencl (\url{https://ctan.org/pkg/nomencl}).
 \begin{equation}\label{eq:mainEq}
 a=\frac{N}{A}
 \end{equation}
 \nomenclature{$a$}{The number of angels per unit area\nomrefeq}% %% include the equation number in the list
 \nomenclature{$N$}{The number of angels per needle point\nomrefpage}% %% include the page number in the list
 \nomenclature{$A$}{The area of the needle point}%
 The equation $\sigma = m a$%
 \nomenclature{$\sigma$}{The total mass of angels per unit area\nomrefeqpage}%
 \nomenclature{$m$}{The mass of one angel}
follows easily from \Cref{eq:mainEq}.

 \section{An even simpler example}
 The formula for the diameter of a circle is shown in \Cref{eq:secondEq} area of a circle in \cref{eq:thirdEq}.
 \begin{equation}\label{eq:secondEq}
 D_{circle}=2\pi r
 \end{equation}
 \nomenclature{$D_{circle}$}{The diameter of a circle\nomrefeqpage}%
 \nomenclature{$r$}{The radius of a circle\nomrefeqpage}%

 \begin{equation}\label{eq:thirdEq}
 A_{circle}=\pi r^2
 \end{equation}
 \nomenclature{$A_{circle}$}{The area of a circle\nomrefeqpage}%

 Some more text that refers to \eqref{eq:thirdEq}.
\fi %% end of nomenclature example

\cleardoublepage
% Information for authors
%\include{README_author}
\subfile{README_author}

\cleardoublepage
% information about the template for everyone
\input{README_notes/README_notes}

\begin{comment}
% information for examiners
\ifxeorlua
\cleardoublepage
\input{README_notes/README_examiner_notes}
\fi
\end{comment}

\begin{comment}
% Information for administrators
\ifxeorlua
\cleardoublepage
\input{README_notes/README_for_administrators.tex}
\fi
\end{comment}

\begin{comment}
% Information for Course Coordinators
\ifxeorlua
\cleardoublepage
\input{README_notes/README_for_course_coordinators}
\fi
\end{comment}

%% The following label is necessary for computing the last page number of the body of the report to include in the "For DIVA" information
\label{pg:lastPageofMainmatter}

\cleardoublepage
\clearpage\thispagestyle{empty}\mbox{} % empty page with backcover on the other side
\kthbackcover
\fancyhead{} % Do not use header on this extra page or pages
\section*{€€€€ For DIVA €€€€}
\lstset{numbers=none} %% remove any list line numbering
\divainfo{pg:lastPageofPreface}{pg:lastPageofMainmatter}

% If there is an acronyms.tex file,
% add it to the end of the For DIVA information
% so that it can be used with the abstracts
% Note that the option "nolol" stops it from being listed in the List of Listings

% The following bit of ugliness is because of the problems PDFLaTeX has handling a non-breaking hyphen
% unless it is converted to UTF-8 encoding.
% If you do not use such characters in your acronyms, this could be simplified.
\ifxeorlua
\IfFileExists{lib/acronyms.tex}{
\section*{acronyms.tex}
\lstinputlisting[language={[LaTeX]TeX}, nolol, basicstyle=\ttfamily\color{black},
commentstyle=\color{black}, backgroundcolor=\color{white}]{lib/acronyms.tex}
}
{}
\else
\IfFileExists{lib/acronyms-for-pdflatex.tex}{
\section*{acronyms.tex}
\lstinputlisting[language={[LaTeX]TeX}, nolol, basicstyle=\ttfamily\color{black},
commentstyle=\color{black}, backgroundcolor=\color{white}]{lib/acronyms-for-pdflatex.tex}
}
{}
\fi


\end{document}
