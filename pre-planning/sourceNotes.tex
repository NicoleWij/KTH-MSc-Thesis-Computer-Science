\documentclass[11pt]{article}
\usepackage{dirtytalk}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage[hidelinks]{hyperref}
\geometry{margin=0.75in}

% Nicely-formatted bullet lists
\usepackage{enumitem}
\setlist[itemize]{label=--, leftmargin=*, itemsep=0.4em}

% Optional: simple checkbox symbols (uncomment if you want them)
% \usepackage{pifont}
% \newcommand{\checkbox}{\ding{113}\;}   % empty box
% \newcommand{\checked}{\ding{51}\;}     % check mark

\title{Pre-Study Task Notes}
\date{\today}

\begin{document}
\maketitle

\section*{Event-Driven vs Polling Ingestion}
\begin{itemize}
  \item[] \textbf{1. Analyzing the Efficiency of Event-Driven APIs}
  \begin{itemize}
      \item[] \textbf{Scope \& method}  This is a bachelor-level case study at Bolagsverket that builds three prototypes. These are: MQTT broker, WebSocket server, and the agency's legacy SFTP flow. It then measures nine resource metrics (CPU, duration, network I/O, memory, disk, threads, handles) while replaying messages of increasing payload sizes.
      
      \item[] \textbf{Key findings}  MQTT and WebSocket cut latency and roughly halve CPU/network usage compared with SFTP for typical payloads; MQTT stays fastest as payloads grow, whereas WebSocket performance drops sharply beyond ≈4 kB; SFTP shows linear disk-I/O growth, which signals a bottleneck for bulk updates.
      
      \item[] \textbf{Relevance}  Mirrors this thesis's ingestion axis (polling-style file transfer vs.\ event-driven push) and logs the same resource counters planned for the Klimra benchmark, providing both a comparable metric suite and an additional empirical data point from a Swedish public-sector context.
  \end{itemize}
  \item[] \textbf{2. Migration of a SCADA system to IaaS clouds – a case study}
  \begin{itemize}
      \item[] \textbf{Scope \& method} The authors took a factory-control system (SCADA) that normally \emph{polls} hundreds of sensors every millisecond and moved it to virtual machines in the cloud.  They timed each request and also watched how much CPU, memory, and network traffic the system used.
      
      \item[] \textbf{Key findings} As the number of sensors grew, the round-trip time for each poll exploded—from about 0.01 s with one sensor to roughly 14 s with 800 sensors.  Almost all of that delay happened inside the software, not on the network, showing that constant polling can overload the server itself.
      
      \item[] \textbf{Relevance} This study is a clear real-world example of how polling can become a performance bottleneck.  It backs up my argument that switching to event-driven updates can cut wasted work, useful support for the \textit{ingestion‐strategy} part of my comparison, even though the paper says nothing about databases.
      \item[] \href{https://link.springer.com/article/10.1186/s13677-017-0080-5}{LINK}
  \end{itemize}
  \item[] \textbf{3. Energy-efficient network protocols for domestic IoT application design}
  \begin{itemize}
      \item[] \textbf{Scope \& method} Measured the power draw of five common wireless links (Bluetooth, BLE, ZigBee, Wi-Fi, 433 MHz) in three communication styles—\emph{broadcast}, \emph{polling}, and \emph{event-driven}.  
      A toaster-to-gateway \say{smart kitchen} scenario was used to calculate weekly energy use at different event rates.
      
      \item[] \textbf{Key findings} With infrequent events, event-driven traffic is far cheaper: Bluetooth polling consumed ≈3000× more energy than event-driven for 20 events/week, and Wi-Fi about 10 000×.  
      Above a threshold (~7–10 events/week for their RF433 example), polling or even broadcast becomes the cheaper option, illustrating a clear crossover point between idle overhead and high-rate efficiency.
      
      \item[] \textbf{Relevance} Although focused on battery life rather than latency, the paper nicely quantifies the \textit{same polling-vs-push trade-off} under varying event frequencies.  
      I can cite it to show that the “best” ingestion style depends on workload intensity—an insight that complements my latency-centric benchmark and helps justify testing multiple load levels.
      \item[] \href{https://www.researchgate.net/publication/335426973\_Energy-Efficient\_Network\_Protocols\_for\_Domestic\_IoT\_Application\_Design}{LINK}
  \end{itemize}
  \item[] \textbf{4. The Power of Event-Driven Architecture: Enabling Real-Time Systems and Scalable Solutions}
  \begin{itemize}
      \item[] \textbf{Scope \& method} A narrative review that explains what Event-Driven Architecture (EDA) is, why big platforms (Netflix, Uber) use it, and which technologies (Kafka, serverless functions) make it work.  No new experiments; evidence comes from published case studies and industry reports.
      
      \item[] \textbf{Key points} EDA decouples producers and consumers so systems can react immediately to events instead of polling.  Reported benefits include 40-90 \% drops in CPU or response-time figures in large data-centre workloads; main challenges are event “storming,” eventual consistency, and debugging complex flows.
      
      \item[] \textbf{Relevance} Gives high-level motivation and common pitfalls for switching from polling to push.  Useful as background to frame my benchmark results and to cite generic pros/cons of EDA, but it does not supply hard latency numbers or any database insight—so it will stay a supporting, not central, reference.
  \end{itemize}
  \item[] \textbf{5. On the impact of event-driven architecture on performance: An exploratory study}
  \begin{itemize}
      \item[] \textbf{Scope \& method}  Builds two \emph{identical} web-shop applications: one classic monolith, one decomposed into microservices that talk through an event bus.  
      Runs the same user-load tests on each version and records CPU, RAM, response time, throughput, and the number of network packets.
      
      \item[] \textbf{Key findings}  Contrary to common \say{EDA is faster} claims, the monolith used \emph{less} CPU and memory and answered requests more quickly in every load scenario tested.  
      The event-driven version spent extra time routing and serialising messages, so its raw throughput advantage came at the cost of higher resource use.
      
      \item[] \textbf{Relevance}  A cautionary counter-example that event-driven designs can hurt performance if the message overhead is not tuned.  
      Useful to quote when explaining why my benchmark measures both latency and resource cost instead of assuming EDA will always win.
      \item[] \href{https://www.sciencedirect.com/science/article/pii/S0167739X23003977}{LINK}
  \end{itemize}
  \item[] \textbf{6. Event-Driven Microservices Architecture for Data Center Orchestration}
  \begin{itemize}
      \item[] \textbf{Scope \& method}  A 2025 tutorial-style journal article that surveys why large data-centers are replacing polling-based orchestration with event-driven microservices.  It synthesises industry case studies (Netflix, telcos, banks) and cites benchmark figures from earlier work; no new experiments are run.
      
      \item[] \textbf{Key points}  Polling wastes 22–31 \% CPU and delays actions by ≈40 s in big clusters; switching to event streams cuts CPU by ~47 \% and shrinks response time by ~89 \%.  The paper explains core EDA patterns (event sourcing, CQRS, sagas, circuit-breaker) and warns about \say{event storms} and debugging complexity.
      
      \item[] \textbf{Relevance}  Good high-level background: reinforces the motivation for testing push vs. polling and lists design patterns I can reference when building Klimra prototypes.  Because it is a review, not a benchmark, I will use it as supporting context rather than as evidence for performance numbers in my results chapter.
      \item[] \href{https://www.ijsat.org/research-paper.php?id=3113}{LINK}
  \end{itemize}
\end{itemize}

\newpage
\section*{Performance of Time-Series vs Relational vs NoSQL Databases Under High-Ingestion Workloads}
\subsection*{Background and Context}
Modern data-intensive applications (e.g. IoT sensor networks, financial tick feeds, telemetry) generate time-series data at high velocity. Choosing the right database system is critical for real-time ingestion and querying of this data. Traditional relational databases (SQL/RDBMS) can struggle with the volume and sequential nature of time-stamped data, while specialized time-series databases (TSDBs) and NoSQL databases promise optimizations for high ingestion rates and scalable query performance. Recent research has empirically compared these systems under heavy write loads and real-time query scenarios, often using IoT or sensor data as benchmarks. Below, we summarize several peer-reviewed studies and academic theses that benchmark time-series vs. relational vs. NoSQL databases (including AWS services like Amazon Timestream, DynamoDB, and RDS PostgreSQL) under high-ingestion or real-time workloads. We highlight both performance results and methodology (datasets, queries, and metrics) from each source, to inform database selection for such use cases.
\begin{itemize}
    \item[] \textbf{1. Comparative Analysis of Time Series Databases in the Context of Edge Computing for Low Power Sensor Networks}
    \begin{itemize}
      \item[] \textbf{Scope \& method}  Benchmarks three TSDBs (TimescaleDB, InfluxDB, Riak TS) and two relational engines (PostgreSQL, SQLite) on a Raspberry Pi, using a Python generator that inserts and queries 28 800 synthetic sensor readings; records insert rate and three aggregation-query latencies.
      
      \item[] \textbf{Key findings}  PostgreSQL matched or beat the TSDBs on this small edge device: fastest inserts (1,756 rows/s in 10-row batches) and tied with InfluxDB for read throughput.  TimescaleDB showed no gain over vanilla PostgreSQL, and SQLite lagged badly.  Conclusion: on very constrained hardware, a tuned relational DB can outperform specialised stores.
      
      \item[] \textbf{Relevance}  Useful counter-example for my storage axis: shows that TSDB advantages shrink on low-power nodes, so I should not assume TimescaleDB will always win when server resources are limited.  I will cite it when justifying the need to measure resource usage as well as raw latency in the Klimra benchmark.
  \end{itemize}
    \item[] \textbf{2. Performance Analysis of Time Series Databases for IoT Applications}
    \begin{itemize}
        \item[] \textbf{Scope \& method} A 2024 bachelor thesis from Uppsala University that loads 1 – 50 million real sensor readings into two TSDBs (InfluxDB 2.x, TimescaleDB 2.x) and a relational baseline (MariaDB/MySQL). Four query types are timed: single point lookup, time-range selection, simple aggregate, and windowed (\say{time-sensitive}) aggregate.
      
        \item[] \textbf{Key findings} InfluxDB and TimescaleDB beat MariaDB on every query except the single-row lookup. InfluxDB excels on large datasets and heavy aggregates (≈ 8 – 20× faster than TimescaleDB). TimescaleDB wins point look-ups (≈ 19× faster than InfluxDB) thanks to indexes inherited from PostgreSQL. Conclusion: choose a DB to match query mix—TSDBs shine on range/aggregate workloads; relational engines still dominate keyed.
      
       \item[] \textbf{Relevance} Gives concrete latency numbers for the \textit{storage-engine axis} of my study and illustrates how query pattern changes the winner. I will cite it when justifying why Klimra's benchmark must include both small point queries and wide time-range analytics.
    \end{itemize}
    \newpage
    \item[] \textbf{3. SciTS: A Benchmark for Time-Series Databases in Scientific Experiments and Industrial IoT}
    \begin{itemize}
       \item[] \textbf{Scope \& method}  Proposes \emph{SciTS}, an open-source benchmark that stresses databases with heavy INSERT loads \textit{and} five realistic query types (raw fetch, out-of-range, aggregate, down-sample, two-sensor compare).  Evaluates four engines—InfluxDB, ClickHouse, TimescaleDB, and PostgreSQL—while logging CPU, memory, disk-, and network-I/O.
      
       \item[] \textbf{Key findings}  ClickHouse leads on both ingest (≈1.3 M rows/s) and most queries; InfluxDB and TimescaleDB trade wins depending on query pattern; PostgreSQL falls behind once indexes no longer fit in RAM.  Resource metrics reveal that PostgreSQL’s swap use, not network, drives its slowdown.
      
       \item[] \textbf{Relevance}  Closely matches my study design: same databases (except DynamoDB), same high-ingestion focus, same multi-metric logging.  Offers a ready template for workload definitions and a comparative baseline against which Klimra's train-delay dataset results can be discussed.
  \end{itemize}
    \item[] \textbf{4. A Comparative Analysis of Database Management Systems for Time Series Data}
    \begin{itemize}
      \item[] \textbf{Scope \& method}  Bachelor thesis (KTH EECS, 2023) that loads publicly-available SMHI weather data (millions of readings, many stations) into two systems: PostgreSQL + TimescaleDB and MongoDB (native time-series collections).  
      Five real-world query patterns are timed—from single-station look-ups to full-range interval scans—and runtimes are averaged over repeated runs.
      
      \item[] \textbf{Key findings}  TimescaleDB excels at broad time-range and complex aggregate queries (up to 100× faster); MongoDB wins highly-selective, tag-filtered look-ups (over 30× faster in one case).  Neither DBMS dominates overall; the “right” choice depends on query mix and selectivity.
      
      \item[] \textbf{Relevance}  Directly informs my storage-engine axis by showing complementary strengths of a relational-TSDB extension versus a NoSQL document store.  I will cite it when arguing that Klimra’s workload—mixing broad range analytics with targeted station queries—may favour different back-ends for different tasks.
  \end{itemize}
    \item[] \textbf{5. Comparative Analysis of AWS Cloud Data Storage System Architectures} 
    \begin{itemize}
      \item[] \textbf{Scope \& method}  
      2025 University-of-Helsinki master’s thesis that builds three end-to-end IoT pipelines on AWS:  
      (i) \textit{Amazon Timestream} (serverless TSDB),  
      (ii) \textit{DynamoDB} (key-value NoSQL), and  
      (iii) \textit{Kinesis Data Streams + S3 data-lake}.  
      A simulator publishes high-frequency air-quality readings (MQTT via IoT Core); the study measures retrieval time for three window sizes—last 1 min, last 30 min, and 1-day history—plus counts API calls and estimates cost.
      
      \item[] \textbf{Key findings}  
      *Timestream* excels when queries filter or aggregate individual fields, but performs worst on full-row scans.  
      *DynamoDB* is fastest for very short, recent slices (≤ 30 min) yet times out on multi-hour ranges.  
      *S3 data-lake* (batched through Firehose) dominates bulk historical reads but is unsuitable for “latest-minute” dashboards.  Service configuration (shard count, retention, partitioning) strongly influences results.
      
      \item[] \textbf{Relevance}  
      Directly matches my AWS storage axis (PostgreSQL/Timestream/DynamoDB vs. S3).  Confirms that Timestream is not a guaranteed win and that access-pattern matters—a lesson I will incorporate when selecting query workloads and when discussing why Klimra might need more than one backend.
  \end{itemize}
\end{itemize}

\newpage
\section*{Benchmarking Backend Performance in Cloud Environments (Methodological Studies)}

\subsection*{Background and Context}
To evaluate backend services under realistic cloud workloads, recent research emphasizes rigorous benchmarking methods, careful metric collection, and reliable experimental design. Below are peer-reviewed studies and theses that focus on methodologies for benchmarking cloud-based backends (APIs, databases, data pipelines), highlighting how they ensure accurate measurement of latency, throughput, resource usage, and I/O under realistic conditions:

\begin{itemize}
    \item[] \textbf{Performance evaluation metrics for cloud, fog and edge computing: A review, taxonomy, benchmarks and standards for future research}
    \begin{itemize}
        \item[] This study provide a comprehensive survey of performance metrics and benchmarking standards for distributed cloud environments. This work defines key metrics (e.g. response latency, throughput, CPU/memory usage) and categorizes benchmark tools, offering a taxonomy of real-world metrics relevant to cloud and edge computing. By identifying which metrics are most useful for evaluating cloud services and how to measure them, this review establishes a foundation for methodologically sound performance evaluations of backend systems. It underscores the importance of using diverse metrics and standardized benchmarks to ensure accurate and comparable results when testing cloud-based backends.
    \end{itemize}
    \item[] \textbf{Microservices: A Performance Tester’s Dream or Nightmare?}
    \begin{itemize}
        \item[] This study studies the challenges of performance testing microservice-based applications in cloud environments. They find that cloud-based microservices exhibit high variability in response times across repeated test runs, even when CPU utilization remains stable. The paper highlights that statistically significant differences in response latency can occur due to deployment nondeterminism, complicating result analysis. To obtain reliable insights, the authors emphasize executing enough repetitions of tests to account for environmental variance. They conclude that performance benchmarking for microservices in the cloud is not straightforward, requiring extra care (e.g. automation, statistical analysis) to detect regressions and achieve consistent results. This study underscores methodological best practices like repeated experiments, controlled environments, and careful result interpretation for cloud API performance.
    \end{itemize}
    \item[] \textbf{Benchmarking microservice platforms and applications in the cloud}
    \begin{itemize}
        \item[] This study addresses systematic methods to benchmark microservice architectures in cloud settings. The thesis notes that performance variability in public clouds makes it difficult to precisely measure and reproduce backend performance metrics. It points out a fundamental trade-off between accuracy and effort: running longer or more frequent benchmark tests improves the precision of metrics (e.g. latency, throughput) but is costly and time-consuming. Grambow proposes automating and optimizing the benchmarking process – for example, by adapting workload patterns whenever a service’s API changes and integrating smaller micro-benchmarks into CI/CD pipelines – to balance thoroughness with practicality. These contributions are methodologically significant: they enable continuous and reliable performance evaluation of cloud-native backends despite rapid code changes, while accounting for cloud-induced variability in results. This work is directly relevant to benchmarking API-driven architectures, as it improves how experiments are designed and repeated for accuracy.
    \end{itemize}
    \newpage
    \item[] \textbf{Benchmarking Microservice Performance: A Pattern-Based Approach}
    \begin{itemize}
        \item[] This study introduce a novel approach to reduce the effort of benchmarking RESTful backend services while preserving realistic workload complexity. They observe that each microservice may require a custom benchmark (due to differing APIs), which is costly to implement and update as the service evolves. To address this, the authors propose modeling reusable interaction patterns based on an abstract description of the service’s REST API (e.g. using OpenAPI definitions). Their framework automatically generates and executes a suite of operations (with proper data dependencies resolved at runtime) that emulate real client behavior across the API. This pattern-based method ensures that complex, stateful API interactions (typical of modern microservices) are included in the benchmark with minimal manual effort. By enabling developers to quickly craft representative workloads for any REST API, the study contributes a cloud-native benchmarking technique that keeps pace with evolving backend services and yields meaningful performance metrics under realistic usage patterns.
    \end{itemize}
    \item[] \textbf{Benchmarking scalability of stream processing frameworks deployed as microservices in the cloud}
    \begin{itemize}
        \item[] Henning & Hasselbring conduct an extensive empirical study of data ingestion pipeline performance in cloud environments. They use a systematic, cloud-native benchmark to compare five modern stream processing frameworks (e.g. Apache Flink, Kafka Streams), deploying them as microservice instances on Kubernetes clusters. Over 740 hours of experiments in Google’s cloud (and a private cloud), the authors measure how each framework’s throughput scales with increasing load by monitoring message processing rates (up to 1 million msg/s) and resource consumption as the number of microservice instances grows. The results, achieved under realistic distributed workloads, show approximately linear scalability for all frameworks given sufficient resources, but also reveal differences in resource efficiency and scaling behavior between technologies. This study’s rigorous design – long-running tests, multiple cloud environments, and varied scale-out patterns – exemplifies proper benchmarking technique for backend data-processing services. It demonstrates how to interpret throughput and resource metrics to identify bottlenecks and efficiency trade-offs in cloud-based streaming architectures, guiding practitioners in evaluating ingestion pipeline performance under real-world loads.
    \end{itemize}
    \item[] \textbf{Microservices vs Monolithic Architecture: Load Testing in AWS on ReactJS Web Application for Performance}
    \begin{itemize}
        \item[] Salunkhe provides a practical evaluation of backend architecture performance on Amazon Web Services, illustrating sound benchmarking methodology with industry-relevant tools. In this thesis, a monolithic Node.js backend on EC2 and an equivalent microservices deployment on Amazon EKS are subjected to identical API load tests. The author uses Locust (an open-source load generator) to simulate realistic user traffic to the services and employs AWS CloudWatch to monitor system metrics like CPU utilization and resource usage during tests. By replaying typical API requests at controlled rates and volumes, the study captures detailed performance data (e.g. response time under increasing concurrent users) for each architecture. The combination of CloudWatch’s granular monitoring with Locust’s workload simulation allows identification of performance bottlenecks and scaling limits in a cloud setting. The thesis reports that under low load the monolith exhibited lower latency, but under high concurrency the microservices architecture maintained more stable, lower response times. These findings, derived from a methodologically careful A/B comparison, highlight how cloud-native tools and load-testing frameworks can be used to benchmark backend APIs reliably. The work underscores best practices such as using realistic workloads, cloud monitoring instrumentation, and scenario-based testing to evaluate different backend designs on AWS, yielding actionable insights into their latency-throughput profiles.
    \end{itemize}
\end{itemize}

\end{document}
<z<z<Z<