\documentclass[11pt]{article}
\usepackage{dirtytalk}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage[hidelinks]{hyperref}
\geometry{margin=0.8in}

% Nicely-formatted bullet lists
\usepackage{enumitem}
\setlist[itemize]{label=--, leftmargin=*, itemsep=0.4em}

% Optional: simple checkbox symbols (uncomment if you want them)
% \usepackage{pifont}
% \newcommand{\checkbox}{\ding{113}\;}   % empty box
% \newcommand{\checked}{\ding{51}\;}     % check mark

\title{Pre-Study Task Notes}
\date{\today}

\begin{document}
\maketitle

\section*{Event-Driven vs Polling Ingestion}
\begin{itemize}
  \item[] \textbf{1. Analyzing the Efficiency of Event-Driven APIs}
  \begin{itemize}
      \item[] \textbf{Scope \& method}  This is a bachelor-level case study at Bolagsverket that builds three prototypes. These are: MQTT broker, WebSocket server, and the agency's legacy SFTP flow. It then measures nine resource metrics (CPU, duration, network I/O, memory, disk, threads, handles) while replaying messages of increasing payload sizes.
      
      \item[] \textbf{Key findings}  MQTT and WebSocket cut latency and roughly halve CPU/network usage compared with SFTP for typical payloads; MQTT stays fastest as payloads grow, whereas WebSocket performance drops sharply beyond ≈4 kB; SFTP shows linear disk-I/O growth, which signals a bottleneck for bulk updates.
      
      \item[] \textbf{Relevance}  Mirrors this thesis's ingestion axis (polling-style file transfer vs.\ event-driven push) and logs the same resource counters planned for the Klimra benchmark, providing both a comparable metric suite and an additional empirical data point from a Swedish public-sector context.
  \end{itemize}
  \item[] \textbf{2. Migration of a SCADA system to IaaS clouds – a case study}
  \begin{itemize}
      \item[] \textbf{Scope \& method} The authors took a factory-control system (SCADA) that normally \emph{polls} hundreds of sensors every millisecond and moved it to virtual machines in the cloud.  They timed each request and also watched how much CPU, memory, and network traffic the system used.
      
      \item[] \textbf{Key findings} As the number of sensors grew, the round-trip time for each poll exploded—from about 0.01 s with one sensor to roughly 14 s with 800 sensors.  Almost all of that delay happened inside the software, not on the network, showing that constant polling can overload the server itself.
      
      \item[] \textbf{Relevance} This study is a clear real-world example of how polling can become a performance bottleneck.  It backs up my argument that switching to event-driven updates can cut wasted work, useful support for the \textit{ingestion‐strategy} part of my comparison, even though the paper says nothing about databases.
      \item[] \href{https://link.springer.com/article/10.1186/s13677-017-0080-5}{LINK}
  \end{itemize}
  \item[] \textbf{3. Energy-efficient network protocols for domestic IoT application design}
  \begin{itemize}
      \item[] \textbf{Scope \& method} Measured the power draw of five common wireless links (Bluetooth, BLE, ZigBee, Wi-Fi, 433 MHz) in three communication styles—\emph{broadcast}, \emph{polling}, and \emph{event-driven}.  
      A toaster-to-gateway \say{smart kitchen} scenario was used to calculate weekly energy use at different event rates.
      
      \item[] \textbf{Key findings} With infrequent events, event-driven traffic is far cheaper: Bluetooth polling consumed ≈3000× more energy than event-driven for 20 events/week, and Wi-Fi about 10 000×.  
      Above a threshold (~7–10 events/week for their RF433 example), polling or even broadcast becomes the cheaper option, illustrating a clear crossover point between idle overhead and high-rate efficiency.
      
      \item[] \textbf{Relevance} Although focused on battery life rather than latency, the paper nicely quantifies the \textit{same polling-vs-push trade-off} under varying event frequencies.  
      I can cite it to show that the “best” ingestion style depends on workload intensity—an insight that complements my latency-centric benchmark and helps justify testing multiple load levels.
      \item[] \href{https://www.researchgate.net/publication/335426973\_Energy-Efficient\_Network\_Protocols\_for\_Domestic\_IoT\_Application\_Design}{LINK}
  \end{itemize}
  \newpage
  \item[] \textbf{The Power of Event-Driven Architecture: Enabling Real-Time Systems and Scalable Solutions}
  \begin{itemize}
      \item[] This paper examines the impact of adopting an event-driven architecture (EDA) in enterprise back-end systems, compared to traditional periodic polling. Kommera reports that in large-scale data center orchestration, polling-based mechanisms were wasting substantial resources – roughly 22–31\% of available CPU on average – with nearly 70\% of polling operations finding no new data (i.e. idle work). The study cites that switching to an event-driven model yielded significant performance gains: across several enterprise use cases, CPU utilization dropped by about 47\% and critical operation response times improved by ~89\% when moving from polling to event-driven ingestion. These results underscore the efficiency advantage of event-driven designs in API-driven systems – by reacting to events in real time rather than constantly querying, systems can vastly reduce overhead and latency.
  \end{itemize}
  \item[] \textbf{On the impact of event-driven architecture on performance: An exploratory
study}
  \begin{itemize}
      \item[] This recent empirical study directly compares an event-driven microservices architecture with a traditional monolithic architecture under the same application workload. The authors implemented two versions of an application – one using asynchronous event queues between services, and one using a straightforward monolithic (synchronous) design – and measured metrics like CPU usage, memory consumption, throughput, and response times. Perhaps counterintuitively, the monolithic (polling/request-response) architecture outperformed the event-driven version in their tests: it used fewer computational resources and achieved faster response times than the event-driven architecture. The paper notes that the overhead of event routing, message handling, and intermediate brokers in the EDA introduced extra latency and resource usage, suggesting that the benefits of event-driven ingestion are not automatic and may depend on the context. This study provides a nuanced view that while EDA can improve scalability, it may incur performance overhead unless carefully optimized.
      \item[] \href{https://www.sciencedirect.com/science/article/pii/S0167739X23003977}{LINK}
  \end{itemize}
  \item[] \textbf{Event-Driven Microservices Architecture for Data Center Orchestration}
  \begin{itemize}
      \item[] This work offers a broad review of adopting event-driven microservice architecture (EDMA) in modern back-end systems, highlighting real-time data processing benefits over polling-centric designs. It emphasizes that traditional polling approaches impose heavy overhead by continuously checking for updates and consuming resources for oft-null results, whereas an event-driven approach triggers processing only when relevant events occur. Across various enterprise case studies, moving to an event-driven ingestion model led to dramatic improvements in operational efficiency, responsiveness, and resource utilization. For example, the paper cites industry reports of significantly higher throughput and lower latency in systems that replaced periodic API polling with event-driven notifications. Overall, this source underlines the performance trade-offs: event-driven architectures can greatly enhance real-time responsiveness and efficiency in backend systems, though they require robust messaging infrastructure and design considerations (such as handling eventual consistency and complex event management). 
      \item[] \href{https://www.ijsat.org/research-paper.php?id=3113}{LINK}
  \end{itemize}
\end{itemize}

\newpage
\section*{Performance of Time-Series vs Relational vs NoSQL Databases Under High-Ingestion Workloads}
\subsection*{Background and Context}
Modern data-intensive applications (e.g. IoT sensor networks, financial tick feeds, telemetry) generate time-series data at high velocity. Choosing the right database system is critical for real-time ingestion and querying of this data. Traditional relational databases (SQL/RDBMS) can struggle with the volume and sequential nature of time-stamped data, while specialized time-series databases (TSDBs) and NoSQL databases promise optimizations for high ingestion rates and scalable query performance. Recent research has empirically compared these systems under heavy write loads and real-time query scenarios, often using IoT or sensor data as benchmarks. Below, we summarize several peer-reviewed studies and academic theses that benchmark time-series vs. relational vs. NoSQL databases (including AWS services like Amazon Timestream, DynamoDB, and RDS PostgreSQL) under high-ingestion or real-time workloads. We highlight both performance results and methodology (datasets, queries, and metrics) from each source, to inform database selection for such use cases.
\begin{itemize}
    \item[] \textbf{Comparative Analysis of Time-Series Databases in the Context of Edge Computing for Low-Power Sensor Networks}
    \begin{itemize}
        \item[] This study (ICCS 2020 conference) compared three TSDBs (TimescaleDB, InfluxDB, Riak TS) against two relational systems (PostgreSQL and SQLite) on a resource-constrained device (Raspberry Pi). The authors simulated an IoT sensor workload with a custom app for inserting time-series readings and querying them. Result: Despite the low-power hardware, a relational database (PostgreSQL) kept pace with or outperformed the specialized stores in many cases. In fact, PostgreSQL and InfluxDB delivered the best read query throughput in their tests, while PostgreSQL achieved the fastest insert rates of all systems. TimescaleDB (PostgreSQL with time-series extension) did not show a performance gain over vanilla PostgreSQL in that environment, possibly due to the small dataset and device limits. The experiment demonstrated that even on IoT edge devices, a well-tuned relational engine can handle high ingestion, though results might differ at larger scale. 
    \end{itemize}
    \item[] \textbf{Performance Analysis of Time Series Databases for IoT Applications}
    \begin{itemize}
        \item[] This Uppsala University thesis evaluated two purpose-built time-series databases (InfluxDB and TimescaleDB) against a general-purpose RDBMS (MariaDB/MySQL) using real IoT sensor data. The methodology involved loading between 1 million and 50 million timestamped readings and executing four query types: point lookups, time-range selects, aggregate computations, and “time-sensitive” aggregates (windowed analysis). Findings: Both InfluxDB and TimescaleDB significantly outperformed the relational database on almost all queries except the simplest point lookup. In particular, InfluxDB excelled at large-range and aggregate queries, running about 8–20× faster than TimescaleDB for certain heavy analytics on big datasets. TimescaleDB, on the other hand, matched or beat InfluxDB on small-scale queries and was about 19× faster than Influx for single-record point retrievals due to its indexing on keys. These results underscore that specialized TSDBs handle high-volume time-series aggregation workloads more efficiently than a traditional SQL store, though a relational engine with a time-series extension (Timescale) can still shine for selective queries. The thesis also emphasizes tailoring the database choice to specific IoT use-cases and query patterns.
    \end{itemize}
    \item[] \textbf{SciTS: A Benchmark for Time-Series Databases in Scientific Experiments and Industrial IoT}
    \begin{itemize}
        \item[] This paper is a strong fit for my thesis because it addresses almost the same experimental questions I need to answer. The authors benchmark three purpose-built time-series databases (InfluxDB, ClickHouse, TimescaleDB) against a conventional relational engine (PostgreSQL) while sustaining a high ingest rate—very similar to the constant stream of train-position updates I will pull from Trafikverket. Crucially, they don’t stop at latency: their tests log CPU, memory, and both disk- and network-I/O for every run, giving me a concrete template for the “memory and I/O usage” measurements my examiner asked about. They also evaluate range, aggregation and down-sampling queries after ingestion, which mirrors the analytics I plan to run over historical delay data. Because the study is peer-reviewed (ACM SSDBM 2022), I can cite it with confidence and use its methodology and results as a comparative baseline.
    \end{itemize}
    \item[] \textbf{A Comparative Analysis of Database Management Systems for Time Series Data}
    \begin{itemize}
        \item[] A Swedish master’s thesis compared a relational time-series extension (TimescaleDB on PostgreSQL) with a NoSQL document store (MongoDB) featuring its native time-series collections. The case study used a weather sensor dataset (millions of readings across many stations) and measured query response times for typical analytics (e.g. retrieving all data in a given time interval vs. fetching data from specific sensors). Result: The two systems exhibited complementary strengths. TimescaleDB handled complex, large-range interval queries more efficiently – e.g. pulling all readings over a time window across many sensors was faster with Timescale’s partitioned hypertables. MongoDB, however, outperformed Timescale for targeted lookups involving a subset of sensors (e.g. querying a single weather station’s data). This suggests that a document-oriented NoSQL DB, with flexible JSON schema, can be advantageous when queries are highly selective on tags (like sensor ID), whereas a time-series relational DB excels at bulk time-range scans. The thesis methodology logged wall-clock timings for each query type, repeating trials to account for variability. The authors conclude that neither system is universally better – the “optimal” choice depends on whether the workload is broad-scale analysis or narrow, per-sensor retrieval in this time-series context
    \end{itemize}
    \item[] \textbf{Comparative Analysis of AWS Cloud Data Storage System Architectures} 
    \begin{itemize}
        \item[]  This University of Helsinki master’s thesis (2025) focuses on AWS-managed database services for time-series IoT data, comparing Amazon Timestream (a serverless TSDB), Amazon DynamoDB (NoSQL key-value store), and an AWS data lake approach (e.g. storing time-series in S3 or similar) for ingesting and querying high-frequency sensor streams. The thesis designed an end-to-end IoT data pipeline (using AWS IoT Core and Lambda) to feed each storage solution, then evaluated query performance under different access patterns (e.g. retrieving recent vs. entire history of the data) and ingestion throughput. Finding: Amazon Timestream was found to underperform significantly in certain scenarios – in particular, it showed “significantly poorer performance” when retrieving full data items (i.e. full dataset scans or large result sets) compared to the other solutions. This indicates that while Timestream is optimized for time-series, it may struggle with large ad-hoc queries, potentially due to internal storage layouts or query engine trade-offs. DynamoDB and the data lake approach likely handled such full-data queries more efficiently in the study’s tests (e.g. Dynamo’s key-value access was faster for bulk retrieval in this case). The thesis underscores the importance of evaluating cloud database services under realistic workload scenarios – a system like DynamoDB might offer better consistency for high-concurrency writes, whereas Timestream’s performance benefits appear only under certain query patterns and not for exhaustive scans. The overall methodology and results provide guidance on architecting IoT data platforms on AWS, balancing factors like query latency, data volume, and cost.
    \end{itemize}
\end{itemize}

\newpage
\section*{Benchmarking Backend Performance in Cloud Environments (Methodological Studies)}

\subsection*{Background and Context}
To evaluate backend services under realistic cloud workloads, recent research emphasizes rigorous benchmarking methods, careful metric collection, and reliable experimental design. Below are peer-reviewed studies and theses that focus on methodologies for benchmarking cloud-based backends (APIs, databases, data pipelines), highlighting how they ensure accurate measurement of latency, throughput, resource usage, and I/O under realistic conditions:

\begin{itemize}
    \item[] \textbf{Performance evaluation metrics for cloud, fog and edge computing: A review, taxonomy, benchmarks and standards for future research}
    \begin{itemize}
        \item[] This study provide a comprehensive survey of performance metrics and benchmarking standards for distributed cloud environments. This work defines key metrics (e.g. response latency, throughput, CPU/memory usage) and categorizes benchmark tools, offering a taxonomy of real-world metrics relevant to cloud and edge computing. By identifying which metrics are most useful for evaluating cloud services and how to measure them, this review establishes a foundation for methodologically sound performance evaluations of backend systems. It underscores the importance of using diverse metrics and standardized benchmarks to ensure accurate and comparable results when testing cloud-based backends.
    \end{itemize}
    \item[] \textbf{Microservices: A Performance Tester’s Dream or Nightmare?}
    \begin{itemize}
        \item[] This study studies the challenges of performance testing microservice-based applications in cloud environments. They find that cloud-based microservices exhibit high variability in response times across repeated test runs, even when CPU utilization remains stable. The paper highlights that statistically significant differences in response latency can occur due to deployment nondeterminism, complicating result analysis. To obtain reliable insights, the authors emphasize executing enough repetitions of tests to account for environmental variance. They conclude that performance benchmarking for microservices in the cloud is not straightforward, requiring extra care (e.g. automation, statistical analysis) to detect regressions and achieve consistent results. This study underscores methodological best practices like repeated experiments, controlled environments, and careful result interpretation for cloud API performance.
    \end{itemize}
    \item[] \textbf{Benchmarking microservice platforms and applications in the cloud}
    \begin{itemize}
        \item[] This study addresses systematic methods to benchmark microservice architectures in cloud settings. The thesis notes that performance variability in public clouds makes it difficult to precisely measure and reproduce backend performance metrics. It points out a fundamental trade-off between accuracy and effort: running longer or more frequent benchmark tests improves the precision of metrics (e.g. latency, throughput) but is costly and time-consuming. Grambow proposes automating and optimizing the benchmarking process – for example, by adapting workload patterns whenever a service’s API changes and integrating smaller micro-benchmarks into CI/CD pipelines – to balance thoroughness with practicality. These contributions are methodologically significant: they enable continuous and reliable performance evaluation of cloud-native backends despite rapid code changes, while accounting for cloud-induced variability in results. This work is directly relevant to benchmarking API-driven architectures, as it improves how experiments are designed and repeated for accuracy.
    \end{itemize}
    \newpage
    \item[] \textbf{Benchmarking Microservice Performance: A Pattern-Based Approach}
    \begin{itemize}
        \item[] This study introduce a novel approach to reduce the effort of benchmarking RESTful backend services while preserving realistic workload complexity. They observe that each microservice may require a custom benchmark (due to differing APIs), which is costly to implement and update as the service evolves. To address this, the authors propose modeling reusable interaction patterns based on an abstract description of the service’s REST API (e.g. using OpenAPI definitions). Their framework automatically generates and executes a suite of operations (with proper data dependencies resolved at runtime) that emulate real client behavior across the API. This pattern-based method ensures that complex, stateful API interactions (typical of modern microservices) are included in the benchmark with minimal manual effort. By enabling developers to quickly craft representative workloads for any REST API, the study contributes a cloud-native benchmarking technique that keeps pace with evolving backend services and yields meaningful performance metrics under realistic usage patterns.
    \end{itemize}
    \item[] \textbf{Benchmarking scalability of stream processing frameworks deployed as microservices in the cloud}
    \begin{itemize}
        \item[] Henning & Hasselbring conduct an extensive empirical study of data ingestion pipeline performance in cloud environments. They use a systematic, cloud-native benchmark to compare five modern stream processing frameworks (e.g. Apache Flink, Kafka Streams), deploying them as microservice instances on Kubernetes clusters. Over 740 hours of experiments in Google’s cloud (and a private cloud), the authors measure how each framework’s throughput scales with increasing load by monitoring message processing rates (up to 1 million msg/s) and resource consumption as the number of microservice instances grows. The results, achieved under realistic distributed workloads, show approximately linear scalability for all frameworks given sufficient resources, but also reveal differences in resource efficiency and scaling behavior between technologies. This study’s rigorous design – long-running tests, multiple cloud environments, and varied scale-out patterns – exemplifies proper benchmarking technique for backend data-processing services. It demonstrates how to interpret throughput and resource metrics to identify bottlenecks and efficiency trade-offs in cloud-based streaming architectures, guiding practitioners in evaluating ingestion pipeline performance under real-world loads.
    \end{itemize}
    \item[] \textbf{Microservices vs Monolithic Architecture: Load Testing in AWS on ReactJS Web Application for Performance}
    \begin{itemize}
        \item[] Salunkhe provides a practical evaluation of backend architecture performance on Amazon Web Services, illustrating sound benchmarking methodology with industry-relevant tools. In this thesis, a monolithic Node.js backend on EC2 and an equivalent microservices deployment on Amazon EKS are subjected to identical API load tests. The author uses Locust (an open-source load generator) to simulate realistic user traffic to the services and employs AWS CloudWatch to monitor system metrics like CPU utilization and resource usage during tests. By replaying typical API requests at controlled rates and volumes, the study captures detailed performance data (e.g. response time under increasing concurrent users) for each architecture. The combination of CloudWatch’s granular monitoring with Locust’s workload simulation allows identification of performance bottlenecks and scaling limits in a cloud setting. The thesis reports that under low load the monolith exhibited lower latency, but under high concurrency the microservices architecture maintained more stable, lower response times. These findings, derived from a methodologically careful A/B comparison, highlight how cloud-native tools and load-testing frameworks can be used to benchmark backend APIs reliably. The work underscores best practices such as using realistic workloads, cloud monitoring instrumentation, and scenario-based testing to evaluate different backend designs on AWS, yielding actionable insights into their latency-throughput profiles.
    \end{itemize}
\end{itemize}

\end{document}
<z<z<Z<